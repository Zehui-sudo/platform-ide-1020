#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å››é˜¶æ®µå­¦ä¹ å¤§çº²ç”Ÿæˆå™¨ï¼ˆBrainstorm â†’ Structure â†’ Detail â†’ Reviewï¼‰ã€‚

è¾“å…¥ï¼š
- ä¸»é¢˜ï¼ˆ--topicï¼‰
- æ·±åº¦ï¼ˆ--depthï¼Œæ§åˆ¶ç²’åº¦ä¸è§„æ¨¡ï¼‰

è¾“å‡ºï¼š
- seeds.jsonï¼ˆé˜¶æ®µ1ï¼šå‘æ•£è„‘æš´äº§ç‰©ï¼‰
- skeleton.jsonï¼ˆé˜¶æ®µ2ï¼šç« èŠ‚éª¨æ¶ï¼‰
- outline.jsonï¼ˆé˜¶æ®µ3ï¼šå®Œæ•´ç« /èŠ‚/ç‚¹ + primary_goal + suggested_modules + ç¨³å®šIDï¼‰
- learning-path.mdï¼ˆä¸ç°æœ‰é£æ ¼ä¸€è‡´çš„ Markdown å¤§çº²ï¼‰
- review.jsonï¼ˆé˜¶æ®µ4ï¼šè¯„åˆ†ä¸å»ºè®®ï¼Œå«è½»é‡ä¿®å¤åçš„è®°å½•ï¼‰

è¯´æ˜ï¼š
- æœ¬è„šæœ¬ä¸å†ä¾èµ– archetype + switchesã€‚æ”¹ä¸ºä»¥ primary_goalï¼ˆä¸»è¦æ•™å­¦ç›®æ ‡ï¼‰ä¸ suggested_modulesï¼ˆå»ºè®®å†…å®¹æ¨¡å—ï¼‰è½¯çº¦æŸé©±åŠ¨åç»­å†…å®¹ç”Ÿæˆã€‚
- ä¾èµ– config.json é€‰æ‹©å¤§æ¨¡å‹ï¼Œæ”¯æŒ DeepSeekï¼ˆOpenAI å…¼å®¹ï¼‰ä¸ Geminiã€‚
- å¹¶å‘å— config.json çš„ max_parallel_requests æ§åˆ¶ï¼Œå¯ç”¨ --max-parallel è¦†ç›–ã€‚
"""

from __future__ import annotations

import argparse
import concurrent.futures as cf
import json
import math
import os
import re
import statistics
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union


# -----------------------------
# é…ç½®ä¸æ¨¡å‹é€‰æ‹©
# -----------------------------


def load_config(path: str) -> Dict[str, Any]:
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶: {path}")
    try:
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception as e:
        raise RuntimeError(f"é…ç½®æ–‡ä»¶è§£æå¤±è´¥: {e}")


@dataclass
class LLMConfig:
    key: str
    provider: str
    model: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    temperature: Optional[float] = 0.4


def choose_llm(cfg: Dict[str, Any], llm_key: Optional[str], node_key_fallback: str) -> LLMConfig:
    llms = cfg.get("llms") or {}
    if not llms:
        raise RuntimeError("é…ç½®ä¸­ç¼ºå°‘ 'llms'ã€‚")
    if not llm_key:
        node_llm = cfg.get("node_llm") or {}
        llm_key = node_llm.get(node_key_fallback) or node_llm.get("default")
    if not llm_key:
        llm_key = next(iter(llms.keys()))
    entry = llms.get(llm_key)
    if not entry:
        raise RuntimeError(f"åœ¨ config.json ä¸­æ‰¾ä¸åˆ° llm: {llm_key}")
    provider = entry.get("provider") or "openai"
    model = entry.get("model") or llm_key
    api_key = entry.get("api_key") or os.environ.get("OPENAI_API_KEY")
    base_url = entry.get("base_url")
    temperature = entry.get("temperature", 0.4)
    return LLMConfig(key=llm_key, provider=provider, model=model, api_key=api_key, base_url=base_url, temperature=temperature)


class LLMCaller:
    """å…¼å®¹ DeepSeek/OpenAI ä¸ Gemini çš„æœ€å°è°ƒç”¨åŒ…è£…ã€‚"""

    def __init__(self, conf: LLMConfig):
        self.conf = conf
        self._client = None
        self._init()

    def _init(self) -> None:
        if self.conf.provider in ("deepseek", "openai"):
            try:
                from openai import OpenAI
            except Exception:
                raise SystemExit("ç¼ºå°‘ openai åº“ï¼Œè¯·å…ˆå®‰è£…ï¼špip install openai")
            if not self.conf.api_key:
                raise SystemExit("æœªé…ç½® API Keyã€‚è¯·åœ¨ config.json æˆ–ç¯å¢ƒå˜é‡ä¸­æä¾›ã€‚")
            self._client = OpenAI(api_key=self.conf.api_key, base_url=self.conf.base_url)
        elif self.conf.provider == "gemini":
            try:
                import google.generativeai as genai
            except Exception:
                raise SystemExit("ç¼ºå°‘ google-generativeai åº“ï¼Œè¯·å…ˆå®‰è£…ï¼špip install google-generativeai")
            if not self.conf.api_key:
                raise SystemExit("æœªé…ç½® Gemini API Keyã€‚")
            genai.configure(api_key=self.conf.api_key)
            self._client = genai
        else:
            raise SystemExit(f"æš‚ä¸æ”¯æŒçš„ provider: {self.conf.provider}")

    def chat_json(self, prompt: str, max_tokens: int = 2048) -> str:
        if self.conf.provider in ("deepseek", "openai"):
            resp = self._client.chat.completions.create(
                model=self.conf.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåªè¾“å‡º JSON çš„åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt},
                ],
                temperature=self.conf.temperature or 0.4,
                max_tokens=max_tokens,
            )
            return resp.choices[0].message.content or ""
        elif self.conf.provider == "gemini":
            model = self._client.GenerativeModel(self.conf.model)
            resp = model.generate_content(prompt)
            return getattr(resp, "text", "") or ""
        else:
            raise RuntimeError("æœªçŸ¥ provider")


# -----------------------------
# å…±ç”¨å·¥å…·
# -----------------------------


# archetype/switches å·²åºŸå¼ƒï¼šæ”¹ç”± primary_goal + suggested_modules è½¯å»ºè®®é©±åŠ¨


def _iter_balanced_fragments(s: str, open_char: str, close_char: str):
    """Yield top-level balanced JSON fragments delimited by open_char/close_char.

    Handles quotes and escapes so brackets/braces inside strings do not affect
    balance. Returns raw substrings (not parsed).
    """
    in_string = False
    string_char = ""
    escape = False
    depth = 0
    start = -1
    for i, c in enumerate(s):
        if in_string:
            if escape:
                escape = False
            elif c == "\\":
                escape = True
            elif c == string_char:
                in_string = False
            continue
        else:
            if c in ("'", '"'):
                in_string = True
                string_char = c
                continue
            if c == open_char:
                if depth == 0:
                    start = i
                depth += 1
                continue
            if c == close_char and depth > 0:
                depth -= 1
                if depth == 0 and start != -1:
                    yield s[start : i + 1]
                    start = -1


def extract_json(s: str) -> Optional[Union[Dict[str, Any], List[Any]]]:
    """Best-effort JSON extraction.

    Supports:
    - Pure JSON (object or array)
    - JSON inside fenced code blocks (```json ... ``` or ~~~json ... ~~~)
    - JSON arrays/objects embedded in surrounding text via balanced scanning
    Returns a parsed object (dict or list) or None.
    """
    s = (s or "").strip()
    if not s:
        return None

    # 1) Direct parse attempt
    try:
        return json.loads(s)
    except Exception:
        pass

    # 2) Try fenced code blocks (both ``` and ~~~)
    fence_patterns = [
        r"```(?:json|JSON)?\s*([\s\S]*?)\s*```",
        r"~~~(?:json|JSON)?\s*([\s\S]*?)\s*~~~",
    ]
    for pat in fence_patterns:
        for m in re.finditer(pat, s, flags=re.IGNORECASE):
            block = (m.group(1) or "").strip()
            if not block:
                continue
            # 2.1 Parse block as a whole
            try:
                return json.loads(block)
            except Exception:
                pass
            # 2.2 Try arrays first, then objects inside the block
            for frag in _iter_balanced_fragments(block, "[", "]"):
                try:
                    return json.loads(frag)
                except Exception:
                    continue
            for frag in _iter_balanced_fragments(block, "{", "}"):
                try:
                    return json.loads(frag)
                except Exception:
                    continue

    # 3) No code blocks: balanced scan on the whole string
    for frag in _iter_balanced_fragments(s, "[", "]"):
        try:
            return json.loads(frag)
        except Exception:
            continue
    for frag in _iter_balanced_fragments(s, "{", "}"):
        try:
            return json.loads(frag)
        except Exception:
            continue

    return None


def slugify(text: str, fallback: str = "outline") -> str:
    t = (
        text.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
        .replace("â€”", "-")
        .strip()
    )
    t = re.sub(r"[^0-9A-Za-z\-_.\s]", "", t)
    t = re.sub(r"\s+", "-", t)
    t = t.strip("-_.").lower()
    return t or fallback


# -----------------------------
# é˜¶æ®µ 1ï¼šå‘æ•£è„‘æš´
# -----------------------------


PERSPECTIVES = {
    "professor": {
        "name": "èµ„æ·±å¤§å­¦æ•™æˆ (Senior University Professor)",
        "focus": "è´Ÿè´£æ„å»ºåšå®çš„ç†è®ºåŸºç¡€å’ŒçŸ¥è¯†ä½“ç³»ã€‚é‡ç‚¹åœ¨äºæ ¸å¿ƒæ¦‚å¿µçš„å®šä¹‰ã€ç†è®ºçš„æ¨æ¼”ã€çŸ¥è¯†çš„è„‰ç»œã€å†å²æ¼”è¿›ä»¥åŠç›®å‰æœ€æ–°çš„æŠ€æœ¯æ–¹å‘ã€‚ç¡®ä¿å­¦ä¹ è·¯å¾„çš„å­¦æœ¯ä¸¥è°¨æ€§å’Œä½“ç³»å®Œæ•´æ€§ã€‚",
        "tags": ["core_concept", "theory", "principle", "history", "definition","trend"],
        "count": 60,
    },
    "engineer": {
        "name": "èµ„æ·±è¡Œä¸šå·¥ç¨‹å¸ˆ (Senior Industry Engineer)",
        "focus": "è´Ÿè´£æä¾›æ¥è‡ªä¸€çº¿çš„å®è·µç»éªŒå’ŒæŠ€èƒ½ã€‚é‡ç‚¹åœ¨äºå®ç”¨å·¥å…·ã€æŠ€æœ¯æ ˆã€å·¥ä½œæµç¨‹ã€æœ€ä½³å®è·µã€çœŸå®æ¡ˆä¾‹ã€ä»¥åŠç›®å‰æœ€æ–°çš„æŠ€æœ¯æ–¹å‘ã€‚ç¡®ä¿å­¦ä¹ å†…å®¹çš„å®ç”¨æ€§ã€å¯æ“ä½œæ€§å’Œå‰ç»æ€§ã€‚",
        "tags": ["skill", "tool", "workflow", "best_practice", "case_study", "pitfall", "trend"],
        "count": 60,
    },
}

def build_brainstorm_prompt_by_perspective(topic: str, language: str, perspective: Dict[str, Any]) -> str:
    return f"""
ä½ æ˜¯â€œ{perspective['name']}â€ï¼Œä¸€ä¸ªä¸“æ³¨äºç‰¹å®šè§†è§’çš„é¢†åŸŸä¸“å®¶ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ä¸ºä¸»é¢˜â€œ{topic}â€çš„å­¦ä¹ è·¯çº¿å›¾ï¼Œä»ä½ çš„ä¸“ä¸šè§†è§’å‡ºå‘ï¼Œè¿›è¡Œæ·±å…¥çš„å¤´è„‘é£æš´ã€‚

**ä½ çš„è§†è§’**: {perspective['focus']}

è¯·ä¸º**åˆå­¦è€…**è®¾æƒ³ï¼Œè¾“å‡ºä¸€ä»½çŸ¥è¯†ç‚¹æ¸…å•ï¼Œé‡ç‚¹çªå‡ºï¼Œè¦†ç›–å…¨é¢ã€‚

**è¾“å‡ºæ ¼å¼ (JSON)**:
{{
  "seeds": [
    {{"text": "<çŸ­è¯­æˆ–çŸ¥è¯†ç‚¹>", "tags": {json.dumps(perspective['tags'])}, "why": "<ä¸€å¥è¯ç†ç”±>"}},
    ... è‡³å°‘ {perspective['count']} é¡¹ ...
  ]
}}

**å…³é”®è¦æ±‚**:
- **èšç„¦ä½ çš„è§†è§’**: æ‰€æœ‰çŸ¥è¯†ç‚¹éƒ½åº”ä¸ä½ çš„ä¸“å®¶èº«ä»½å’Œè§†è§’ï¼ˆ{perspective['focus']}ï¼‰é«˜åº¦ç›¸å…³ã€‚
- **å¯¹åˆå­¦è€…å‹å¥½**: å³ä½¿æ˜¯é«˜çº§ä¸»é¢˜ï¼Œä¹Ÿè¦æ€è€ƒå¦‚ä½•ä¸ºæ–°æ‰‹å¼•å…¥ã€‚
- **ä¸¥æ ¼JSON**: åªè¾“å‡º JSONï¼Œç¦æ­¢ä»»ä½• Markdown æˆ–è§£é‡Šæ€§æ–‡å­—ã€‚

è¯·å¼€å§‹ä½ çš„å¤´è„‘é£æš´ã€‚
"""

def stage1_brainstorm(
    caller: LLMCaller, 
    topic: str, 
    language: str, 
    out_dir: Path, 
    basename: str, 
    max_tokens: int,
    max_parallel: int
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    Stage 1: Multi-perspective brainstorming.
    Returns a tuple of (professor_seeds, engineer_seeds).
    """
    print("  -> é‡‡ç”¨å¤šè§†è§’è„‘æš´ç­–ç•¥ï¼Œå¹¶è¡Œç”ŸæˆçŸ¥è¯†ç‚¹...")

    results: Dict[str, List[Dict[str, Any]]] = {}
    def run_perspective(p_key: str, p_data: Dict[str, Any]) -> Tuple[str, List[Dict[str, Any]]]:
        print(f"    -> è§†è§’ '{p_data['name']}' æ­£åœ¨ç”Ÿæˆ...")
        prompt = build_brainstorm_prompt_by_perspective(topic, language, p_data)
        raw = caller.chat_json(prompt, max_tokens=max_tokens)
        data = extract_json(raw)
        seeds = (data or {}).get("seeds", [])
        print(f"    <- è§†è§’ '{p_data['name']}' å®Œæˆï¼Œäº§å‡º {len(seeds)} ä¸ªçŸ¥è¯†ç‚¹ã€‚")
        for seed in seeds:
            if isinstance(seed, dict):
                seed['perspective'] = p_key
        return p_key, seeds

    with cf.ThreadPoolExecutor(max_workers=max_parallel) as executor:
        futures = {executor.submit(run_perspective, p_key, p_data): p_key for p_key, p_data in PERSPECTIVES.items()}
        for future in cf.as_completed(futures):
            try:
                p_key, seeds_from_perspective = future.result()
                results[p_key] = seeds_from_perspective
            except Exception as e:
                p_key_on_error = futures[future]
                print(f"    [é”™è¯¯] è§†è§’ '{p_key_on_error}' ç”Ÿæˆå¤±è´¥: {e}", file=sys.stderr)
                results[p_key_on_error] = []

    professor_seeds = results.get("professor", [])
    engineer_seeds = results.get("engineer", [])
    
    print(f"  -> æ‰€æœ‰è§†è§’è„‘æš´å®Œæˆã€‚æ•™æˆ: {len(professor_seeds)}ï¼Œå·¥ç¨‹å¸ˆ: {len(engineer_seeds)}")
    
    # For logging and compatibility, save a combined, deduplicated file
    all_seeds_raw = professor_seeds + engineer_seeds
    final_seeds = []
    seen_texts = set()
    for seed in all_seeds_raw:
        if not isinstance(seed, dict): continue
        text = seed.get("text", "").strip()
        if not text: continue
        normalized_text = re.sub(r'\s+', ' ', text).lower()
        if normalized_text not in seen_texts:
            seen_texts.add(normalized_text)
            final_seeds.append(seed)

    print(f"  -> çŸ¥è¯†ç§å­å»é‡åå…± {len(final_seeds)} ä¸ªã€‚")

    final_data = {"seeds": final_seeds, "notes": "Generated using multi-perspective brainstorming (Professor + Engineer)."}
    path = out_dir / f"{basename}.seeds.json"
    path.write_text(json.dumps(final_data, ensure_ascii=False, indent=2), encoding="utf-8")
    
    return professor_seeds, engineer_seeds


# -----------------------------
# é˜¶æ®µ 2ï¼šèšç±»ä¸æ’åºï¼ˆç« èŠ‚éª¨æ¶ï¼‰
# -----------------------------


def build_macro_planner_prompt_from_perspectives(topic: str, professor_seeds: List[Dict], engineer_seeds: List[Dict], depth: str, language: str) -> str:
    target_chapters = {"overview": [4, 5], "core": [5, 7], "advanced": [6, 8]}[depth]
    
    return f"""
ä½ æ˜¯ä¸€ä½é¡¶çº§çš„è¯¾ç¨‹æ€»è®¾è®¡å¸ˆï¼Œæ“…é•¿èåˆç†è®ºä¸å®è·µï¼Œä¸º**å®Œå…¨æ²¡æœ‰åŸºç¡€çš„åˆå­¦è€…**è®¾è®¡å®Œç¾çš„å­¦ä¹ è·¯å¾„ã€‚

**å½“å‰ä»»åŠ¡**:
ä½ æ”¶åˆ°äº†å…³äºä¸»é¢˜â€œ**{topic}**â€çš„ä¸¤ä»½æ¥è‡ªä¸åŒä¸“å®¶çš„çŸ¥è¯†ç‚¹åˆ—è¡¨ï¼š
1.  **å¤§å­¦æ•™æˆçš„åˆ—è¡¨**: ä¾§é‡ç†è®ºã€æ¦‚å¿µå’ŒçŸ¥è¯†ä½“ç³»ã€‚
2.  **è¡Œä¸šå·¥ç¨‹å¸ˆçš„åˆ—è¡¨**: ä¾§é‡å®è·µã€å·¥å…·å’Œä¸€çº¿ç»éªŒã€‚

ä½ çš„ä»»åŠ¡æ˜¯**ç»¼åˆè¿™ä¸¤ä»½åˆ—è¡¨**ï¼Œè®¾è®¡ä¸€æ¡ä»ç†è®ºåˆ°å®è·µã€å¾ªåºæ¸è¿›çš„å­¦ä¹ è·¯å¾„ã€‚ä½ éœ€è¦å°†æ‰€æœ‰çŸ¥è¯†ç‚¹ï¼ˆç†è®º+å®è·µï¼‰èšç±»æˆ {target_chapters[0]} åˆ° {target_chapters[1]} ä¸ªé€»è¾‘ä¸Šè¿è´¯çš„æ ¸å¿ƒç« èŠ‚ï¼Œå¹¶ä¸ºæ¯ä¸ªç« èŠ‚æ’°å†™æ ‡é¢˜å’Œå­¦ä¹ ç›®æ ‡ã€‚

**å…³é”®åŸåˆ™**:
- **èåˆç†è®ºä¸å®è·µ**: è®¾è®¡çš„ç« èŠ‚åº”è¯¥è‡ªç„¶åœ°å°†ç†è®ºå’Œå®è·µç»“åˆèµ·æ¥ï¼Œè€Œä¸æ˜¯å°†å®ƒä»¬å‰²è£‚ã€‚
- **ç¬¦åˆå­¦ä¹ è®¤çŸ¥è§„å¾‹**: ä»ä¸ºä»€ä¹ˆå­¦ã€å­¦ä»€ä¹ˆã€æ€ä¹ˆå­¦çš„è§’åº¦å‡ºå‘ï¼Œç¡®ä¿ç« èŠ‚å®‰æ’ç¬¦åˆäººç±»è®¤çŸ¥è§„å¾‹ã€‚
- **ç»å¯¹çš„å¾ªåºæ¸è¿›**: å¿…é¡»ä»æœ€ç®€å•ã€æœ€æ ¸å¿ƒçš„æ¦‚å¿µè®²èµ·ã€‚è¿™æ˜¯æœ€é‡è¦çš„åŸåˆ™ã€‚

**è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰**:
- ä»…è¾“å‡ºä¸€ä¸ª JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ ä»£è¡¨ä¸€ä¸ªç« èŠ‚ï¼ŒåŒ…å« "title" ä¸ "objective" ä¸¤ä¸ªå­—æ®µã€‚
- ç¦æ­¢ä»»ä½•é JSON å†…å®¹ã€‚

**è¾“å…¥åˆ—è¡¨**:

**1. æ•™æˆçš„çŸ¥è¯†ç‚¹ (ç†è®ºè§†è§’)**:
```json
{json.dumps(professor_seeds, ensure_ascii=False, indent=2)[:3000]}
```

**2. å·¥ç¨‹å¸ˆçš„çŸ¥è¯†ç‚¹ (å®è·µè§†è§’)**:
```json
{json.dumps(engineer_seeds, ensure_ascii=False, indent=2)[:3000]}
```

è¯·æ ¹æ®ä»¥ä¸Šè¦æ±‚ï¼Œä¸ºå½“å‰ä¸»é¢˜â€œ{topic}â€è®¾è®¡ç« èŠ‚è§„åˆ’ï¼Œå¹¶æœ€ç»ˆåªè¾“å‡ºä¸€ä¸ªçº¯ JSON æ•°ç»„ã€‚
"""

def stage2_structure(
    caller: LLMCaller, 
    topic: str, 
    professor_seeds: List[Dict[str, Any]],
    engineer_seeds: List[Dict[str, Any]],
    depth: str, 
    language: str, 
    out_dir: Path, 
    basename: str, 
    max_tokens: int
) -> Dict[str, Any]:
    """
    Stage 2: Synthesize perspectives and create macro-structure.
    """
    path = out_dir / f"{basename}.skeleton.json"
    prompt = build_macro_planner_prompt_from_perspectives(
        topic, professor_seeds, engineer_seeds, depth, language
    )
    raw = caller.chat_json(prompt, max_tokens=max_tokens)
    
    # Enhanced parsing logic
    chapters_data = []
    parsed = extract_json(raw)

    if isinstance(parsed, list):
        chapters_data = parsed
    elif isinstance(parsed, dict):
        for key in ["chapters", "outline", "plan", "structure"]:
            if isinstance(parsed.get(key), list):
                chapters_data = parsed[key]
                break
    
    if not chapters_data and isinstance(raw, str):
        print("[è­¦å‘Š] Stage 2 è¿”å›æ ¼å¼éæ ‡å‡†æˆ–è§£æå¤±è´¥ï¼Œå°è¯•åœ¨æ–‡æœ¬ä¸­æœç´¢JSONæ•°ç»„...", file=sys.stderr)
        m = re.search(r"```(?:json|JSON)?\s*(\[[\s\S]*?\])\s*```", raw)
        if m:
            try:
                candidate = json.loads(m.group(1))
                if isinstance(candidate, list):
                    chapters_data = candidate
            except Exception: pass
        if not chapters_data:
            for frag in _iter_balanced_fragments(raw, "[", "]"):
                try:
                    candidate = json.loads(frag)
                    if isinstance(candidate, list):
                        chapters_data = candidate
                        break
                except Exception: continue
        if not chapters_data:
            chapters_data = []

    for i, ch in enumerate(chapters_data):
        if isinstance(ch, dict):
             ch['id'] = ch.get('id') or f"temp-ch-{i+1}"

    # 2.2 ä¸ºæ¯ä¸ªç« èŠ‚é€‰æ‹©ç›¸å…³ seeds
    def build_chapter_seed_selector_prompt(ch_title: str, ch_objective: str, seed_list: List[Dict[str, Any]]) -> str:
        listed = [{"id": i + 1, "text": (s.get("text") or "")} for i, s in enumerate(seed_list)]
        seeds_json = json.dumps(listed, ensure_ascii=False)
        return f"""
ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯åˆ†ç±»åŠ©æ‰‹ã€‚ä¸‹é¢ç»™å‡ºä¸€ç»„â€œçŸ¥è¯†ç‚¹â€åˆ—è¡¨ï¼ˆseedsï¼Œå·²æŒ‰ id ç¼–å·ï¼‰ï¼Œä»¥åŠæŸä¸€ç« èŠ‚çš„æ ‡é¢˜ä¸å­¦ä¹ ç›®æ ‡ã€‚
ä»»åŠ¡ï¼šä»åˆ—è¡¨ä¸­é€‰å‡ºä¸è¯¥ç« èŠ‚æœ€ç›¸å…³çš„ 15ï½20 ä¸ªçŸ¥è¯†ç‚¹ã€‚
è¦æ±‚ï¼š
- åªè¾“å‡ºä¸€ä¸ª JSON æ•°ç»„ï¼Œæ•°ç»„å…ƒç´ æ˜¯è¢«é€‰ä¸­çš„ seed çš„ idï¼ˆæ•´æ•°ï¼‰ã€‚
- æ•°é‡æ§åˆ¶åœ¨ 15ï½20 ä¸ªä¹‹é—´ï¼ŒæŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½æ’åºã€‚
- æ¯ä¸ª id åªèƒ½å‡ºç°ä¸€æ¬¡ï¼Œç¦æ­¢é‡å¤ã€‚
- ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ€§æ–‡å­—æˆ– Markdownï¼Œåªè¾“å‡ºçº¯ JSON æ•°ç»„ã€‚
ã€ç« èŠ‚ä¿¡æ¯ã€‘
- æ ‡é¢˜: {ch_title}
- å­¦ä¹ ç›®æ ‡: {ch_objective}
ã€å€™é€‰ Seedsï¼ˆç¤ºä¾‹å­—æ®µä»…å« id ä¸ textï¼‰ã€‘
```json
{seeds_json}
```
è¯·åªè¾“å‡ºæœ€ç»ˆçš„ id æ•°ç»„ï¼Œä¾‹å¦‚ï¼š [3, 8, 12, ...]
"""

    # Combine seeds for the selection step
    seed_list = professor_seeds + engineer_seeds
    if chapters_data and seed_list:
        print("[é˜¶æ®µ 2/4] 2.2 æ­£åœ¨ä¸ºæ¯ä¸ªç« èŠ‚é€‰æ‹©ç›¸å…³ seedsï¼ˆå¹¶è¡Œï¼‰...")

        def select_for_chapter(idx: int, ch: Dict[str, Any]) -> Tuple[int, List[Dict[str, Any]]]:
            title = ch.get("title") or f"ç¬¬{idx}ç« "
            obj = ch.get("objective") or ""
            sel_prompt = build_chapter_seed_selector_prompt(title, obj, seed_list)
            raw_sel = caller.chat_json(sel_prompt, max_tokens=min(max_tokens, 1024))
            selected_ids: List[int] = []
            parsed = extract_json(raw_sel)
            def sanitize_to_ids(p) -> List[int]:
                ids: List[int] = []
                if isinstance(p, list):
                    for item in p:
                        if isinstance(item, int): ids.append(item)
                        elif isinstance(item, dict):
                            v = item.get("id") or item.get("idx") or item.get("seed_id")
                            if isinstance(v, int): ids.append(v)
                elif isinstance(p, dict):
                    for k in ("ids", "indices", "selected", "seed_ids"):
                        v = p.get(k)
                        if isinstance(v, list): ids.extend([x for x in v if isinstance(x, int)])
                return ids
            selected_ids = sanitize_to_ids(parsed)

            if not selected_ids:
                key = (title or "") + " " + (obj or "")
                key = key.strip()
                ranked = []
                for i, s in enumerate(seed_list, start=1):
                    t = s.get("text") or ""
                    score = 0
                    for part in re.split(r"\s+", key):
                        if part and part in t: score += 1
                    ranked.append((score, i))
                ranked.sort(key=lambda x: (-x[0], x[1]))
                selected_ids = [i for (_, i) in ranked[:18]]

            seen = set()
            final_ids: List[int] = []
            for i in selected_ids:
                if isinstance(i, int) and 1 <= i <= len(seed_list) and i not in seen:
                    final_ids.append(i)
                    seen.add(i)
                if len(final_ids) >= 20: break
            if len(final_ids) < 15:
                for j in range(1, len(seed_list) + 1):
                    if j not in seen:
                        final_ids.append(j)
                        seen.add(j)
                    if len(final_ids) >= 15: break

            selected_seeds = [seed_list[i - 1] for i in final_ids]
            return idx, selected_seeds

        results_sel: Dict[int, List[Dict[str, Any]]] = {}
        max_workers = min(8, max(1, len(chapters_data)))
        with cf.ThreadPoolExecutor(max_workers=max_workers) as ex:
            futures = [ex.submit(select_for_chapter, i, ch) for i, ch in enumerate(chapters_data, start=1)]
            for fut in cf.as_completed(futures):
                idx, selected_seeds = fut.result()
                results_sel[idx] = selected_seeds
        for i, ch in enumerate(chapters_data, start=1):
            ch["selected_seeds"] = results_sel.get(i, [])

    data = {"chapters": chapters_data}
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
    return data


# -----------------------------
# é˜¶æ®µ 3ï¼šé€’å½’æ·±åŒ–ï¼ˆä¸ºæ¯ç« ç”Ÿæˆå°èŠ‚ä¸çŸ¥è¯†ç‚¹ï¼‰+ åŸå‹åˆ¤åˆ«
# -----------------------------


def build_chapter_architect_prompt(
    chapter_title: str,
    chapter_objective: str,
    full_skeleton: List[Dict[str, Any]],
    current_index: int,
) -> str:
    """æ„å»ºç« èŠ‚æ¶æ„å¸ˆï¼ˆStage 3ï¼‰çš„One-Shot Promptã€‚

    æ–°å¢ï¼šæä¾›å®Œæ•´ç« èŠ‚éª¨æ¶ï¼ˆStage 2 è¾“å‡ºï¼‰ä½œä¸ºä¸Šæ–‡ä¸Šä¸‹æ–‡ï¼Œé¿å…è·¨ç« é‡å¤ï¼Œä¿æŒå…¨å±€è¿è´¯ã€‚
    current_index ä» 1 å¼€å§‹ï¼Œå¯¹åº”å½“å‰ç« èŠ‚åœ¨éª¨æ¶ä¸­çš„ä½ç½®ã€‚
    """
    
    # The full one-shot prompt as a multi-line f-string
    return f"""### 1. è§’è‰²ä¸ç›®æ ‡ (Role and Goal)

**è§’è‰²:** ä½ æ˜¯ä¸€ä½ä¸–ç•Œé¡¶çº§çš„è¯¾ç¨‹è®¾è®¡å¸ˆå’Œæ•™å­¦æ¶æ„å¸ˆã€‚ä½ çš„ä¸“é•¿æ˜¯å°†ä»»ä½•é¢†åŸŸå†…ä¸€ç»„é›¶æ•£çš„çŸ¥è¯†ç‚¹ï¼Œé‡æ„æˆä¸€ä¸ªç¬¦åˆäººç±»è®¤çŸ¥è§„å¾‹ã€å™äº‹æµç•…ã€ç»“æ„æ¸…æ™°çš„â€œæœ€ä½³å­¦ä¹ è·¯å¾„â€ã€‚


**æ ¸å¿ƒç›®æ ‡:** å°†ç”¨æˆ·æä¾›çš„ç« èŠ‚æ ‡é¢˜ã€ç›®æ ‡å’Œç›¸å…³çŸ¥è¯†ç‚¹åˆ—è¡¨ï¼Œè½¬åŒ–ä¸ºä¸€ä¸ªå¸¦æœ‰ä¸°å¯Œè¯­ä¹‰å…ƒæ•°æ® (`structure_type`, `relation_to_previous`, `primary_goal`, `suggested_modules`) çš„ã€ç»“æ„åŒ–çš„å•ç« èŠ‚è¯¦ç»†JSONå¤§çº²ã€‚

### 2. æ ¸å¿ƒæ¦‚å¿µå®šä¹‰ (Core Concepts Definition)

åœ¨ä½ çš„è®¾è®¡ä¸­ï¼Œå¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹ä¸¤ç§åŸå­ç»“æ„æ¨¡å‹ï¼š

*   **æµæ°´çº¿ (Pipeline):**
    *   **å®šä¹‰:** ä¸€ç³»åˆ—å…·æœ‰**å¼ºæ—¶åºæ€§æˆ–å¼ºä¾èµ–æ€§**çš„çŸ¥è¯†ç‚¹ã€‚å®ƒä»¬é€šå¸¸æè¿°ä¸€ä¸ªè¿ç»­çš„è¿‡ç¨‹ã€å·¥ä½œæµæˆ–é€»è¾‘æ¨æ¼”ã€‚
    *   **å…³é”®ç‰¹å¾:** å‰ä¸€ä¸ªçŸ¥è¯†ç‚¹çš„**è¾“å‡º**æ˜¯åä¸€ä¸ªçŸ¥è¯†ç‚¹çš„**è¾“å…¥**ã€‚å­¦ä¹ é¡ºåº**å‡ ä¹ä¸å¯æ›´æ”¹**ã€‚
    *   **ä¾‹å­:** æ–‡æœ¬é¢„å¤„ç†æµç¨‹ã€æ•°å­¦å®šç†çš„è¯æ˜æ­¥éª¤ã€ä¸€ä¸ªç®—æ³•çš„æ‰§è¡Œè¿‡ç¨‹ã€‚

*   **å·¥å…·ç®± (Toolbox):**
    *   **å®šä¹‰:** ä¸€ç»„å›´ç»•**å…±åŒä¸»é¢˜æˆ–ç›®æ ‡**ï¼Œä½†å½¼æ­¤**ç›¸å¯¹ç‹¬ç«‹**çš„çŸ¥è¯†ç‚¹ã€‚
    *   **å…³é”®ç‰¹å¾:** çŸ¥è¯†ç‚¹ä¹‹é—´æ²¡æœ‰ä¸¥æ ¼çš„é¡ºåºä¾èµ–ï¼Œå¯ä»¥å¹¶è¡Œå­¦ä¹ æˆ–æŒ‰ä»»æ„é¡ºåºå­¦ä¹ ã€‚å®ƒä»¬æ˜¯è§£å†³ç›¸å…³é—®é¢˜çš„ä¸åŒæ–¹æ³•ã€å·¥å…·æˆ–æ¦‚å¿µã€‚
    *   **ä¾‹å­:** Pythonçš„å„ç§æ•°æ®ç»“æ„ã€æœºå™¨å­¦ä¹ çš„å„ç§åˆ†ç±»ç®—æ³•ã€CSSçš„å„ç§é€‰æ‹©å™¨ã€‚

### 3. æ‰§è¡Œæ­¥éª¤ (Execution Steps)

1.  **åˆ†æä¸åˆ†ç»„:** ä»”ç»†é˜…è¯»è¾“å…¥çš„çŸ¥è¯†ç‚¹åˆ—è¡¨ã€‚è¯†åˆ«å‡ºå“ªäº›çŸ¥è¯†ç‚¹å¯ä»¥ä¸²è”æˆâ€œæµæ°´çº¿â€ï¼Œå“ªäº›å¯ä»¥å½’ç±»åˆ°ä¸åŒçš„â€œå·¥å…·ç®±â€ä¸­ï¼Œå½¢æˆå°èŠ‚(Group)ã€‚
2.  **è®¾è®¡å¾®è§‚ç»“æ„:** åœ¨æ¯ä¸ªå°èŠ‚å†…éƒ¨ï¼Œæ’åˆ—çŸ¥è¯†ç‚¹ï¼ˆSectionï¼‰çš„é¡ºåºï¼Œç¡®ä¿é€»è¾‘é€šé¡ºã€‚
3.  **ç²¾ç‚¼æ ‡é¢˜:** ä¸ºæ¯ä¸ªå°èŠ‚å’ŒçŸ¥è¯†ç‚¹æ’°å†™æ¸…æ™°ã€ç®€æ´ä¸”å…·æœ‰å¼•å¯¼æ€§çš„æ ‡é¢˜ã€‚
4.  **æ³¨å…¥å…ƒæ•°æ® (å…³é”®):** åœ¨æœ€ç»ˆçš„JSONç»“æ„ä¸­ï¼Œå¿…é¡»ä¸ºæ¯ä¸ªå°èŠ‚ï¼ˆGroupï¼‰å’ŒçŸ¥è¯†ç‚¹ï¼ˆSectionï¼‰æ·»åŠ ä»¥ä¸‹å…ƒæ•°æ®ï¼š
    *   ä¸ºæ¯ä¸ª**å°èŠ‚ (Group)** æ·»åŠ  `structure_type: "pipeline" | "toolbox"` å­—æ®µã€‚
    *   ä¸ºæ¯ä¸ª**çŸ¥è¯†ç‚¹ (Section)** æ·»åŠ ï¼š
        - `relation_to_previous`: `builds_on | tool_in_toolbox | alternative_to | deep_dive_into | first_in_sequence`
        - `primary_goal`: ç”¨ä¸€å¥è¯æ¸…æ™°åœ°å®šä¹‰è¯¥çŸ¥è¯†ç‚¹çš„**æ ¸å¿ƒå†…å®¹ç›®æ ‡**ã€‚å®ƒåº”ç²¾å‡†æè¿°â€œè¿™èŠ‚å†…å®¹éœ€è¦è®²æ¸…æ¥šä»€ä¹ˆæ ¸å¿ƒé—®é¢˜â€æˆ–â€œå®ƒè¦ä»ä»€ä¹ˆè§’åº¦å»å†™â€ï¼Œè€Œä¸æ˜¯å®šä¹‰å­¦ä¹ è€…çš„èƒ½åŠ›ç›®æ ‡ã€‚
            - **åä¾‹ (Bad)**: "å­¦ä¹ æ•°æ®æ¸…æ´—" (è¿‡äºå®½æ³›ï¼Œæ˜¯å­¦ä¹ ç›®æ ‡)
            - **æ­£ä¾‹ (Good)**: "ä»‹ç»æ•°æ®æ¸…æ´—ä½œä¸ºé¢„å¤„ç†æµç¨‹ç¬¬ä¸€æ­¥çš„æ ¸å¿ƒä»»åŠ¡ï¼Œå¹¶å±•ç¤ºå¸¸è§çš„æ¸…æ´—æŠ€æœ¯ã€‚" (æ˜ç¡®äº†å†…å®¹èŒƒå›´å’Œä»»åŠ¡ï¼Œæ˜¯å†…å®¹ç›®æ ‡)
        - `suggested_modules`: åœ¨æ­£å¸¸çš„æ–‡å­—é˜è¿°ä¹‹å¤–ï¼Œå¯ä»¥é¢å¤–ä½¿ç”¨çš„**å¢å¼ºè¡¨è¾¾å½¢å¼**æ¸…å•ï¼ˆæ•°ç»„ï¼Œå…è®¸ä»ä»¥ä¸‹æšä¸¾ä¸­æŒ‘é€‰ï¼‰ï¼š
            `["code_example", "common_mistake_warning", "mermaid diagram", "checklist", "comparison", "case_study"]`
5.  **æ ¼å¼åŒ–è¾“å‡º:** ç¡®ä¿æœ€ç»ˆè¾“å‡ºæ˜¯ä¸€ä¸ªç»“æ„ä¸¥è°¨ã€æ ¼å¼æ­£ç¡®çš„å•ä¸€JSONå¯¹è±¡ï¼Œä»£è¡¨è¿™ä¸€ä¸ªç« èŠ‚çš„å®Œæ•´ç»“æ„ã€‚
                        
### 4. é«˜è´¨é‡èŒƒä¾‹ (One-Shot Example)

**è¾“å…¥èŒƒä¾‹:**
*   **ç« èŠ‚æ ‡é¢˜:** "ç¬¬ä¸€ç« ï¼šåŸºç¡€ç¯‡ Â· è®©æœºå™¨è¯»æ‡‚è¯­è¨€"
*   **ç« èŠ‚å­¦ä¹ ç›®æ ‡:** "å­¦ä¹ å¦‚ä½•å°†åŸå§‹æ–‡æœ¬è½¬åŒ–ä¸ºæœºå™¨å¯ä»¥å¤„ç†çš„ç»“æ„åŒ–æ•°æ®ï¼Œå¹¶äº†è§£ä¸åŒçš„æ–‡æœ¬è¡¨ç¤ºæ–¹æ³•ã€‚"
*   **ç›¸å…³çŸ¥è¯†ç‚¹Seeds:** `["æ–‡æœ¬æ¸…æ´—", "åˆ†è¯", "åœç”¨è¯ç§»é™¤", "è¯å½¢å½’ä¸€åŒ–", "è¯è¢‹æ¨¡å‹", "TF-IDF", "Word2Vec"]`

**è¾“å‡ºèŒƒä¾‹ (JSON):**
```json
{{
    "title": "ç¬¬ä¸€ç« ï¼šåŸºç¡€ç¯‡ Â· è®©æœºå™¨è¯»æ‡‚è¯­è¨€",
    "id": "nlp-ch-1",
    "groups": [
    {{
        "title": "1.1 æ–‡æœ¬é¢„å¤„ç†ï¼šä»åŸå§‹è¯­æ–™åˆ°ç»“æ„åŒ–æ•°æ®",
        "id": "nlp-gr-1-1",
        "structure_type": "pipeline",
        "sections": [
        {{
            "title": "1.1.1 èµ·å§‹ç‚¹ï¼šé¢„å¤„ç†çš„é‡è¦æ€§",
            "id": "nlp-sec-1-1-1",
            "relation_to_previous": "first_in_sequence",
            "primary_goal": "è§£é‡ŠåŸå§‹æ–‡æœ¬ä¸­å­˜åœ¨çš„æ­§ä¹‰æ€§ã€éç»“æ„åŒ–ç­‰é—®é¢˜ï¼Œé˜æ˜æœºå™¨è¿›è¡Œè‡ªç„¶è¯­è¨€ç†è§£å‰å¿…é¡»è¿›è¡Œé¢„å¤„ç†çš„åŸå› ã€‚",
            "suggested_modules": ["diagram", "case_study"]
        }},
        {{
            "title": "1.1.2 ç¬¬ä¸€æ­¥ï¼šåŸºç¡€æ¸…æ´—",
            "id": "nlp-sec-1-1-2",
            "relation_to_previous": "builds_on",
            "primary_goal": "ä»‹ç»æ–‡æœ¬é¢„å¤„ç†æµç¨‹ä¸­çš„ç¬¬ä¸€ä¸ªå…·ä½“æ­¥éª¤â€”â€”æ•°æ®æ¸…æ´—ï¼Œå¹¶åˆ—ä¸¾å¸¸è§çš„æ¸…æ´—ç›®æ ‡ï¼ˆå¦‚ç§»é™¤HTMLæ ‡ç­¾ã€ç‰¹æ®Šå­—ç¬¦ç­‰ï¼‰ã€‚",
            "suggested_modules": ["checklist", "code_example", "common_mistake_warning"]
        }},
        {{
            "title": "1.1.3 ç¬¬äºŒæ­¥ï¼šæ–‡æœ¬åˆ‡åˆ† (Tokenization)",
            "id": "nlp-sec-1-1-3",
            "relation_to_previous": "builds_on",
            "primary_goal": "è®²è§£â€œåˆ†è¯â€çš„æ ¸å¿ƒæ¦‚å¿µï¼Œå¹¶å¯¹æ¯”ä¸åŒåˆ†è¯ç²’åº¦ï¼ˆå¦‚æŒ‰è¯ã€æŒ‰å­è¯ï¼‰çš„é€‚ç”¨åœºæ™¯ã€‚",
            "suggested_modules": ["code_example", "comparison"]
        }},
        {{
            "title": "1.1.4 ç¬¬ä¸‰æ­¥ï¼šç²¾ç‚¼è¯å…ƒ (åœç”¨è¯ç§»é™¤ä¸è¯å½¢å½’ä¸€åŒ–)",
            "id": "nlp-sec-1-1-4",
            "relation_to_previous": "builds_on",
            "primary_goal": "é˜è¿°åœç”¨è¯ç§»é™¤å’Œè¯å½¢å½’ä¸€åŒ–ï¼ˆè¯å¹²æå–/è¯å½¢è¿˜åŸï¼‰çš„ç­–ç•¥åŠå…¶å¯¹åç»­ä»»åŠ¡çš„å½±å“ã€‚",
            "suggested_modules": ["code_example", "common_mistake_warning"]
        }}
        ]
    }},
    {{
        "title": "1.2 æ–‡æœ¬è¡¨ç¤ºæ–¹æ³•ï¼šå°†è¯è¯­å˜ä¸ºå‘é‡",
        "id": "nlp-gr-1-2",
        "structure_type": "toolbox",
        "sections": [
        {{
            "title": "1.2.1 æ ¸å¿ƒæ€æƒ³ï¼šä¸ºä½•éœ€è¦æ–‡æœ¬è¡¨ç¤º",
            "id": "nlp-sec-1-2-1",
            "relation_to_previous": "first_in_sequence",
            "primary_goal": "é˜è¿°è®¡ç®—æœºæ— æ³•ç›´æ¥å¤„ç†æ–‡æœ¬ï¼Œå¿…é¡»å°†å…¶æ•°å€¼åŒ–ï¼ˆå‘é‡åŒ–ï¼‰çš„æ ¸å¿ƒåŸå› ã€‚",
            "suggested_modules": ["diagram"]
        }},
        {{
            "title": "1.2.2 å·¥å…·ä¸€ (ç¦»æ•£è¡¨ç¤º)ï¼šè¯è¢‹æ¨¡å‹ä¸TF-IDF",
            "id": "nlp-sec-1-2-2",
            "relation_to_previous": "tool_in_toolbox",
            "primary_goal": "å®Œæ•´ä»‹ç»è¯è¢‹æ¨¡å‹å’ŒTF-IDFçš„æ ¸å¿ƒæ€æƒ³ã€æ„å»ºæ­¥éª¤ã€ä»¥åŠä¸¤è€…ä½œä¸ºç¦»æ•£è¡¨ç¤ºæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚",
            "suggested_modules": ["code_example", "comparison", "exercise"]
        }},
        {{
            "title": "1.2.3 å·¥å…·äºŒ (åˆ†å¸ƒå¼è¡¨ç¤º)ï¼šè¯å‘é‡",
            "id": "nlp-sec-1-2-3",
            "relation_to_previous": "tool_in_toolbox",
            "primary_goal": "ä»‹ç»è¯åµŒå…¥ï¼ˆWord Embeddingï¼‰ä½œä¸ºåˆ†å¸ƒå¼è¡¨ç¤ºçš„æ ¸å¿ƒæ€æƒ³ï¼Œå¹¶è§£é‡Šå…¶å¦‚ä½•é€šè¿‡å‘é‡ç©ºé—´æ•æ‰è¯æ±‡çš„è¯­ä¹‰å…³ç³»ã€‚",
            "suggested_modules": ["code_example", "case_study"]
        }}
        ]
    }}
    ]
}}
```---
**ã€å…¨å±€ç« èŠ‚éª¨æ¶æ¦‚è§ˆï¼ˆæ¥è‡ª Stage 2ï¼‰ã€‘**

ä¸ºç¡®ä¿ä¸ä¸å…¶å®ƒç« èŠ‚å†…å®¹é‡å¤ï¼Œå¹¶ä¿æŒå…¨å±€ä¸€è‡´æ€§ï¼Œä»¥ä¸‹æ˜¯æ•´æœ¬è¯¾ç¨‹çš„ç« èŠ‚éª¨æ¶ï¼ˆåªå«æ ‡é¢˜ä¸å­¦ä¹ ç›®æ ‡ï¼‰ã€‚è¯·åœ¨è®¾è®¡å½“å‰ç« èŠ‚æ—¶å‚è€ƒè¯¥å…¨å±€ç»“æ„ï¼š

```json
{json.dumps([
    {"index": i+1, "title": ch.get("title"), "objective": ch.get("objective")}
    for i, ch in enumerate(full_skeleton)
], ensure_ascii=False)}
```

å½“å‰å¤„ç†çš„ç« èŠ‚åºå·ï¼ˆä»1å¼€å§‹ï¼‰: {current_index}

è¯·ä¸¥æ ¼é¿å…ä¸å…¶å®ƒç« èŠ‚æ ‡é¢˜/ç›®æ ‡é‡å ï¼Œé¿å…é‡å¤è¦†ç›–å…¶å®ƒç« èŠ‚åº”è®²è¿°çš„å†…å®¹ï¼›å¦‚éœ€æ‰¿æ¥æˆ–å¼•ç”¨å…¶å®ƒç« èŠ‚ï¼Œè¯·ä»¥â€œæ‰¿æ¥ç¬¬Xç«  ...â€çš„æ–¹å¼åœ¨å°èŠ‚è¯´æ˜ä¸­ä½“ç°ã€‚

**ã€å½“å‰ä»»åŠ¡ã€‘**

**ç« èŠ‚æ ‡é¢˜:** `{chapter_title}`

**ç« èŠ‚å­¦ä¹ ç›®æ ‡:** `{chapter_objective}`

è¯·æ ¹æ®ä»¥ä¸Šè¦æ±‚ï¼Œä¸º**å½“å‰ä»»åŠ¡**çš„ç« èŠ‚è®¾è®¡è¯¦ç»†çš„å†…éƒ¨ç»“æ„ï¼Œå¹¶ä»¥JSONæ ¼å¼è¾“å‡ºã€‚
"""

def stage3_detail_and_classify(
    caller: LLMCaller,
    topic: str,
    skeleton: Dict[str, Any],
    seeds: Dict[str, Any],
    out_dir: Path,
    basename: str,
    max_tokens: int,
    max_parallel: int,
) -> Dict[str, Any]:
    """ Stage 3: ç« èŠ‚æ¶æ„å¸ˆï¼Œå¾ªç¯ä¸ºæ¯ä¸ªç« èŠ‚è®¾è®¡å¾®è§‚ç»“æ„ã€‚"""
    chapters = skeleton.get("chapters") or []
    detailed_chapters: List[Dict[str, Any]] = []

    print("[é˜¶æ®µ 3/4] å¼€å§‹è°ƒç”¨ç« èŠ‚æ¶æ„å¸ˆï¼Œé€ç« è®¾è®¡å¾®è§‚ç»“æ„...")

    def design_one_chapter(idx: int, ch_info: Dict[str, Any]) -> Tuple[int, Dict[str, Any]]:
        ch_title = ch_info.get("title") or f"ç¬¬{idx}ç« "
        ch_objective = ch_info.get("objective") or ""
        print(f"  -> æ­£åœ¨ä¸ºç« èŠ‚ '{ch_title}' ({idx}/{len(chapters)}) è®¾è®¡ç»“æ„...")
        
        prompt = build_chapter_architect_prompt(
            chapter_title=ch_title,
            chapter_objective=ch_objective,
            full_skeleton=chapters,
            current_index=idx,
        )
        raw = caller.chat_json(prompt, max_tokens=max_tokens)
        
        # è§£æLLMè¿”å›çš„å•ç« èŠ‚JSON
        detailed_chapter_json = extract_json(raw)
        
        if not detailed_chapter_json or not isinstance(detailed_chapter_json, dict):
            print(f"    [è­¦å‘Š] è§£æç« èŠ‚ '{ch_title}' çš„ç»“æ„å¤±è´¥ï¼Œå°†ä½¿ç”¨ç©ºç»“æ„ã€‚", file=sys.stderr)
            return idx, {"title": ch_title, "id": ch_info.get('id'), "groups": []}

        # ç¡®ä¿é¡¶å±‚ä¿¡æ¯ä¸€è‡´
        detailed_chapter_json['title'] = ch_title
        detailed_chapter_json['id'] = ch_info.get('id')
        detailed_chapter_json['objective'] = ch_objective

        return idx, detailed_chapter_json

    if chapters:
        results: Dict[int, Dict[str, Any]] = {}
        with cf.ThreadPoolExecutor(max_workers=max_parallel) as ex:
            futures = [ex.submit(design_one_chapter, i, ch) for i, ch in enumerate(chapters, start=1)]
            for fut in cf.as_completed(futures):
                idx, detailed_ch = fut.result()
                results[idx] = detailed_ch
        detailed_chapters = [results[i] for i in range(1, len(chapters) + 1)]

    print("  -> æ‰€æœ‰ç« èŠ‚çš„å¾®è§‚ç»“æ„è®¾è®¡å®Œæˆã€‚ ")
    final_outline = {"chapters": detailed_chapters}
    (out_dir / f"{basename}.outline.stage3.json").write_text(json.dumps(final_outline, ensure_ascii=False, indent=2), encoding="utf-8")
    return final_outline


# -----------------------------
# ç»Ÿä¸€è¡¥é»˜è®¤ + ç”Ÿæˆç¨³å®š ID + æ¸²æŸ“ Markdown
# -----------------------------


def apply_defaults_and_ids(topic: str, data: Dict[str, Any]) -> Dict[str, Any]:
    # å…è®¸ä¸Šæ¸¸ä¼ å…¥ meta.topic_slugï¼ˆä¾‹å¦‚å¤–éƒ¨å·²ç”Ÿæˆå¯è¯»çš„è‹±æ–‡slugï¼‰
    pre_slug = (data.get("meta") or {}).get("topic_slug") if isinstance(data.get("meta"), dict) else None
    topic_slug = pre_slug or slugify(topic)
    data.setdefault("meta", {})
    data["meta"].update({"topic": topic, "topic_slug": topic_slug, "schema_version": "pipeline.v1"})
    chapters = data.get("chapters") or []
    for ci, ch in enumerate(chapters, start=1):
        ch_id = f"{topic_slug}-ch-{ci}"
        ch["id"] = ch_id
        groups = ch.get("groups") or []
        for gi, gr in enumerate(groups, start=1):
            gr_id = f"{topic_slug}-gr-{ci}-{gi}"
            gr["id"] = gr_id
            sections = gr.get("sections") or []
            diff_order = {"beginner": 0, "intermediate": 1, "expert": 2}
            try:
                sections.sort(key=lambda s: diff_order.get((s.get("difficulty") or "intermediate"), 1))
            except Exception:
                pass
            for si, sec in enumerate(sections, start=1):
                title = sec.get("title") or f"Section {ci}.{gi}.{si}"
                sec_slug = slugify(title, fallback=f"sec-{ci}-{gi}-{si}")
                sec_id = f"{topic_slug}-sec-{ci}-{gi}-{si}-{sec_slug}"
                sec["id"] = sec_id
                # ç»Ÿä¸€é‡‡ç”¨ primary_goal + suggested_modules çš„è½¯å»ºè®®æ¨¡å¼
                if "primary_goal" not in sec:
                    # å›é€€å…¼å®¹æ—§å­—æ®µ goal
                    if "goal" in sec:
                        sec["primary_goal"] = sec.get("goal")
                if not isinstance(sec.get("suggested_modules"), list):
                    sec["suggested_modules"] = []
    return data


def render_markdown(data: Dict[str, Any]) -> str:
    topic = data.get("meta", {}).get("topic", "ä¸»é¢˜")
    topic_slug = data.get("meta", {}).get("topic_slug", slugify(topic))
    lines: List[str] = []
    lines.append(f"# {topic} (id: {topic_slug})")
    chapters = data.get("chapters") or []
    for ci, ch in enumerate(chapters, start=1):
        ch_title = ch.get("title") or f"ç¬¬{ci}ç« "
        lines.append("")
        lines.append(f"## ç¬¬{ci}ç« ï¼š{ch_title} (id: {ch.get('id')})")
        groups = ch.get("groups") or []
        for gi, gr in enumerate(groups, start=1):
            gr_title = gr.get("title") or f"{ci}.{gi} å°èŠ‚"
            lines.append(f"### {ci}.{gi} {gr_title} (id: {gr.get('id')})")
            sections = gr.get("sections") or []
            for si, sec in enumerate(sections, start=1):
                s_title = sec.get("title") or f"{ci}.{gi}.{si} çŸ¥è¯†ç‚¹"
                # å¦‚æœæ ‡é¢˜å·²åŒ…å«å½¢å¦‚ â€œX.Yâ€ æˆ– â€œX.Y.Zâ€ çš„ç¼–å·å‰ç¼€ï¼Œåˆ™å»æ‰ï¼Œé¿å…ä¸æˆ‘ä»¬è®¡ç®—ç¼–å·é‡å¤
                try:
                    import re as _re
                    s_clean = _re.sub(r"^\s*\d+(?:\.\d+)*\s+", "", s_title)
                except Exception:
                    s_clean = s_title
                lines.append(f"#### {ci}.{gi}.{si} {s_clean} (id: {sec.get('id')})")
    return "\n".join(lines) + "\n"

def build_global_review_prompt(topic: str, data_with_ids: Dict[str, Any]) -> str:
    """æ„å»ºâ€œå…¨å±€è§†è§’â€çš„LLMå®¡é˜…æç¤ºï¼Œè¦æ±‚LLMåŒæ—¶è¾“å‡ºå®¡æ ¸æŠ¥å‘Šå’Œä¿®æ”¹åçš„å¤§çº²ã€‚"""
    
    outline_json_str = json.dumps(data_with_ids, ensure_ascii=False, indent=2)

    return f"""
ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„â€œè¯¾ç¨‹å¤§çº²æ€»ç¼–è¾‘â€ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„æ•™å­¦è®¾è®¡ç»éªŒå’Œæ‰¹åˆ¤æ€§æ€ç»´ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†ä¸¤æ­¥å®Œæˆå¯¹ä¸€ä¸ªè¯¾ç¨‹å¤§çº²çš„å®¡é˜…å’Œä¿®è®¢ã€‚

ã€è¾“å…¥ã€‘
å¾…å®¡é˜…çš„è¯¾ç¨‹å¤§çº²JSONï¼š
```json
{outline_json_str}
```

ã€ä»»åŠ¡ç¬¬ä¸€æ­¥ï¼šå†…éƒ¨å®¡é˜…ã€‘
è¯·åœ¨ä½ çš„â€œè„‘ä¸­â€ä¸¥æ ¼ä¾æ®ä»¥ä¸‹å…­å¤§â€œé»„é‡‘æ ‡å‡†â€å¯¹è¾“å…¥çš„å¤§çº²è¿›è¡Œæ·±å…¥åˆ†æï¼Œå¹¶æ„æ€å‡ºæ‰€æœ‰å¿…è¦çš„ä¿®æ”¹ã€‚

*å®¡é˜…æ ‡å‡† (é»„é‡‘æ ‡å‡†)*
1.  **é€»è¾‘é€’è¿›æ€§ (Logical Progression)**: çŸ¥è¯†ç‚¹çš„ç»„ç»‡é¡ºåºæ˜¯å¦ç¬¦åˆè®¤çŸ¥è§„å¾‹ï¼Ÿ
2.  **çŸ¥è¯†è„šæ‰‹æ¶ (Knowledge Scaffolding)**: å¤æ‚çš„åæœŸçŸ¥è¯†ç‚¹ï¼Œå…¶æ‰€éœ€çš„å‰ç½®åŸºç¡€æ¦‚å¿µæ˜¯å¦å·²é“ºå«ï¼Ÿ
3.  **å®Œæ•´æ€§ä¸è¦†ç›–åº¦ (Completeness & Coverage)**: æ˜¯å¦è¦†ç›–äº†æ‰€æœ‰å¿…è¦çš„çŸ¥è¯†é¢†åŸŸï¼Ÿæœ‰æ— å…³é”®ç›²åŒºï¼Ÿ
4.  **ç»“æ„å‡è¡¡æ€§ (Structural Balance)**: å„ç« èŠ‚çš„ä½“é‡æ˜¯å¦ä¸å…¶é‡è¦æ€§å¤§è‡´æˆæ­£æ¯”ï¼Ÿå‘½åæ˜¯å¦ç²¾å‡†ï¼Ÿ
5.  **å®ç”¨æ€§ä¸æ·±åº¦ (Practicality & Depth)**: æ˜¯å¦åŒ…å«å®è·µåº”ç”¨ï¼Œå¹¶æ·±å…¥åˆ°â€œä¸ºä»€ä¹ˆâ€çš„å±‚é¢ï¼Ÿ
6.  **å…ƒæ•°æ®åˆç†æ€§ (Metadata Suitability)**: `primary_goal` æ˜¯å¦ç²¾å‡†åˆ‡é¢˜ï¼Ÿ`suggested_modules` æ˜¯å¦åŒ¹é…ï¼Ÿ

ã€ä»»åŠ¡ç¬¬äºŒæ­¥ï¼šè¾“å‡ºç»“æœã€‘
å®Œæˆå†…éƒ¨å®¡é˜…å’Œæ„æ€åï¼Œè¯·å°†ä½ çš„å·¥ä½œæˆæœä»¥ä¸€ä¸ªå•ä¸€JSONå¯¹è±¡çš„æ ¼å¼è¾“å‡ºã€‚è¿™ä¸ªJSONå¯¹è±¡å¿…é¡»åŒ…å«ä¸¤ä¸ªé¡¶çº§é”®ï¼š`review_report` å’Œ `modified_outline`ã€‚

1.  `review_report`: è¿™æ˜¯ä½ ç»™äººç±»çœ‹çš„å®¡æ ¸æŠ¥å‘Šï¼Œè®°å½•äº†ä½ æ‰€åšçš„ä¸»è¦ä¿®æ”¹ã€‚è¯·éµå¾ªèŒƒä¾‹çš„æ ¼å¼ï¼Œæ¸…æ™°åœ°è¯´æ˜ä¿®æ”¹çš„ç±»å‹ã€ç†ç”±å’Œç»†èŠ‚ã€‚
2.  `modified_outline`: è¿™æ˜¯ä½ å°†æ‰€æœ‰ä¿®æ”¹å»ºè®®åº”ç”¨åˆ°åŸå§‹å¤§çº²åï¼Œäº§å‡ºçš„æœ€ç»ˆçš„ã€å®Œæ•´çš„ã€ç»“æ„ä¸è¾“å…¥å®Œå…¨ç›¸åŒçš„å¤§çº²JSONå¯¹è±¡ã€‚

ã€é«˜è´¨é‡èŒƒä¾‹ã€‘
*è¾“å…¥èŒƒä¾‹ (ä¸€ä¸ªæœ‰é—®é¢˜çš„â€œé¢åŒ…çƒ˜ç„™â€å¤§çº²JSON):*
```json
{{
  "meta": {{ "topic": "é¢åŒ…çƒ˜ç„™å…¥é—¨" }},
  "chapters": [
    {{ "title": "ç¬¬ä¸€ç« ï¼šåŸºç¡€åŸæ–™", "id": "bread-ch-1", "groups": [] }},
    {{ "title": "ç¬¬äºŒç« ï¼šé«˜çº§å¡‘å½¢æŠ€å·§", "id": "bread-ch-2", "groups": [] }},
    {{
      "title": "ç¬¬ä¸‰ç« ï¼šæ‰é¢", 
      "id": "bread-ch-3", 
      "groups": [{{
        "title": "3.1 å…³äºé…µæ¯çš„ä¸œè¥¿", 
        "id": "bread-gr-3-1",
        "sections": [{{ "title": "é…µæ¯", "id": "bread-sec-3-1-1", "primary_goal": "å­¦ä¹ é…µæ¯"}}]
      }}]
    }}
  ]
}}
```

*è¾“å‡ºèŒƒä¾‹ (åŒ…å«review_reportå’Œmodified_outlineçš„å®Œæ•´JSON):*
```json
{{
  "review_report": {{
    "overall_assessment": "æ•´ä½“ç»“æ„å­˜åœ¨é€»è¾‘é—®é¢˜ï¼Œä¸”æœ‰å…³é”®æ­¥éª¤ç¼ºå¤±ï¼Œéƒ¨åˆ†å‘½åå’Œç›®æ ‡è¿‡äºæ¨¡ç³Šã€‚",
    "suggested_changes": [
      {{
        "type": "å®è§‚ç»“æ„è°ƒæ•´ - ç« èŠ‚é‡æ’åº",
        "reasoning": "æ ¹æ®â€˜é€»è¾‘é€’è¿›æ€§â€™æ ‡å‡†ï¼Œâ€˜æ‰é¢â€™åº”åœ¨â€˜é«˜çº§å¡‘å½¢â€™ä¹‹å‰ã€‚",
        "change_detail": "äº¤æ¢äº†åŸç¬¬äºŒç« å’Œç¬¬ä¸‰ç« çš„é¡ºåºã€‚"
      }},
      {{
        "type": "å¾®è§‚å†…å®¹è°ƒæ•´ - é‡å‘½åä¸ç›®æ ‡ä¿®æ­£",
        "target_id": "bread-sec-3-1-1",
        "reasoning": "åŸå§‹å‘½åâ€˜å…³äºé…µæ¯çš„ä¸œè¥¿â€™å’Œç›®æ ‡â€˜å­¦ä¹ é…µæ¯â€™è¿‡äºæ¨¡ç³Šã€‚",
        "change_detail": "å°†æ ‡é¢˜ä¿®æ”¹ä¸ºâ€˜æ¿€æ´»é…µæ¯ï¼šå‘é…µæˆåŠŸçš„ç¬¬ä¸€æ­¥â€™ï¼Œå¹¶å…·ä½“åŒ–äº†primary_goalã€‚"
      }},
      {{
        "type": "å®Œæ•´æ€§è°ƒæ•´ - è¡¥å……",
        "reasoning": "ç¼ºå°‘äº†â€˜çƒ˜çƒ¤â€™è¿™ä¸€æ ¸å¿ƒç¯èŠ‚ã€‚",
        "change_detail": "åœ¨â€˜é«˜çº§å¡‘å½¢æŠ€å·§â€™ä¹‹åè¡¥å……äº†æ–°çš„ä¸€ç« â€˜ç¬¬å››ç« ï¼šçƒ˜çƒ¤çš„è‰ºæœ¯â€™ã€‚"
      }}
    ]
  }},
  "modified_outline": {{
    "meta": {{ "topic": "é¢åŒ…çƒ˜ç„™å…¥é—¨" }},
    "chapters": [
      {{ "title": "ç¬¬ä¸€ç« ï¼šåŸºç¡€åŸæ–™", "id": "bread-ch-1", "groups": [] }},
      {{
        "title": "ç¬¬äºŒç« ï¼šæ‰é¢ä¸å‘é…µ", 
        "id": "bread-ch-3", 
        "groups": [{{
          "title": "2.1 æ¿€æ´»é…µæ¯ï¼šå‘é…µæˆåŠŸçš„ç¬¬ä¸€æ­¥", 
          "id": "bread-gr-3-1",
          "sections": [{{ 
            "title": "æ¿€æ´»é…µæ¯", 
            "id": "bread-sec-3-1-1", 
            "primary_goal": "è§£é‡Šæ´»æ€§å¹²é…µæ¯å’Œå³å‘å¹²é…µæ¯çš„åŒºåˆ«ï¼Œå¹¶æä¾›ä¸€ä¸ªæ¿€æ´»é…µæ¯çš„è¯¦ç»†åˆ†æ­¥æŒ‡å—ã€‚"
          }}]
        }}]
      }},
      {{ "title": "ç¬¬ä¸‰ç« ï¼šé«˜çº§å¡‘å½¢æŠ€å·§", "id": "bread-ch-2", "groups": [] }},
      {{ "title": "ç¬¬å››ç« ï¼šçƒ˜çƒ¤çš„è‰ºæœ¯", "id": "bread-ch-4-new", "groups": [] }}
    ]
  }}
}}
```

ã€ä½ çš„ä»»åŠ¡ã€‘
ç°åœ¨ï¼Œè¯·å¯¹æœ€ä¸Šæ–¹æä¾›çš„ã€è¾“å…¥ã€‘å¤§çº²è¿›è¡Œå…¨é¢å®¡é˜…å’Œä¿®è®¢ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§ã€è¾“å‡ºæ ¼å¼ã€‘å’Œã€é«˜è´¨é‡èŒƒä¾‹ã€‘çš„æ ·å¼ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«`review_report`å’Œ`modified_outline`çš„æœ€ç»ˆJSONå¯¹è±¡ã€‚
"""

def stage4_llm_global_review(
    caller_review: LLMCaller,
    topic: str,
    data_with_ids: Dict[str, Any],
    max_tokens: int = 4096,
) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """æ‰§è¡ŒLLMå…¨å±€å®¡é˜…ï¼Œè¿”å›(å®¡æ ¸æŠ¥å‘Š, ä¿®æ”¹åçš„å¤§çº²)å…ƒç»„ã€‚"""
    prompt = build_global_review_prompt(topic, data_with_ids)
    raw = caller_review.chat_json(prompt, max_tokens=max_tokens)
    parsed_response = extract_json(raw) or {}

    # å®‰å…¨åœ°æå–ä¸¤ä¸ªéƒ¨åˆ†
    review_report = parsed_response.get("review_report") or {
        "overall_assessment": "Failed to parse LLM review report.",
        "suggested_changes": [],
    }
    modified_outline = parsed_response.get("modified_outline") or data_with_ids # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å¤§çº²

    # åŒ…è£…å®¡æ ¸æŠ¥å‘Šï¼Œå¢åŠ æ¨¡å‹ä¿¡æ¯
    final_review_report = {
        "mode": "llm_review_and_apply",
        "model": getattr(caller_review, "conf", None) and getattr(caller_review.conf, "key", None),
        "review": review_report
    }

    return final_review_report, modified_outline


# -----------------------------
# ä¸»æµç¨‹ï¼ˆå››é˜¶æ®µï¼‰
# -----------------------------


def main(argv: Optional[List[str]] = None) -> int:
    parser = argparse.ArgumentParser(description="å››é˜¶æ®µå­¦ä¹ å¤§çº²ç”Ÿæˆï¼ˆBrainstormâ†’Structureâ†’Detailâ†’Reviewï¼‰")
    parser.add_argument("--topic", required=True, help="ä¸»é¢˜")
    parser.add_argument("--depth", choices=["overview", "core", "advanced"], default="core", help="æ§åˆ¶è§„æ¨¡ä¸ç²’åº¦")
    parser.add_argument("--language", default="zh", help="è¾“å‡ºè¯­è¨€ï¼ˆé»˜è®¤ zhï¼‰")
    parser.add_argument("--config", default="config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--llm-gen", help="ç”¨äºç”Ÿæˆï¼ˆé˜¶æ®µ1/2/3ç»†èŠ‚ï¼‰çš„ LLM keyï¼ˆè¦†ç›– config.node_llm.generate_outlineï¼‰")
    parser.add_argument("--llm-cls", help="ç”¨äºå¤§çº²ç»†åŒ–/åˆ†ç±»çš„ LLM keyï¼ˆè¦†ç›– config.node_llm.generate_prompt_templateï¼‰")
    parser.add_argument("--llm-review", help="ç”¨äºå…¨å±€å®¡é˜…ï¼ˆé˜¶æ®µ4 LLMï¼‰çš„ LLM keyï¼ˆè¦†ç›– config.node_llm.generate_and_review_parallel.reviewï¼‰")
    parser.add_argument("--out-dir", default="output", help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ outputï¼‰")
    parser.add_argument("--basename", help="è¾“å‡ºæ–‡ä»¶åŸºåï¼ˆé»˜è®¤ç”± topic æ´¾ç”Ÿï¼‰")
    parser.add_argument("--max-tokens", type=int, default=4096, help="å•æ¬¡è°ƒç”¨æœ€å¤§ tokens")
    parser.add_argument("--max-parallel", type=int, help="å¹¶å‘ï¼ˆè¦†ç›– config.max_parallel_requestsï¼‰")
    parser.add_argument("--resume", action="store_true", help="å¦‚å­˜åœ¨ä¸­é—´äº§ç‰©åˆ™å°½é‡å¤ç”¨ï¼ˆseeds/skeletonï¼‰")
    parser.add_argument("--review-mode", choices=["llm", "none"], default="llm", help="é˜¶æ®µ4å®¡æŸ¥æ¨¡å¼ï¼šllm=æ‰§è¡ŒLLMå®¡æ ¸ï¼Œnone=è·³è¿‡")
    parser.add_argument("--pretty", action="store_true", help="åœ¨ stderr æ‰“å°ç®€è¦æ‘˜è¦")

    args = parser.parse_args(argv)

    try:
        cfg = load_config(args.config)
        gen_cfg = choose_llm(cfg, args.llm_gen, "generate_outline")
        cls_cfg = choose_llm(cfg, args.llm_cls, "generate_prompt_template")
        # å®¡é˜…æ¨¡å‹ï¼šä¼˜å…ˆ --llm-reviewï¼Œå…¶æ¬¡ node_llm.generate_and_review_parallel.reviewï¼Œæœ€å fallback default
        review_cfg = None
        if args.review_mode == "llm":
            try:
                review_cfg = choose_llm(cfg, args.llm_review, "generate_and_review_parallel.review")
            except Exception:
                # å°è¯• fallback åˆ° default
                try:
                    review_cfg = choose_llm(cfg, args.llm_review, "default")
                except Exception:
                    review_cfg = None
    except Exception as e:
        print(f"[é”™è¯¯] è¯»å–/é€‰æ‹©å¤§æ¨¡å‹å¤±è´¥ï¼š{e}", file=sys.stderr)
        return 2

    try:
        gen_caller = LLMCaller(gen_cfg)
        cls_caller = LLMCaller(cls_cfg)
        review_caller = None
        if args.review_mode == "llm" and review_cfg is not None:
            review_caller = LLMCaller(review_cfg)
    except SystemExit as e:
        print(str(e), file=sys.stderr)
        return 2
    except Exception as e:
        print(f"[é”™è¯¯] åˆå§‹åŒ– LLM å®¢æˆ·ç«¯å¤±è´¥ï¼š{e}", file=sys.stderr)
        return 2

    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    basename = args.basename or slugify(args.topic)

    max_parallel = args.max_parallel or int(cfg.get("max_parallel_requests", 5))
    if max_parallel <= 0:
        max_parallel = 1
    
    print(f"ğŸš€ å¼€å§‹ä¸ºä¸»é¢˜ '{args.topic}' ç”Ÿæˆå­¦ä¹ å¤§çº²...")

    # é˜¶æ®µ 1ï¼šseeds
    print("\n[é˜¶æ®µ 1/4] æ­£åœ¨è¿›è¡Œå‘æ•£è„‘æš´ï¼Œç”ŸæˆçŸ¥è¯†ç§å­...")
    seeds_path = out_dir / f"{basename}.seeds.json"
    if args.resume and seeds_path.exists():
        # If we resume, we can't distinguish professor/engineer seeds.
        # So we load the combined seeds and pass them to both arguments in stage2.
        print("  -> å‘ç°å¹¶å¤ç”¨ 'seeds.json'ã€‚")
        seeds_for_downstream = json.loads(seeds_path.read_text(encoding="utf-8"))
        professor_seeds = seeds_for_downstream.get("seeds", [])
        engineer_seeds = [] # Can't know, so leave empty. Stage 2 prompt will handle it.
        print(f"  -> å¤ç”¨çŸ¥è¯†ç§å­ {len(professor_seeds)} ä¸ªã€‚")
    else:
        professor_seeds, engineer_seeds = stage1_brainstorm(
            gen_caller, args.topic, args.language, out_dir, basename, args.max_tokens, max_parallel
        )
        # For stage3, it needs a single list. Let's use the combined list from the saved seeds.json
        seeds_for_downstream = json.loads(seeds_path.read_text(encoding="utf-8"))

    # é˜¶æ®µ 2ï¼šskeleton
    print("\n[é˜¶æ®µ 2/4] æ­£åœ¨èšç±»ä¸æ’åºï¼Œæ„å»ºç« èŠ‚éª¨æ¶...")
    skeleton_path = out_dir / f"{basename}.skeleton.json"
    if args.resume and skeleton_path.exists():
        skeleton = json.loads(skeleton_path.read_text(encoding="utf-8"))
        print("  -> å‘ç°å¹¶å¤ç”¨ 'skeleton.json'ã€‚")
    else:
        skeleton = stage2_structure(
            caller=gen_caller,
            topic=args.topic,
            professor_seeds=professor_seeds,
            engineer_seeds=engineer_seeds,
            depth=args.depth,
            language=args.language,
            out_dir=out_dir,
            basename=basename,
            max_tokens=args.max_tokens
        )
    print(f"  -> ç« èŠ‚éª¨æ¶æ„å»ºå®Œæ¯•ï¼Œå…± {len(skeleton.get('chapters', []))} ç« ã€‚")

    # é˜¶æ®µ 3ï¼šdetail + classify
    stage3 = stage3_detail_and_classify(
        caller=cls_caller, # stage3 uses the classification model
        topic=args.topic,
        skeleton=skeleton,
        seeds=seeds_for_downstream, # Pass the combined seeds dict
        out_dir=out_dir,
        basename=basename,
        max_tokens=args.max_tokens,
        max_parallel=max_parallel,
    )

    print("\n[é˜¶æ®µ 4/4] æ­£åœ¨è¿›è¡Œæœ€ç»ˆå¤„ç†ã€å®¡æŸ¥ä¸ç”Ÿæˆæ–‡ä»¶...")
    # Stage 3's output is the initial outline
    initial_outline = apply_defaults_and_ids(args.topic, {"chapters": stage3.get("chapters") or []})

    # Always save the pre-review version from Stage 3
    (out_dir / f"{basename}.outline.json").write_text(json.dumps(initial_outline, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"  -> å·²ä¿å­˜ Stage 3 åŸå§‹å¤§çº² ({basename}.outline.json)")

    review_json = {"mode": args.review_mode}
    modified_outline = initial_outline # Default to initial if review is skipped

    if args.review_mode == "llm" and review_caller is not None:
        print(f"  -> å¼€å§‹è°ƒç”¨ LLM ({review_caller.conf.key}) è¿›è¡Œå…¨å±€å®¡é˜…å’Œä¿®è®¢...")
        review_report, modified_outline_data = stage4_llm_global_review(
            caller_review=review_caller,
            topic=args.topic,
            data_with_ids=initial_outline,
            max_tokens=args.max_tokens,
        )
        review_json.update(review_report)
        modified_outline = modified_outline_data # Use the LLM's modified version
        print("  -> LLM å…¨å±€å®¡é˜…å’Œä¿®è®¢å®Œæˆã€‚")
    else:
        print("  -> è·³è¿‡ Stage 4 å®¡é˜…ã€‚ ")
        review_json["review"] = {"overall_assessment": "Review was skipped.", "suggested_changes": []}

    # Save the review report
    (out_dir / f"{basename}.review.json").write_text(json.dumps(review_json, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"  -> å·²ä¿å­˜å®¡æ ¸æŠ¥å‘Š ({basename}.review.json)")

    # Save the final, potentially modified outline
    (out_dir / f"{basename}.outline.reviewed.json").write_text(json.dumps(modified_outline, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"  -> å·²ä¿å­˜ Stage 4 ä¿®è®¢åå¤§çº² ({basename}.outline.reviewed.json)")

    # Render and save the final markdown from the MODIFIED outline
    final_md_text = render_markdown(modified_outline)
    (out_dir / f"{basename}.learning-path.md").write_text(final_md_text, encoding="utf-8")
    print(f"  -> å·²ä¿å­˜æœ€ç»ˆå­¦ä¹ è·¯å¾„ ({basename}.learning-path.md)")


    print("\nâœ… å¤§çº²ç”Ÿæˆæµç¨‹å®Œæ¯•ï¼")
    if args.pretty:
        # Use the modified_outline for the final count
        ch_count = len(modified_outline.get("chapters") or [])
        sec_count = sum(len(gr.get("sections") or []) for ch in modified_outline.get("chapters") or [] for gr in ch.get("groups") or [])
        print("---", file=sys.stderr)
        print(f"ä¸»é¢˜ï¼š{args.topic}", file=sys.stderr)
        print(f"ç”Ÿæˆå®Œæˆï¼š{out_dir / (basename + '.learning-path.md')}", file=sys.stderr)
        print(f"ç« èŠ‚æ•°ï¼š{ch_count}ï¼ŒçŸ¥è¯†ç‚¹æ•°ï¼š{sec_count}", file=sys.stderr)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
