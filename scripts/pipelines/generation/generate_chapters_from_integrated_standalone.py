#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç« èŠ‚çº§ç”Ÿæˆè„šæœ¬ï¼ˆç‹¬ç«‹èŠ‚ç‚¹ç‰ˆï¼‰

åŠŸèƒ½ï¼šä¸ scripts/generate_chapters_from_integrated.py ç›¸åŒï¼Œä½†ä¸å¤ç”¨ run_full ä¸­çš„èŠ‚ç‚¹ï¼Œ
è€Œæ˜¯å°†ç›¸å…³èŠ‚ç‚¹ä¸ä¾èµ–é€»è¾‘å¤åˆ¶åˆ°æœ¬è„šæœ¬ä¸­ä½¿ç”¨ã€‚

æµç¨‹ï¼šä»â€œé€‰æ‹©ç« èŠ‚â€å¼€å§‹ â†’ ç»“æ„æ„ŸçŸ¥ç”Ÿæˆ â†’ å®¡æŸ¥ â†’ï¼ˆå¯é€‰ï¼‰ä¿®å¤ææ¡ˆä¸è‡ªåŠ¨åº”ç”¨ â†’ å‘å¸ƒ â†’ æ±‡æ€»

è¾“å…¥ï¼šåŒ…å« reconstructed_outline çš„é›†æˆ JSONï¼ˆå¦‚ llm-integrated-20251013-125553.jsonï¼‰

ç”¨æ³•ç¤ºä¾‹ï¼š
  python scripts/pipelines/generation/generate_chapters_from_integrated_standalone.py \
    --input output/integrated_pipeline/quantum-mechanics-integrated-20251016-164313.json \
    --selected-chapters 1,2,3 \
    --config config.json \
    --skip-content-review \
    --debug

è°ƒè¯•æ¨¡å¼ï¼š
- --debug å¼€å¯åï¼Œè¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼ŒåŒ…å«ï¼šæ‰€ç”¨æ¨¡å‹ä¿¡æ¯ã€æ¯ä¸ª LLM è°ƒç”¨çš„å®Œæ•´ Promptï¼›
- æ—¥å¿—ä¼šå†™å…¥ output/<output_subdir>/log.txtã€‚

æ–°å¢é€‰é¡¹ï¼š
- --subject-type tool|theory æ‰‹åŠ¨æŒ‡å®šä¸»é¢˜ç±»å‹ï¼›æœªæŒ‡å®šæ—¶è‡ªåŠ¨åˆ†ç±»ã€‚
- --classify-llm-key <key> æŒ‡å®šåˆ†ç±»æ‰€ç”¨ LLM é”®åï¼ˆè¦†ç›– node_llm/classify_subject/defaultï¼‰ã€‚
"""

from __future__ import annotations

import argparse
import asyncio
import json
import logging
import os
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, TypedDict

def _find_repo_root() -> Path:
    p = Path(__file__).resolve()
    for parent in [p] + list(p.parents):
        if (parent / "config.json").exists():
            return parent
    return Path(__file__).resolve().parents[-1]

BASE_DIR = _find_repo_root()
PUBLIC_DIR = BASE_DIR / "web-learner" / "public"
CONTENT_ROOT = PUBLIC_DIR / "content"

# åŠ å…¥çˆ¶ç›®å½•æœç´¢è·¯å¾„ï¼ˆå¯¼å…¥ mermaid å·¥å…·å’Œå¯é€‰ LLM å·¥å…·ï¼‰
if str(BASE_DIR) not in sys.path:
    sys.path.insert(0, str(BASE_DIR))

from mermaid_sanitizer import sanitize_mermaid_in_markdown


# ----------------------------
# åŸºç¡€å·¥å…·
# ----------------------------

def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def slugify(text: str) -> str:
    s = (text or "").strip().lower()
    s = re.sub(r"[\s/]+", "-", s)
    s = re.sub(r"[^a-z0-9\-_.]+", "", s)
    s = re.sub(r"-+", "-", s).strip("-")
    return s or "topic"


def load_config(path: str) -> Dict[str, Any]:
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    try:
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception as e:
        raise RuntimeError(f"è§£æé…ç½®æ–‡ä»¶å¤±è´¥: {e}")


def try_parse_json_array(text: str) -> List[Dict[str, Any]]:
    t = (text or "").strip()
    if not t:
        return []
    if "```" in t:
        parts = t.split("```")
        for part in parts:
            s = part.strip()
            if s.startswith("[") and s.endswith("]"):
                try:
                    return json.loads(s)
                except Exception:
                    pass
    try:
        i = t.index("[")
        j = t.rindex("]")
        return json.loads(t[i : j + 1])
    except Exception:
        return []


def try_parse_json_object(text: str) -> Dict[str, Any]:
    t = (text or "").strip()
    if not t:
        return {}
    if "```" in t:
        parts = t.split("```")
        for part in parts:
            s = part.strip()
            if s.startswith("{") and s.endswith("}"):
                try:
                    return json.loads(s)
                except Exception:
                    pass
    try:
        i = t.index("{")
        j = t.rindex("}")
        return json.loads(t[i : j + 1])
    except Exception:
        return {}


# ----------------------------
# æ•°æ®ç»“æ„
# ----------------------------

@dataclass
class Point:
    id: str
    title: str
    chapter: str
    section: str


class WorkState(TypedDict, total=False):
    topic: str
    topic_meta: Dict[str, Any]
    topic_slug: str
    config: Dict[str, Any]
    outline_final_md: str
    outline_struct: Dict[str, Any]
    points: List[Point]
    drafts: List[Dict[str, str]]
    reviews: List[Dict[str, Any]]
    publish_paths: List[str]
    failures: List[Dict[str, Any]]
    report_md: str
    fix_proposals: List[Dict[str, Any]]
    fix_applied: List[str]
    fix_skipped: List[str]
    fix_iterations: List[Dict[str, Any]]
    output_subdir: str
    selected_chapters: List[str]
    auto_apply_stats: Dict[str, Any]


# ----------------------------
# LLM å®¢æˆ·ç«¯ä¸æ¨¡å‹é€‰æ‹©ï¼ˆå®Œå…¨å»è€¦ï¼‰
# ----------------------------

class _AsyncLLM:
    async def ainvoke(self, prompt: str) -> str:
        raise NotImplementedError


class OpenAICompatClient(_AsyncLLM):
    def __init__(self, api_key: str, base_url: str, model: str, temperature: float, max_tokens: int) -> None:
        self._ok = False
        try:
            from openai import AsyncOpenAI  # type: ignore
        except Exception:
            logging.getLogger(__name__).error("ç¼ºå°‘ openai åº“ï¼Œè¯·å®‰è£…: pip install openai")
            self._client = None
            self._ok = False
            return
        self._client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        self._model = model
        self._temperature = temperature
        self._max_tokens = max_tokens
        self._ok = True

    async def ainvoke(self, prompt: str) -> str:
        if not self._ok:
            raise RuntimeError("OpenAI å…¼å®¹å®¢æˆ·ç«¯æœªæ­£ç¡®åˆå§‹åŒ–")
        resp = await self._client.chat.completions.create(
            model=self._model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„æŠ€æœ¯å†™ä½œä¸ç¼–è¾‘åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=self._temperature,
            max_tokens=self._max_tokens,
        )
        return (resp.choices[0].message.content or "") if (resp and resp.choices) else ""


class GeminiClient(_AsyncLLM):
    def __init__(self, api_key: str, model: str) -> None:
        self._ok = False
        try:
            import google.generativeai as genai  # type: ignore
        except Exception:
            logging.getLogger(__name__).error("ç¼ºå°‘ google-generativeaiï¼Œè¯·å®‰è£…: pip install -U google-generativeai")
            self._model = None
            return
        if not api_key:
            logging.getLogger(__name__).error("ç¼ºå°‘ GOOGLE_API_KEY / gemini_api_key é…ç½®")
            return
        genai.configure(api_key=api_key)
        self._model = genai.GenerativeModel(model)
        self._ok = True

    async def ainvoke(self, prompt: str) -> str:
        if not self._ok:
            raise RuntimeError("Gemini å®¢æˆ·ç«¯æœªæ­£ç¡®åˆå§‹åŒ–")
        def _sync_call() -> str:
            resp = self._model.generate_content(prompt)
            txt = getattr(resp, "text", "")
            if txt:
                return txt
            try:
                cands = getattr(resp, "candidates", [])
                if cands:
                    parts = getattr(getattr(cands[0], "content", None), "parts", [])
                    if parts and hasattr(parts[0], "text"):
                        return parts[0].text or ""
            except Exception:
                pass
            return ""
        return await asyncio.to_thread(_sync_call)


def init_llm(cfg: Dict[str, Any]) -> _AsyncLLM:
    provider = str(cfg.get("api_provider", "openai_compat")).lower()
    model = cfg.get("model", "gpt-4o-mini")
    temperature = float(cfg.get("temperature", 0.6))
    max_tokens = int(cfg.get("max_tokens", 8192))
    if provider in {"openai_compat", "deepseek", "openai"}:
        api_key = cfg.get("openai_api_key") or cfg.get("deepseek_api_key") or os.environ.get("OPENAI_API_KEY") or os.environ.get("DEEPSEEK_API_KEY")
        base_url = cfg.get("openai_base_url") or cfg.get("deepseek_base_url") or os.environ.get("OPENAI_BASE_URL") or os.environ.get("DEEPSEEK_BASE_URL") or "https://api.openai.com/v1"
        return OpenAICompatClient(api_key=api_key or "", base_url=base_url, model=model, temperature=temperature, max_tokens=max_tokens)
    elif provider in {"gemini", "google"}:
        api_key = cfg.get("gemini_api_key") or os.environ.get("GOOGLE_API_KEY") or ""
        return GeminiClient(api_key=api_key, model=model)
    else:
        raise SystemExit(f"æœªçŸ¥çš„ api_provider: {provider}")


def _resolve_provider(entry: Dict[str, Any], fallback: Dict[str, Any]) -> str:
    p = entry.get("provider") or entry.get("api_provider") or fallback.get("api_provider") or "openai_compat"
    return str(p).lower()


def _make_llm_from_entry(entry: Dict[str, Any], fallback: Dict[str, Any]) -> _AsyncLLM:
    provider = _resolve_provider(entry, fallback)
    model = entry.get("model") or fallback.get("model", "gpt-4o-mini")
    temperature = float(entry.get("temperature", fallback.get("temperature", 0.6)))
    max_tokens = int(entry.get("max_tokens", fallback.get("max_tokens", 8192)))
    if provider in {"openai_compat", "deepseek", "openai"}:
        api_key = (
            entry.get("api_key")
            or entry.get("openai_api_key")
            or entry.get("deepseek_api_key")
            or fallback.get("openai_api_key")
            or fallback.get("deepseek_api_key")
            or os.environ.get("OPENAI_API_KEY")
            or os.environ.get("DEEPSEEK_API_KEY")
            or ""
        )
        base_url = (
            entry.get("base_url")
            or entry.get("openai_base_url")
            or entry.get("deepseek_base_url")
            or fallback.get("openai_base_url")
            or fallback.get("deepseek_base_url")
            or os.environ.get("OPENAI_BASE_URL")
            or os.environ.get("DEEPSEEK_BASE_URL")
            or "https://api.openai.com/v1"
        )
        return OpenAICompatClient(api_key=api_key, base_url=base_url, model=model, temperature=temperature, max_tokens=max_tokens)
    elif provider in {"gemini", "google"}:
        api_key = entry.get("api_key") or entry.get("gemini_api_key") or fallback.get("gemini_api_key") or os.environ.get("GOOGLE_API_KEY") or ""
        return GeminiClient(api_key=api_key, model=str(model))
    else:
        raise SystemExit(f"æœªçŸ¥çš„ api_provider: {provider}")


def build_llm_registry(cfg: Dict[str, Any]) -> Dict[str, _AsyncLLM]:
    reg: Dict[str, _AsyncLLM] = {}
    reg["default"] = init_llm(cfg)
    entries = cfg.get("llms", {}) or {}
    if isinstance(entries, dict):
        for name, entry in entries.items():
            try:
                if not isinstance(entry, dict):
                    continue
                reg[name] = _make_llm_from_entry(entry, cfg)
            except Exception as e:
                logging.getLogger(__name__).warning(f"LLM æ³¨å†Œå¤±è´¥: {name}: {e}")
    return reg


def select_llm_for_node(cfg: Dict[str, Any], registry: Dict[str, _AsyncLLM], node_key: str, subrole: Optional[str] = None) -> _AsyncLLM:
    mapping = cfg.get("node_llm", {}) or {}

    def _resolve_name(nk: str, sr: Optional[str]) -> Optional[str]:
        if sr:
            return mapping.get(f"{nk}.{sr}") or mapping.get(nk)
        return mapping.get(nk)

    name: Optional[str] = _resolve_name(node_key, subrole)
    if not name and node_key == "generate_and_review_by_chapter":
        name = _resolve_name("generate_and_review_parallel", subrole)
    if name and name in registry:
        return registry[name]
    map_default = mapping.get("default")
    if map_default and map_default in registry:
        return registry[map_default]
    return registry.get("default")  # type: ignore


# ----------------------------
# èŠ‚ç‚¹ä¾èµ–å‡½æ•°ï¼ˆå¤åˆ¶è‡ª run_fullï¼‰
# ----------------------------

def _parse_indices_from_id(pid: str) -> Tuple[Optional[int], Optional[int], Optional[int]]:
    try:
        m = re.match(r"^.*?-sec-(\d+)-(\d+)-(\d+)-.*$", pid)
        if not m:
            return None, None, None
        return int(m.group(1)), int(m.group(2)), int(m.group(3))
    except Exception:
        return None, None, None


def _make_filename(pid: str, title: str, style: str = "id") -> str:
    if style == "id":
        return f"{pid}.md"
    ci, gi, si = _parse_indices_from_id(pid)
    title_slug = slugify(title)
    if style == "structured":
        if ci and gi and si:
            base = f"sec-{ci}-{gi}-{si}-{title_slug or 'section'}"
        else:
            base = title_slug or pid
        return f"{base}.md"
    return f"{pid}.md"


def _clean_title_for_filename(title: str) -> str:
    """æ¸…ç†æ ‡é¢˜ç”¨äºæ–‡ä»¶ååç¼€ï¼š
    - å»æ‰å‰å¯¼åºå·ï¼ˆå¦‚ "1.2 "ã€"1.2.3 "ã€"1) ", "1." ç­‰å¸¸è§æ ¼å¼ï¼‰
    - å»æ‰ç»“å°¾çš„è‹±æ–‡æ‹¬å·æ³¨é‡Šï¼ˆå¦‚ "(Dirac Notation)"ï¼‰
    - æ›¿æ¢æ–‡ä»¶ç³»ç»Ÿä¸å‹å¥½çš„åˆ†éš”ç¬¦ï¼ˆ/ å’Œ \ï¼‰ä¸º '-'
    - ä¿ç•™ä¸­æ–‡åŠä¸­æ–‡æ ‡ç‚¹ï¼ˆå¦‚å…¨è§’å†’å· 'ï¼š'ï¼‰
    """
    t = str(title or "").strip()
    # å»æ‰å‰ç¼€ç¼–å·ï¼ˆå¦‚ 1.2 ã€1.2.3 ã€1) ã€1. ç­‰ï¼‰åŠå…¶åå¸¸è§åˆ†éš”
    t = re.sub(r"^\s*(?:\d+(?:\.\d+)*|\d+[\.)])\s*[ï¼š:ã€.\-\s]*", "", t)
    # å»æ‰ç»“å°¾æ‹¬å·å†…çš„è‹±æ–‡æ³¨é‡Šï¼ˆä»…ç§»é™¤çº¯ ASCII çš„æ‹¬å·å†…å®¹ï¼‰
    t = re.sub(r"\s*\([A-Za-z0-9 ,.'\-_/+&:#]+\)\s*$", "", t)
    # æ›¿æ¢ä¸å®‰å…¨è·¯å¾„å­—ç¬¦
    t = t.replace("/", "-").replace("\\", "-")
    return t.strip()


def _build_contextual_content_prompt(
    *,
    topic: str,
    language: str,
    path: str,
    section_title: str,
    primary_goal: str = "",
    suggested_modules: Optional[List[str]] = None,
    suggested_contents: Optional[List[str]] = None,
    structure_type: str = "pipeline",
    relation_to_previous: str = "",
    prior_context: str = "",
) -> str:
    lang = (language or "zh").strip().lower()
    role = "ä½ æ˜¯ä¸€ä½ä¸–ç•Œçº§çš„æ•™è‚²å®¶ä¸ä½œå®¶ï¼Œä»¥å…¶èƒ½å°†å¤æ‚ã€æŠ½è±¡çš„ç†è®ºçŸ¥è¯†å˜å¾—æµ…æ˜¾æ˜“æ‡‚ã€å¼•äººå…¥èƒœè€Œé—»åã€‚ä½ çš„å¤©èµ‹åœ¨äºä¸ä»…ä»…æ˜¯è§£é‡Šï¼Œæ›´æ˜¯å»å¯å‘ï¼Œå°†é”™ç»¼å¤æ‚çš„æ¦‚å¿µç¼–ç»‡æˆä¸€ä¸ªå¼•äººå…¥èƒœçš„å™äº‹ï¼Œä»è€Œä¿ƒè¿›è¯»è€…å½¢æˆæ·±åˆ»ä¸”æŒä¹…çš„ç†è§£ã€‚"
    head = f"# è¯¾ç¨‹å†…å®¹ç”Ÿæˆä»»åŠ¡\n\n{role}\n"
    if path:
        head += f"\nã€å®šä½ã€‘{path}\n"

    goal_part = f"\nã€æ•™å­¦ç›®æ ‡ã€‘{primary_goal}\n" if primary_goal else "\nã€æ•™å­¦ç›®æ ‡ã€‘å›´ç»•å½“å‰çŸ¥è¯†ç‚¹å±•å¼€ï¼Œé«˜è´¨é‡è§£é‡Šå¹¶ç»™å‡ºå¿…è¦ç¤ºä¾‹ã€‚\n"
    mods = suggested_modules or []
    if mods:
        mods_part = "\nã€å»ºè®®å†…å®¹æ¨¡å—ã€‘" + ", ".join(mods) + "\n"
    else:
        mods_part = "\nã€å»ºè®®å†…å®¹æ¨¡å—ã€‘summary, code_exampleï¼ˆå¦‚é€‚åˆï¼‰, common_mistake_warningï¼ˆå¦‚æœ‰ï¼‰, diagramï¼ˆå¦‚æœ‰å› æœ/æµç¨‹ï¼‰\n"

    conts = [c for c in (suggested_contents or []) if isinstance(c, str)]
    conts_part = "\nã€æ ¸å¿ƒå†…å®¹ã€‘" + ", ".join(conts) + "\n" if conts else "\n"

    style_part = """
ã€å†™ä½œé£æ ¼ä¸æ·±åº¦è¦æ±‚ã€‘
- **ç±»æ¯”ä¸å…·è±¡åŒ–**ï¼šå¯¹äºæŠ½è±¡çš„æ ¸å¿ƒæ¦‚å¿µï¼Œè¯·ä½¿ç”¨è¯»è€…ç”Ÿæ´»ä¸­å¯èƒ½ç†Ÿæ‚‰çš„ç°è±¡æˆ–ç»éªŒè¿›è¡Œç±»æ¯”ï¼Œå¸®åŠ©ä»–ä»¬å»ºç«‹ç›´è§‚æ„Ÿå—ã€‚**é‡è¦ï¼šç¡®ä¿ç±»æ¯”åœ¨ç®€åŒ–æ¦‚å¿µçš„åŒæ—¶ï¼Œä¸ä¼šç‰ºç‰²å…³é”®çš„æŠ€æœ¯ç²¾ç¡®æ€§ã€‚**
- **èƒŒæ™¯ä¸å™äº‹**ï¼šå¯¹äºä»»ä½•ä¸€ä¸ªåŸºç¡€ç†è®ºã€åŸåˆ™æˆ–å…³é”®æ€æƒ³ï¼Œè¯·æ·±å…¥æŒ–æ˜å…¶æå‡ºçš„èƒŒæ™¯ã€‚è§£é‡Šå®ƒè¯•å›¾è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿåœ¨æ­¤ä¹‹å‰çš„ä¸»æµè§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿå®ƒçš„å‡ºç°å¸¦æ¥äº†å“ªäº›å…³é”®æ€§çš„å½±å“ï¼Ÿ**å¯¹äºæŠ€æœ¯æ€§å¼ºçš„å­¦ç§‘ï¼Œè¿™æ„å‘³ç€æ¸…æ™°åœ°é˜è¿°å…¶â€œé—®é¢˜-è§£å†³æ–¹æ¡ˆ-å½±å“â€çš„é€»è¾‘é“¾æ¡ï¼Œè€Œéæ–‡å­¦æ€§æè¿°ã€‚**
- **å¯å‘æ€§ç»“å°¾**ï¼šåœ¨æ–‡ç« æœ«å°¾ï¼Œé™¤äº†æ€»ç»“è¦ç‚¹ï¼Œè¿˜åº”æå‡ºä¸€äº›å‘äººæ·±çœçš„é—®é¢˜ï¼Œæˆ–ä¸€ä¸ªèƒ½æ‰¿ä¸Šå¯ä¸‹çš„å‰ç»æ€§è§‚ç‚¹ï¼Œä»¥æ¿€å‘è¯»è€…çš„å¥½å¥‡å¿ƒå’Œè¿›ä¸€æ­¥æ¢ç´¢çš„æ¬²æœ›ã€‚
- **ç¯‡å¹…æŒ‡å¯¼**ï¼šä¸ºç¡®ä¿å†…å®¹çš„æ·±åº¦ï¼Œæ¯ä¸€ç¯‡çŸ¥è¯†ç‚¹éƒ½åº”è¢«å……åˆ†åœ°æ¢è®¨ã€‚è¯·åŠ›æ±‚å†…å®¹è¯¦å°½ï¼Œç›®æ ‡ç¯‡å¹…åœ¨ **2500-3500å­—** å·¦å³ã€‚è¯·ä¼˜å…ˆè€ƒè™‘å†…å®¹çš„æ·±åº¦ä¸æ¸…æ™°åº¦ï¼Œè€Œéç®€æ´ã€‚
"""

    if structure_type == "pipeline":
        ctx_part = (f"\nã€å·²å®Œæˆçš„å°èŠ‚å†…å®¹ï¼ˆContextï¼‰ã€‘\n> " + prior_context.replace("\n", "\n> ") + "\n") if prior_context else "\n"
        task = (
            f"\nã€ä½ çš„ä»»åŠ¡ã€‘è¯·ä¸¥æ ¼éµå¾ªã€å†™ä½œé£æ ¼ä¸æ·±åº¦è¦æ±‚ã€‘ï¼Œç´§æ¥ä¸Šè¿°å†…å®¹ï¼Œå›´ç»•â€œ{section_title}â€è‡ªç„¶è¿‡æ¸¡å¹¶ç»­å†™ä¸‹ä¸€æ®µã€‚è¯·ä»¥ã€æ ¸å¿ƒå†…å®¹ã€‘ä¸ºåŸºç¡€ï¼Œè¿›è¡Œè¯¦å°½åœ°å±•å¼€ä¸é˜è¿°ï¼Œç¡®ä¿è®²è§£ä¸ä»…ç³»ç»Ÿã€é€»è¾‘æ¸…æ™°ï¼Œè€Œä¸”å†…å®¹ä¸°å¯Œã€ç»†èŠ‚é¥±æ»¡ã€å¯Œæœ‰å¯å‘æ€§ã€‚\n"
        )
    else:
        dep = relation_to_previous.strip().lower()
        if prior_context and dep in {"builds_on", "deep_dive_into"}:
            ctx_part = f"\nã€çˆ¶çº§çŸ¥è¯†ç‚¹ï¼ˆParent Contextï¼‰ã€‘\n> " + prior_context.replace("\n", "\n> ") + "\n"
        else:
            ctx_part = "\n"
        task = f"\nã€ä½ çš„ä»»åŠ¡ã€‘è¯·ä¸¥æ ¼éµå¾ªã€å†™ä½œé£æ ¼ä¸æ·±åº¦è¦æ±‚ã€‘ï¼Œæ’°å†™ä¸€ç¯‡å…³äºâ€œ{section_title}â€çš„ç‹¬ç«‹æ•™å­¦æ®µè½ã€‚è¯·ä»¥ã€æ ¸å¿ƒå†…å®¹ã€‘ä¸ºåŸºç¡€ï¼Œè¿›è¡Œè¯¦å°½åœ°å±•å¼€ä¸é˜è¿°ï¼Œç¡®ä¿è®²è§£ä¸ä»…ç³»ç»Ÿã€é€»è¾‘æ¸…æ™°ï¼Œè€Œä¸”å†…å®¹ä¸°å¯Œã€ç»†èŠ‚é¥±æ»¡ã€å¯Œæœ‰å¯å‘æ€§ã€‚\n"

    constraints = """
ã€è¾“å‡ºçº¦æŸã€‘
- ä½¿ç”¨ Markdownï¼›ç»“æ„æ¸…æ™°ï¼Œæ ‡é¢˜å±‚çº§åˆç†ï¼›
- å™äº‹è¿è´¯ï¼šé¿å…ä¸å·²ç»™ä¸Šä¸‹æ–‡é‡å¤ï¼›å¿…è¦æ—¶ç”¨ä¸€å¥è¯æ‰¿æ¥ï¼›
- å¦‚å¼•ç”¨æ•°å­¦/å›¾è¡¨/æµç¨‹ï¼Œè¯·ä½¿ç”¨é€‚å½“çš„æ¨¡å—ï¼›
- å¦‚ Markdown ä¸­æ¶‰åŠä»£ç ç¤ºä¾‹ï¼Œè¯·ç”¨ä»£ç å—è¿›è¡Œå£°æ˜åŒ…è£¹ï¼›
- è¡¨æ ¼ä¸­ä¸è¦å‡ºç°ä»£ç æ ¼å¼çš„å†…å®¹ï¼›
- ç»“å°¾å«ç®€çŸ­æ€»ç»“æˆ–è¦ç‚¹å›é¡¾ã€‚
"""
    lang_line = f"\nã€è¯­è¨€ã€‘{'ä¸­æ–‡' if lang.startswith('zh') else 'English'}\n"
    return head + goal_part + mods_part + conts_part + style_part + ctx_part + task + constraints + lang_line


def _build_theory_opening_prompt(
    *,
    topic: str,
    language: str,
    path: str,
    section_title: str,
    primary_goal: str = "",
    suggested_modules: Optional[List[str]] = None,
    suggested_contents: Optional[List[str]] = None,
    current_chapter_index: int,
    all_chapters_struct: List[Dict[str, Any]],
) -> str:
    lang = (language or "zh").strip().lower()
    role = "ä½ æ˜¯ä¸€ä½ä¸–ç•Œçº§çš„æ•™è‚²å®¶ä¸ä½œå®¶ï¼Œä»¥å…¶èƒ½å°†å¤æ‚ã€æŠ½è±¡çš„ç†è®ºçŸ¥è¯†å˜å¾—æµ…æ˜¾æ˜“æ‡‚ã€å¼•äººå…¥èƒœè€Œé—»åã€‚ä½ çš„å¤©èµ‹åœ¨äºä¸ä»…ä»…æ˜¯è§£é‡Šï¼Œæ›´æ˜¯å»å¯å‘ï¼Œå°†é”™ç»¼å¤æ‚çš„æ¦‚å¿µç¼–ç»‡æˆä¸€ä¸ªå¼•äººå…¥èƒœçš„å™äº‹ï¼Œä»è€Œä¿ƒè¿›è¯»è€…å½¢æˆæ·±åˆ»ä¸”æŒä¹…çš„ç†è§£ã€‚"
    head = f"# è¯¾ç¨‹å†…å®¹ç”Ÿæˆä»»åŠ¡\n\n{role}\n"
    if path:
        head += f"\nã€å®šä½ã€‘{path}\n"

    context_str = ""
    if current_chapter_index > 1 and all_chapters_struct:
        global_overview_lines = ["ã€å…¨å±€ç›®å½•æ¦‚è§ˆã€‘"]
        for i in range(current_chapter_index - 1):
            if i < len(all_chapters_struct):
                ch_title = all_chapters_struct[i].get("title", f"ç¬¬{i+1}ç« ")
                global_overview_lines.append(f"- {ch_title}")

        prev_chapter_detail_lines = ["\nã€å‰æ–‡ç« èŠ‚è¯¦è§£ã€‘"]
        prev_ch_idx = current_chapter_index - 2
        if prev_ch_idx < len(all_chapters_struct):
            prev_ch = all_chapters_struct[prev_ch_idx]
            prev_ch_title = prev_ch.get("title", f"ç¬¬{current_chapter_index-1}ç« ")
            prev_chapter_detail_lines.append(prev_ch_title)
            for gr in prev_ch.get("groups", []):
                for sec in gr.get("sections", []):
                    sec_title = sec.get("title", "")
                    if sec_title:
                        prev_chapter_detail_lines.append(f"- {sec_title}")
        context_str = "\n".join(global_overview_lines) + "\n" + "\n".join(prev_chapter_detail_lines)

    goal_part = f"\nã€æ•™å­¦ç›®æ ‡ã€‘{primary_goal}\n" if primary_goal else "\nã€æ•™å­¦ç›®æ ‡ã€‘å›´ç»•å½“å‰çŸ¥è¯†ç‚¹å±•å¼€ï¼Œé«˜è´¨é‡è§£é‡Šå¹¶ç»™å‡ºå¿…è¦ç¤ºä¾‹ã€‚\n"
    mods = suggested_modules or []
    if mods:
        mods_part = "\nã€å»ºè®®å†…å®¹æ¨¡å—ã€‘" + ", ".join(mods) + "\n"
    else:
        mods_part = "\nã€å»ºè®®å†…å®¹æ¨¡å—ã€‘summary, code_exampleï¼ˆå¦‚é€‚åˆï¼‰, common_mistake_warningï¼ˆå¦‚æœ‰ï¼‰, diagramï¼ˆå¦‚æœ‰å› æœ/æµç¨‹ï¼‰\n"

    conts = [c for c in (suggested_contents or []) if isinstance(c, str)]
    conts_part = "\nã€æ ¸å¿ƒå†…å®¹ã€‘" + ", ".join(conts) + "\n" if conts else "\n"

    style_part = """
ã€å†™ä½œé£æ ¼ä¸æ·±åº¦è¦æ±‚ã€‘
- **ç±»æ¯”ä¸å…·è±¡åŒ–**ï¼šå¯¹äºæŠ½è±¡çš„æ ¸å¿ƒæ¦‚å¿µï¼Œè¯·ä½¿ç”¨è¯»è€…ç”Ÿæ´»ä¸­å¯èƒ½ç†Ÿæ‚‰çš„ç°è±¡æˆ–ç»éªŒè¿›è¡Œç±»æ¯”ï¼Œå¸®åŠ©ä»–ä»¬å»ºç«‹ç›´è§‚æ„Ÿå—ã€‚**é‡è¦ï¼šç¡®ä¿ç±»æ¯”åœ¨ç®€åŒ–æ¦‚å¿µçš„åŒæ—¶ï¼Œä¸ä¼šç‰ºç‰²å…³é”®çš„æŠ€æœ¯ç²¾ç¡®æ€§ã€‚**
- **èƒŒæ™¯ä¸å™äº‹**ï¼šå¯¹äºä»»ä½•ä¸€ä¸ªåŸºç¡€ç†è®ºã€åŸåˆ™æˆ–å…³é”®æ€æƒ³ï¼Œè¯·æ·±å…¥æŒ–æ˜å…¶æå‡ºçš„èƒŒæ™¯ã€‚è§£é‡Šå®ƒè¯•å›¾è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿåœ¨æ­¤ä¹‹å‰çš„ä¸»æµè§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿå®ƒçš„å‡ºç°å¸¦æ¥äº†å“ªäº›å…³é”®æ€§çš„å½±å“ï¼Ÿ**å¯¹äºæŠ€æœ¯æ€§å¼ºçš„å­¦ç§‘ï¼Œè¿™æ„å‘³ç€æ¸…æ™°åœ°é˜è¿°å…¶â€œé—®é¢˜-è§£å†³æ–¹æ¡ˆ-å½±å“â€çš„é€»è¾‘é“¾æ¡ï¼Œè€Œéæ–‡å­¦æ€§æè¿°ã€‚**
- **å¯å‘æ€§ç»“å°¾**ï¼šåœ¨æ–‡ç« æœ«å°¾ï¼Œé™¤äº†æ€»ç»“è¦ç‚¹ï¼Œè¿˜åº”æå‡ºä¸€äº›å‘äººæ·±çœçš„é—®é¢˜ï¼Œæˆ–ä¸€ä¸ªèƒ½æ‰¿ä¸Šå¯ä¸‹çš„å‰ç»æ€§è§‚ç‚¹ï¼Œä»¥æ¿€å‘è¯»è€…çš„å¥½å¥‡å¿ƒå’Œè¿›ä¸€æ­¥æ¢ç´¢çš„æ¬²æœ›ã€‚
- **ç¯‡å¹…æŒ‡å¯¼**ï¼šä¸ºç¡®ä¿å†…å®¹çš„æ·±åº¦ï¼Œæ¯ä¸€ç¯‡çŸ¥è¯†ç‚¹éƒ½åº”è¢«å……åˆ†åœ°æ¢è®¨ã€‚è¯·åŠ›æ±‚å†…å®¹è¯¦å°½ï¼Œç›®æ ‡ç¯‡å¹…åœ¨ **2500-3500å­—** å·¦å³ã€‚è¯·ä¼˜å…ˆè€ƒè™‘å†…å®¹çš„æ·±åº¦ä¸æ¸…æ™°åº¦ï¼Œè€Œéç®€æ´ã€‚
"""

    if current_chapter_index == 1:
        task = (
            f"\nã€ä½ çš„ä»»åŠ¡ã€‘è¯·ä¸¥æ ¼éµå¾ªã€å†™ä½œé£æ ¼ä¸æ·±åº¦è¦æ±‚ã€‘ï¼Œä½œä¸ºä¸€åè¯¥é¢†åŸŸçš„ä¸“å®¶ï¼Œå›´ç»•â€œ{section_title}â€è¿™ä¸ªä¸»é¢˜ï¼Œæ’°å†™æ•´ä¸ªè¯¾ç¨‹çš„å¼€ç¯‡å†…å®¹ã€‚"
            f"è¯·ä»¥ã€æ ¸å¿ƒå†…å®¹ã€‘ä¸ºåŸºç¡€ï¼Œè¿›è¡Œè¯¦å°½åœ°å±•å¼€ä¸é˜è¿°ï¼Œç¡®ä¿è®²è§£ä¸ä»…ç³»ç»Ÿã€é€»è¾‘æ¸…æ™°ï¼Œè€Œä¸”å†…å®¹ä¸°å¯Œã€ç»†èŠ‚é¥±æ»¡ã€å¯Œæœ‰å¯å‘æ€§ï¼Œå¹¶ä¸ºåç»­æ‰€æœ‰ç« èŠ‚çš„å­¦ä¹ åšå¥½é“ºå«ã€‚\n"
        )
    else:
        task = (
            f"\nã€ä½ çš„ä»»åŠ¡ã€‘è¯·ä¸¥æ ¼éµå¾ªã€å†™ä½œé£æ ¼ä¸æ·±åº¦è¦æ±‚ã€‘ï¼Œä½œä¸ºä¸€åè¯¥é¢†åŸŸçš„ä¸“å®¶ï¼Œå‚è€ƒã€å…¨å±€ç›®å½•æ¦‚è§ˆã€‘å’Œã€å‰æ–‡ç« èŠ‚è¯¦è§£ã€‘ã€‚ç°åœ¨ï¼Œè¯·å¼€å¯ä¸€ä¸ªå…¨æ–°çš„ç« èŠ‚ï¼Œå›´ç»•â€œ{section_title}â€è¿™ä¸€ä¸»é¢˜æ’°å†™å¼€ç¯‡å†…å®¹ã€‚"
            f"è¯·ä»¥ã€æ ¸å¿ƒå†…å®¹ã€‘ä¸ºåŸºç¡€ï¼Œè¿›è¡Œè¯¦å°½åœ°å±•å¼€ä¸é˜è¿°ï¼Œç¡®ä¿è®²è§£ä¸ä»…ç³»ç»Ÿã€é€»è¾‘æ¸…æ™°ï¼Œè€Œä¸”å†…å®¹ä¸°å¯Œã€ç»†èŠ‚é¥±æ»¡ã€å¯Œæœ‰å¯å‘æ€§ï¼Œå¹¶ä¸ºæœ¬ç« åç»­å†…å®¹çš„å­¦ä¹ åšå¥½é“ºå«ã€‚\n"
        )

    constraints = """
ã€è¾“å‡ºçº¦æŸã€‘
- ä½¿ç”¨ Markdownï¼›ç»“æ„æ¸…æ™°ï¼Œæ ‡é¢˜å±‚çº§åˆç†ï¼›
- å™äº‹è¿è´¯ï¼šé¿å…ä¸å·²ç»™ä¸Šä¸‹æ–‡é‡å¤ï¼›å¿…è¦æ—¶ç”¨ä¸€å¥è¯æ‰¿æ¥ï¼›
- å¦‚å¼•ç”¨æ•°å­¦/å›¾è¡¨/æµç¨‹ï¼Œè¯·ä½¿ç”¨é€‚å½“çš„æ¨¡å—ï¼›
- å¦‚ Markdown ä¸­æ¶‰åŠä»£ç ç¤ºä¾‹ï¼Œè¯·ç”¨ä»£ç å—è¿›è¡Œå£°æ˜åŒ…è£¹ï¼›
- è¡¨æ ¼ä¸­ä¸è¦å‡ºç°ä»£ç æ ¼å¼çš„å†…å®¹ï¼›
- ç»“å°¾å«ç®€çŸ­æ€»ç»“æˆ–è¦ç‚¹å›é¡¾ã€‚
"""
    lang_line = f"\nã€è¯­è¨€ã€‘{'ä¸­æ–‡' if lang.startswith('zh') else 'English'}\n"
    return head + (context_str + "\n" if context_str else "") + goal_part + mods_part + conts_part + style_part + task + constraints + lang_line



# ----------------------------
# å·¥å…·å‹ä¸»é¢˜ Promptï¼ˆPrompt 2ï¼‰
# ----------------------------

PROMPT_CLASSIFY_SUBJECT = r"""
You are a curriculum designer's assistant. Your task is to classify a given subject into one of two categories: "theory" or "tool".

Category Definitions:
* Theory: a field of knowledge, a discipline, or a conceptual framework focused on principles and the "why".
* Tool: a specific language, library, framework, or technology focused on the "how-to".

Task: Classify the following subject. Respond with a single word: theory or tool.

Subject: "[subject]"
"""


def _build_tool_content_prompt(
    *,
    topic: str,
    language: str,
    path: str,
    section_title: str,
    primary_goal: str = "",
    suggested_modules: Optional[List[str]] = None,
    suggested_contents: Optional[List[str]] = None,
    structure_type: str = "pipeline",
    relation_to_previous: str = "",
    prior_context: str = "",
) -> str:
    lang = (language or "zh").strip().lower()
    mods = [m for m in (suggested_modules or []) if isinstance(m, str)]
    conts = [c for c in (suggested_contents or []) if isinstance(c, str)]
    role = f"ä½ æ˜¯ä¸€ä½ä¸–ç•Œçº§çš„æŠ€æœ¯æ•™è‚²è€…å’Œ {topic} ä¸“å®¶ã€‚"
    header = [
        role,
        "ä½ çš„ä»»åŠ¡æ˜¯æ¥æ”¶ä¸€ä»½ç”±â€œæ€»å»ºç­‘å¸ˆâ€è®¾è®¡çš„â€œæ•™å­¦è®¾è®¡å›¾â€ï¼ˆä¸€ä¸ªJSONå¯¹è±¡ï¼‰ï¼Œå¹¶ä¾æ®è¿™ä»½è®¾è®¡å›¾ï¼Œå°†å…¶ä¸­æè¿°çš„çŸ¥è¯†ç‚¹ï¼Œè½¬åŒ–ä¸ºä¸€ç¯‡é«˜è´¨é‡ã€å¤šå±‚æ¬¡ã€ç»“æ„æ¸…æ™°çš„Markdownæ•™ç¨‹ã€‚",
    ]
    if path:
        header.append(f"ã€å®šä½ã€‘{path}")
    design_obj = {
        "title": section_title,
        "id": "point",
        "primary_goal": primary_goal or "",
        "suggested_modules": mods,
        "suggested_contents": conts,
    }
    ctx = ""
    if structure_type == "pipeline":
        if prior_context:
            ctx = "\n".join([
                "ã€å·²å®Œæˆçš„å°èŠ‚å†…å®¹ã€‘",
                "> " + (prior_context or "").replace("\n", "\n> "),
                "",
                "è¯·åœ¨ä¸é‡å¤ä»¥ä¸Šå†…å®¹çš„å‰æä¸‹ï¼Œè‡ªç„¶è¿‡æ¸¡å¹¶ç»­å†™æœ¬èŠ‚ã€‚",
            ])
    else:
        dep = (relation_to_previous or "").strip().lower()
        if prior_context and dep in {"builds_on", "deep_dive_into"}:
            ctx = "\n".join([
                "ã€çˆ¶çº§çŸ¥è¯†ç‚¹ï¼ˆParent Contextï¼‰ã€‘",
                "> " + (prior_context or "").replace("\n", "\n> "),
                "",
                ("è¯·åœ¨çˆ¶çº§åŸºç¡€ä¸Šæ·±å…¥è®²è§£å½“å‰çŸ¥è¯†ç‚¹ï¼Œçªå‡ºå†…åœ¨è”ç³»ä¸æ‰©å±•ã€‚" if dep == "deep_dive_into" else "è¯·åœ¨çˆ¶çº§åŸºç¡€ä¸Šæ¨è¿›å½“å‰çŸ¥è¯†ç‚¹ï¼Œè¯´æ˜æ”¹è¿›æˆ–æ–°å¢èƒ½åŠ›ã€‚"),
            ])
    sections = [
"""
### ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ
ç”¨ä¸€å¥è¯è¯´æ˜è¿™ä¸ªçŸ¥è¯†ç‚¹è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œä¸ºä»€ä¹ˆéœ€è¦å®ƒã€‚è¯­è¨€è¦ç²¾ç‚¼ï¼Œç›´å‡»è¦å®³ã€‚

### ğŸ’¡ ä½¿ç”¨æ–¹å¼
ä»‹ç»è¿™ä¸ªçŸ¥è¯†ç‚¹çš„å…·ä½“ä½¿ç”¨æ–¹å¼

### ğŸ“š Level 1: åŸºç¡€è®¤çŸ¥ï¼ˆ30ç§’ç†è§£ï¼‰
æä¾›ä¸€ä¸ªæœ€ç®€å•ã€æœ€ç›´è§‚çš„ä»£ç ç¤ºä¾‹ï¼Œè®©åˆå­¦è€…ä¸€çœ¼å°±èƒ½æ˜ç™½åŸºæœ¬ç”¨æ³•ã€‚ä»£ç å¿…é¡»å®Œæ•´å¯è¿è¡Œï¼Œå¹¶ä»¥æ³¨é‡Šçš„å½¢å¼åŒ…å«é¢„æœŸè¾“å‡ºç»“æœã€‚
```python
# ç¤ºä¾‹ä»£ç 
```

### ğŸ“ˆ Level 2: æ ¸å¿ƒç‰¹æ€§ï¼ˆæ·±å…¥ç†è§£ï¼‰
å±•ç¤º2-3ä¸ªè¯¥çŸ¥è¯†ç‚¹çš„å…³é”®ç‰¹æ€§æˆ–é«˜çº§ç”¨æ³•ï¼Œæ¯ä¸ªç‰¹æ€§é…ä¸€ä¸ªå®Œæ•´çš„ä»£ç ç¤ºä¾‹å’Œç®€è¦è¯´æ˜ã€‚

#### ç‰¹æ€§1: [ç‰¹æ€§åç§°]
(ç®€è¦è¯´æ˜)
```python
# ç¤ºä¾‹ä»£ç 
```

#### ç‰¹æ€§2: [ç‰¹æ€§åç§°]
(ç®€è¦è¯´æ˜)
```python
# ç¤ºä¾‹ä»£ç 
```

### ğŸ” Level 3: å¯¹æ¯”å­¦ä¹ ï¼ˆé¿å…é™·é˜±ï¼‰
é€šè¿‡å¯¹æ¯”â€œé”™è¯¯ç”¨æ³•â€å’Œâ€œæ­£ç¡®ç”¨æ³•â€æ¥å±•ç¤ºå¸¸è§çš„é™·é˜±æˆ–æ˜“æ··æ·†çš„æ¦‚å¿µã€‚æ¯ä¸ªç”¨æ³•éƒ½å¿…é¡»æœ‰å®Œæ•´çš„ä»£ç ç¤ºä¾‹å’Œæ¸…æ™°çš„è§£é‡Šã€‚

```python
# === é”™è¯¯ç”¨æ³• ===
# âŒ å±•ç¤ºå¸¸è§é”™è¯¯
# è§£é‡Šä¸ºä»€ä¹ˆæ˜¯é”™çš„

# === æ­£ç¡®ç”¨æ³• ===
# âœ… å±•ç¤ºæ­£ç¡®åšæ³•
# è§£é‡Šä¸ºä»€ä¹ˆè¿™æ ·æ˜¯å¯¹çš„
```

### ğŸš€ Level 4: å®æˆ˜åº”ç”¨ï¼ˆçœŸå®åœºæ™¯ï¼‰
è®¾è®¡ä¸€ä¸ªç”ŸåŠ¨æœ‰è¶£çš„å®æˆ˜åœºæ™¯æ¥ç»¼åˆè¿ç”¨è¯¥çŸ¥è¯†ç‚¹ã€‚åœºæ™¯è¦å¯Œæœ‰åˆ›æ„ï¼Œä¾‹å¦‚æ¸¸æˆã€ç§‘å¹»ã€ç”Ÿæ´»è¶£äº‹ç­‰ï¼Œé¿å…æ¯ç‡¥çš„çº¯ç†è®ºæˆ–å•†ä¸šæ¡ˆä¾‹ã€‚ä»£ç éœ€å®Œæ•´ï¼Œå¹¶æœ‰æ¸…æ™°çš„è¾“å‡ºç»“æœã€‚

**åœºæ™¯ï¼š** [é€‰æ‹©ä¸€ä¸ªæœ‰è¶£çš„åœºæ™¯ï¼Œå¦‚ï¼šğŸ® æ¸¸æˆè§’è‰²å±æ€§è®¡ç®—å™¨, ğŸš€ æ˜Ÿé™…é£èˆ¹å¯¼èˆªç³»ç»Ÿ, ğŸ• æŠ«è¨è®¢å•å¤„ç†å™¨, ğŸ¾ è™šæ‹Ÿå® ç‰©äº’åŠ¨ç­‰]

```python
# å®æˆ˜åœºæ™¯çš„å®Œæ•´ä»£ç 
```

### ğŸ’¡ è®°å¿†è¦ç‚¹
- **è¦ç‚¹1**: [æ€»ç»“ç¬¬ä¸€ä¸ªå…³é”®è®°å¿†ç‚¹]
- **è¦ç‚¹2**: [æ€»ç»“ç¬¬äºŒä¸ªå…³é”®è®°å¿†ç‚¹]
- **è¦ç‚¹3**: [æ€»ç»“ç¬¬ä¸‰ä¸ªå…³é”®è®°å¿†ç‚¹]
"""
    ]
    constraints = [
        "ã€è¾“å‡ºè¦æ±‚ã€‘",
        "- **å¾ªåºæ¸è¿›**: ä»æœ€ç®€å•çš„æ¦‚å¿µåˆ°å¤æ‚çš„åº”ç”¨ã€‚"
        "- **é‡ç‚¹çªå‡º**: ä½¿ç”¨åŠ ç²—ã€åˆ—è¡¨ç­‰æ–¹å¼çªå‡ºæ ¸å¿ƒçŸ¥è¯†ã€‚"
        "- **ç”ŸåŠ¨æœ‰è¶£**: Level 4çš„å®æˆ˜åœºæ™¯è¦å¯Œæœ‰æƒ³è±¡åŠ›ï¼Œä½¿ç”¨Emojiå¢åŠ è¶£å‘³æ€§ã€‚"
        "- **ä»£ç å¯è¿è¡Œ**: æ‰€æœ‰ä»£ç å—éƒ½å¿…é¡»æ˜¯ç‹¬ç«‹çš„ã€å®Œæ•´çš„ã€å¯ä»¥ç›´æ¥å¤åˆ¶è¿è¡Œçš„ã€‚"
        "- **ä¸­æ–‡è®²è§£**: æ‰€æœ‰è§£é‡Šå’Œæ³¨é‡Šéƒ½ä½¿ç”¨ä¸­æ–‡ã€‚"
    ]
    prompt = []
    prompt.extend(header)
    prompt.append("\nã€æ•™å­¦è®¾è®¡å›¾ã€‘\n" + json.dumps(design_obj, ensure_ascii=False, indent=2))
    if ctx:
        prompt.append("\n" + ctx)
    prompt.append("\nã€è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹Markdownç»“æ„ç”Ÿæˆå†…å®¹ï¼Œç¡®ä¿æ¯ä¸ªä»£ç å—éƒ½æ˜¯å®Œæ•´ã€å¯ç‹¬ç«‹è¿è¡Œçš„ã€‘\n" + "\n".join(sections))
    prompt.append("\n" + "\n".join(constraints))
    return "\n\n".join(prompt)


async def _classify_subject_async(llm, subject: str) -> str:
    prompt = PROMPT_CLASSIFY_SUBJECT.replace("[subject]", subject)
    try:
        text = await llm.ainvoke(prompt)
        t = (text or "").strip().lower()
        m = re.search(r"\b(theory|tool)\b", t)
        if m:
            return m.group(1)
    except Exception:
        pass
    return "theory"


async def _gen_one_point(llm, prompt: str, retries: int, delay: int, debug: bool = False, tag: str = "generate") -> str:
    last = ""
    for _ in range(max(1, retries)):
        try:
            if debug:
                logging.getLogger(__name__).debug("\n==== LLM Prompt [%s] BEGIN ====\n%s\n==== LLM Prompt [%s] END ====\n", tag, prompt, tag)
            last = await llm.ainvoke(prompt)
            if last:
                return last
        except Exception as e:
            logging.getLogger(__name__).error(f"ç”Ÿæˆè°ƒç”¨å¤±è´¥: {e}")
        await asyncio.sleep(max(1, delay))
    return last


async def _review_one_point_with_context(llm, point_id: str, content_md: str, peer_points: List[Dict[str, str]], debug: bool = False) -> Dict[str, Any]:
    review_prompt_template = '''ä½ æ˜¯èµ„æ·±çš„æŠ€æœ¯ç¼–è¾‘ï¼Œä½ çš„ä»»åŠ¡æ˜¯å®¡æŸ¥ä¸‹é¢çš„è‰ç¨¿ï¼Œå¹¶ä»¥JSONæ ¼å¼æä¾›å…·ä½“çš„ã€å¯æ“ä½œçš„åé¦ˆã€‚

ã€å®¡æŸ¥ç»´åº¦ã€‘
1. å‡†ç¡®æ€§: å†…å®¹ä¸ä»£ç æ˜¯å¦æŠ€æœ¯ä¸Šå‡†ç¡®ï¼Ÿ
2. æ¸…æ™°åº¦: è§£é‡Šæ˜¯å¦æ˜“æ‡‚ï¼Ÿç¤ºä¾‹æ˜¯å¦æ¸…æ™°ï¼Ÿ
3. å®Œæ•´æ€§: æ˜¯å¦é—æ¼å…³é”®æ¦‚å¿µæˆ–æ­¥éª¤ï¼Ÿ
4. ä¸€è‡´æ€§: æ˜¯å¦ä¸æ ‡é¢˜åŠå…¶åœ¨è¯¾ç¨‹å¤§çº²ä¸­çš„å®šä½ç›¸ç¬¦ï¼Ÿ

ã€åˆ†ç±»è¦æ±‚ï¼ˆéå¸¸é‡è¦ï¼‰ã€‘
å¯¹æ¯ä¸ªé—®é¢˜è¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä¼°è®¡ä¿¡å¿ƒåº¦(confidence: 0~1)ã€‚åˆ†ç±»categoryä»…èƒ½å–ä»¥ä¸‹å€¼ä¹‹ä¸€ï¼š
- formatting, typo, heading, link_fix, reference, style, redundancy, minor_clarity, minor_structure, example_polish,
- factual_error, code_bug, algorithm_logic, security, api_breaking_change

ã€è¾“å‡ºæ ¼å¼ï¼ˆä»…è¾“å‡ºä¸€ä¸ªJSONå¯¹è±¡ï¼Œæ— ä»»ä½•é¢å¤–æ–‡æœ¬ï¼‰ã€‘
é¡¶å±‚é”®ï¼š
- is_perfect: å¸ƒå°”ï¼›è‹¥æ— éœ€ä»»ä½•ä¿®æ”¹åˆ™ä¸º trueã€‚
- issues: æ•°ç»„ï¼›è‹¥ is_perfect=true åˆ™ä¸ºç©ºæ•°ç»„ã€‚

æ¯ä¸ª issue å¿…é¡»åŒ…å«ï¼š
- severity: 'major' | 'minor'
- category: ä¸Šè¿°åˆ†ç±»ä¹‹ä¸€
- confidence: 0~1 ä¹‹é—´çš„å°æ•°
- description: å­—ç¬¦ä¸²ï¼Œé—®é¢˜æè¿°
- suggestion: å­—ç¬¦ä¸²ï¼Œå…·ä½“ä¸”å¯æ‰§è¡Œçš„ä¿®å¤å»ºè®®

ã€ä¸Šä¸‹æ–‡ã€‘
[æ–‡ä»¶ID] {point_id}
[åŒç« èŠ‚å…¶ä»–çŸ¥è¯†ç‚¹]
{peers_lines}

ã€å½“å‰å†…å®¹ã€‘
{content_md}

ã€ä½ çš„JSONè¾“å‡ºã€‘
'''
    peers_lines = "\n".join([f"- {p.get('id', '')}: {p.get('title', '')}" for p in peer_points])
    prompt = review_prompt_template.format(point_id=point_id, peers_lines=peers_lines if peers_lines else '(æ— )', content_md=content_md)
    if debug:
        logging.getLogger(__name__).debug("\n==== LLM Prompt [review] BEGIN ====\n%s\n==== LLM Prompt [review] END ====\n", prompt)
    try:
        text = await llm.ainvoke(prompt)
        obj = try_parse_json_object(text)
        if obj:
            obj.setdefault("file_id", point_id)
            return obj
    except Exception as e:
        logging.getLogger(__name__).error(f"å¸¦ä¸Šä¸‹æ–‡å®¡æŸ¥å¤±è´¥: {e}")
    return {
        "file_id": point_id,
        "is_perfect": False,
        "issues": [{
            "severity": "major",
            "description": "å®¡æŸ¥èŠ‚ç‚¹æ‰§è¡Œæˆ–JSONè§£æå¤±è´¥ã€‚",
            "suggestion": "è¯·æ£€æŸ¥ä¸Šæ¸¸å†…å®¹æˆ–å®¡æŸ¥æ¨¡å‹çš„è¾“å‡ºæ˜¯å¦ç¨³å®šã€‚"
        }]
    }


def _severity_score(item: Dict[str, Any]) -> int:
    if item.get("is_perfect", False):
        return 0
    score = 0
    for issue in item.get("issues", []) or []:
        if issue.get("severity") == "major":
            score += 2
        else:
            score += 1
    return score


def _has_non_ok(item: Dict[str, Any]) -> bool:
    if item.get("is_perfect", False):
        return False
    return bool(item.get("issues"))


AUTO_APPLY_DEFAULTS = {
    "auto_apply_mode": "off",  # off | safe | aggressive | all
    "auto_apply_threshold_major": 0.8,
}


def _should_auto_apply_by_review(cfg: Dict[str, Any], review_obj: Dict[str, Any]) -> Tuple[bool, str]:
    mode = str(cfg.get("auto_apply_mode") or AUTO_APPLY_DEFAULTS["auto_apply_mode"]).strip().lower()
    if mode == "off":
        return False, "off"
    if mode == "all":
        return True, "all"
    issues = review_obj.get("issues") if isinstance(review_obj.get("issues"), list) else []
    if review_obj.get("is_perfect", False) and not issues:
        return False, "is_perfect"
    majors_conf: List[float] = []
    for it in issues:
        if isinstance(it, dict) and str(it.get("severity", "")).lower() == "major":
            try:
                majors_conf.append(float(it.get("confidence", 1)))
            except Exception:
                majors_conf.append(0.0)
    if mode == "safe":
        ok = len(majors_conf) == 0
        return ok, "safe: all minor" if ok else "safe: has major"
    if mode == "aggressive":
        if not majors_conf:
            return True, "aggressive: all minor"
        thr = float(cfg.get("auto_apply_threshold_major", AUTO_APPLY_DEFAULTS["auto_apply_threshold_major"]))
        ok = min(majors_conf) >= thr
        return ok, (f"aggressive: majors_conf>={thr}" if ok else f"aggressive: majors_conf<{thr}")
    return False, f"unknown_mode:{mode}"


def _render_unified_diff(old: str, new: str, context_lines: int = 3, max_output_lines: int = 120) -> str:
    import difflib
    old_lines = (old or "").splitlines()
    new_lines = (new or "").splitlines()
    diff = list(difflib.unified_diff(old_lines, new_lines, fromfile="åŸç¨¿", tofile="ä¿®è®¢", lineterm="", n=context_lines))
    if len(diff) > max_output_lines:
        head = diff[: max_output_lines // 2]
        tail = diff[-max_output_lines // 2 :]
        diff = head + ["...ï¼ˆdiff çœç•¥ï¼‰..."] + tail
    return "\n".join(diff)


async def _propose_fix(
    llm,
    point_id: str,
    point_title: str,
    topic: str,
    outline_md: str,
    current_md: str,
    review: Dict[str, Any],
    prior_proposal: Optional[Dict[str, Any]] = None,
    user_feedback: str = "",
    debug: bool = False,
) -> Dict[str, Any]:
    constraints = (
        "è¯·ä»…è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸è¦ä»»ä½•é¢å¤–æ–‡å­—ï¼›é”®ï¼š\n"
        "- summary: å¯¹éœ€è¦ä¿®æ”¹ç‚¹ä¸æ”¹åŠ¨çš„ç®€è¦è¯´æ˜ï¼ˆä¸­æ–‡ï¼Œ100-200å­—ï¼‰ï¼›\n"
        "- revised_content: ä¿®è®¢åçš„å®Œæ•´ Markdown å†…å®¹ï¼ˆå¿…é¡»æ˜¯å®Œæ•´æ›¿æ¢ç¨¿è€Œéç‰‡æ®µï¼‰ï¼›\n"
        "- risk (å¯é€‰): 'low'|'medium'|'high'ï¼›\n"
        "- change_categories (å¯é€‰): æ•°ç»„ï¼Œå‚è€ƒå®¡æŸ¥åˆ†ç±»ï¼›\n"
        "- notes (å¯é€‰): å¯¹ä¿®å¤èŒƒå›´çš„ç®€çŸ­è¯´æ˜ã€‚"
    )
    feedback_part = f"\n[ç”¨æˆ·åé¦ˆ]\n{user_feedback}\n" if user_feedback else ""
    prior_part = f"\n[ä¸Šä¸€ç‰ˆä¿®å¤æ–¹æ¡ˆ]\n{json.dumps(prior_proposal, ensure_ascii=False)}\n" if prior_proposal else ""
    prompt = (
        "ä½ æ˜¯ä¸¥è°¨çš„æŠ€æœ¯ç¼–è¾‘ä¸ä½œè€…ã€‚åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼Œæå‡ºä¿®å¤ææ¡ˆå¹¶ç»™å‡ºä¿®è®¢åå®Œæ•´å†…å®¹ã€‚\n\n"
        f"[ä¸»é¢˜]\n{topic}\n\n"
        f"[çŸ¥è¯†ç‚¹]\n{point_title} (ID: {point_id})\n\n"
        f"[å¤§çº²]\n{outline_md}\n\n"
        f"[å½“å‰å†…å®¹]\n{current_md}\n\n"
        f"[å®¡æŸ¥ç»“æœ]\n{json.dumps(review, ensure_ascii=False)}\n"
        f"{prior_part}{feedback_part}\n"
        f"{constraints}\n"
    )
    if debug:
        logging.getLogger(__name__).debug("\n==== LLM Prompt [propose_fix] BEGIN ====\n%s\n==== LLM Prompt [propose_fix] END ====\n", prompt)
    try:
        text = await llm.ainvoke(prompt)
        obj = try_parse_json_object(text)
        if isinstance(obj, dict) and obj.get("revised_content"):
            return obj
    except Exception as e:
        logging.getLogger(__name__).error(f"ç”Ÿæˆä¿®å¤æ–¹æ¡ˆå¤±è´¥: {e}")
    return {
        "summary": "è‡ªåŠ¨ç”Ÿæˆä¿®å¤æ–¹æ¡ˆå¤±è´¥ï¼Œå»ºè®®äººå·¥æ£€æŸ¥å¹¶å®Œå–„ã€‚",
        "revised_content": current_md or "",
    }


# ----------------------------
# èŠ‚ç‚¹ï¼ˆå¤åˆ¶ç‰ˆï¼‰
# ----------------------------

async def generate_and_review_by_chapter_node(state: WorkState, llm_generate, llm_review) -> WorkState:
    cfg = state.get("config", {})
    max_parallel = int(cfg.get("max_parallel_requests", 8))
    retries = int(cfg.get("retry_times", 3))
    delay = int(cfg.get("retry_delay", 10))
    sem = asyncio.Semaphore(max_parallel)

    outline = state.get("outline_struct", {}) or {}
    topic = state.get("topic", "")
    language = str((state.get("topic_meta", {}) or {}).get("lang", "zh"))

    # path æ˜ å°„ id -> å¯è¯»è·¯å¾„
    path_by_id: Dict[str, str] = {}
    chapters_struct = outline.get("chapters") or []
    for ci, ch in enumerate(chapters_struct, start=1):
        ch_title = ch.get("title", f"ç¬¬{ci}ç« ")
        for gi, gr in enumerate(ch.get("groups") or [], start=1):
            gr_title = gr.get("title", f"{ci}.{gi} å°èŠ‚")
            for si, sec in enumerate(gr.get("sections") or [], start=1):
                sid = sec.get("id") or ""
                stitle = sec.get("title") or f"{ci}.{gi}.{si}"
                if sid:
                    path_by_id[sid] = f"{topic} / {gr_title} / {stitle}"

    chapters_ordered = chapters_struct
    selected_titles = state.get("selected_chapters") or [c.get("title", "") for c in chapters_ordered]

    # autosave ç›®å½•
    topic_slug = state.get("topic_slug", "topic")
    out_dir = BASE_DIR / "output" / topic_slug
    drafts_dir = out_dir / "drafts"
    reviews_dir = out_dir / "reviews"
    ensure_dir(drafts_dir)
    ensure_dir(reviews_dir)

    drafts_all: List[Dict[str, str]] = []
    reviews_all: List[Dict[str, Any]] = []
    failures_all: List[Dict[str, Any]] = []

    async def _process_one_group(ci: int, gi: int, ch: Dict[str, Any], gr: Dict[str, Any]) -> Tuple[List[Dict[str, str]], List[Dict[str, Any]], List[Dict[str, Any]]]:
        ch_title = ch.get("title", f"ç¬¬{ci}ç« ")
        # æŒ‰ç”¨æˆ·é€‰æ‹©çš„ç« èŠ‚è¿›è¡Œè¿‡æ»¤
        if selected_titles and ch_title not in selected_titles:
            return [], [], []
        gr_title = gr.get("title", f"{ci}.{gi} å°èŠ‚")
        stype = str(gr.get("structure_type", "toolbox")).strip().lower()
        sections = gr.get("sections") or []
        if not sections:
            return [], [], []

        logging.getLogger(__name__).info(f"  â””â”€ å°èŠ‚ï¼š{gr_title} ç»“æ„={stype}ï¼ŒçŸ¥è¯†ç‚¹={len(sections)}")

        group_drafts: Dict[str, str] = {}
        # Generation
        if stype == "pipeline":
            group_context = ""
            for si, sec in enumerate(sections, start=1):
                sid = sec.get("id") or ""
                stitle = sec.get("title") or f"{ci}.{gi}.{si}"
                primary_goal = sec.get("primary_goal") or sec.get("goal") or ""
                mods = sec.get("suggested_modules") if isinstance(sec.get("suggested_modules"), list) else []
                conts = sec.get("suggested_contents") if isinstance(sec.get("suggested_contents"), list) else []
                if si == 1:
                    prompt = _build_theory_opening_prompt(
                        topic=topic,
                        language=language,
                        path=path_by_id.get(sid, ""),
                        section_title=stitle,
                        primary_goal=str(primary_goal),
                        suggested_modules=mods if isinstance(mods, list) else [],
                        suggested_contents=conts,
                        current_chapter_index=ci,
                        all_chapters_struct=chapters_struct,
                    )
                else:
                    prompt = _build_contextual_content_prompt(
                        topic=topic,
                        language=language,
                        path=path_by_id.get(sid, ""),
                        section_title=stitle,
                        primary_goal=str(primary_goal),
                        suggested_modules=mods if isinstance(mods, list) else [],
                        suggested_contents=conts,
                        structure_type="pipeline",
                        relation_to_previous=str(sec.get("relation_to_previous") or ""),
                        prior_context=group_context,
                    )
                async with sem:
                    txt = await _gen_one_point(llm_generate, prompt, retries, delay, debug=bool(cfg.get("debug")), tag="generate")
                if cfg.get("sanitize_mermaid", True):
                    txt_s, _issues = sanitize_mermaid_in_markdown(txt or "")
                else:
                    txt_s = txt or ""
                group_drafts[sid] = txt_s
                if txt_s:
                    group_context = (group_context + "\n\n" + txt_s).strip()
                try:
                    (drafts_dir / f"{sid}.md").write_text(txt_s, encoding="utf-8")
                except Exception:
                    pass
        else:
            indices = list(range(len(sections)))
            def _rel(i: int) -> str:
                return str((sections[i] or {}).get("relation_to_previous", "")).strip().lower()

            roots = [i for i in indices if _rel(i) in {"first_in_sequence", "tool_in_toolbox", "alternative_to", ""}]
            pending = [i for i in indices if i not in roots]

            async def _gen_root(i: int):
                sec = sections[i]
                sid = sec.get("id") or ""
                stitle = sec.get("title") or ""
                primary_goal = sec.get("primary_goal") or sec.get("goal") or ""
                mods = sec.get("suggested_modules") if isinstance(sec.get("suggested_modules"), list) else []
                conts = sec.get("suggested_contents") if isinstance(sec.get("suggested_contents"), list) else []
                base = _build_contextual_content_prompt(
                    topic=topic,
                    language=language,
                    path=path_by_id.get(sid, ""),
                    section_title=stitle,
                    primary_goal=str(primary_goal),
                    suggested_modules=mods,
                    suggested_contents=conts,
                    structure_type="toolbox",
                    prior_context="",
                )
                async with sem:
                    txt = await _gen_one_point(llm_generate, base, retries, delay, debug=bool(cfg.get("debug")), tag="generate")
                if cfg.get("sanitize_mermaid", True):
                    txt_s, _issues = sanitize_mermaid_in_markdown(txt or "")
                else:
                    txt_s = txt or ""
                group_drafts[sid] = txt_s
                try:
                    (drafts_dir / f"{sid}.md").write_text(txt_s, encoding="utf-8")
                except Exception:
                    pass

            if roots:
                await asyncio.gather(*[_gen_root(i) for i in roots])

            produced_set = set([sections[i].get("id") for i in roots])
            loop_guard = 0
            while pending and loop_guard < len(sections) * 2:
                ready: List[int] = []
                for i in list(pending):
                    dep = _rel(i)
                    if dep not in {"builds_on", "deep_dive_into"}:
                        ready.append(i)
                        continue
                    parent_idx = i - 1
                    if parent_idx >= 0:
                        parent_id = sections[parent_idx].get("id")
                        if parent_id in produced_set:
                            ready.append(i)
                if not ready:
                    ready = pending[:]

                async def _gen_dep(i: int):
                    sec = sections[i]
                    sid = sec.get("id") or ""
                    stitle = sec.get("title") or ""
                    primary_goal = sec.get("primary_goal") or sec.get("goal") or ""
                    mods = sec.get("suggested_modules") if isinstance(sec.get("suggested_modules"), list) else []
                    conts = sec.get("suggested_contents") if isinstance(sec.get("suggested_contents"), list) else []
                    base = _build_contextual_content_prompt(
                        topic=topic,
                        language=language,
                        path=path_by_id.get(sid, ""),
                        section_title=stitle,
                        primary_goal=str(primary_goal),
                        suggested_modules=mods,
                        suggested_contents=conts,
                        structure_type="toolbox",
                        prior_context=group_drafts.get(sections[i-1].get("id"), "") if i > 0 else "",
                    )
                    async with sem:
                        txt = await _gen_one_point(llm_generate, base, retries, delay, debug=bool(cfg.get("debug")), tag="generate")
                    if cfg.get("sanitize_mermaid", True):
                        txt_s, _issues = sanitize_mermaid_in_markdown(txt or "")
                    else:
                        txt_s = txt or ""
                    group_drafts[sid] = txt_s
                    try:
                        (drafts_dir / f"{sid}.md").write_text(txt_s, encoding="utf-8")
                    except Exception:
                        pass

                await asyncio.gather(*[_gen_dep(i) for i in ready])
                for i in ready:
                    produced_set.add(sections[i].get("id"))
                    if i in pending:
                        pending.remove(i)
                loop_guard += 1

        # Reviews for this group (optional)
        group_reviews: List[Dict[str, Any]] = []
        if not cfg.get("skip_content_review", False):
            peer_meta = [{"id": sec.get("id"), "title": sec.get("title")} for sec in sections if sec.get("id")]
            async def _review_one_for_group(sec: Dict[str, Any]) -> Dict[str, Any]:
                pid = sec.get("id") or ""
                peers = [pm for pm in peer_meta if (pm.get("id") or "") != pid]
                content = group_drafts.get(pid, "")
                async with sem:
                    rv = await _review_one_point_with_context(llm_review, pid, content, peers, debug=bool(cfg.get("debug")))
                try:
                    (reviews_dir / f"{pid}.json").write_text(json.dumps(rv, ensure_ascii=False, indent=2), encoding="utf-8")
                except Exception:
                    pass
                return rv

            group_reviews = await asyncio.gather(*[_review_one_for_group(sec) for sec in sections if sec.get("id")])
        # Collect for this group
        drafts_list = []
        for sec in sections:
            sid = sec.get("id") or ""
            if sid:
                drafts_list.append({"id": sid, "content": group_drafts.get(sid, "")})
        failures_list = []
        for rv in group_reviews:
            if _severity_score(rv) >= 3:
                failures_list.append({"id": rv.get("file_id"), "review": rv})
        return drafts_list, group_reviews, failures_list

    async def _process_one_chapter(ci: int, ch: Dict[str, Any]) -> Tuple[List[Dict[str, str]], List[Dict[str, Any]], List[Dict[str, Any]]]:
        ch_title = ch.get("title", f"ç¬¬{ci}ç« ")
        if selected_titles and ch_title not in selected_titles:
            return [], [], []
        logging.getLogger(__name__).info(f"[ç»“æ„æ„ŸçŸ¥] å¤„ç†ç« èŠ‚ï¼šã€Š{ch_title}ã€‹ â€¦")
        groups = ch.get("groups") or []
        tasks = [
            _process_one_group(ci, gi, ch, gr)
            for gi, gr in enumerate(groups, start=1)
            if (gr.get("sections") or [])
        ]
        if not tasks:
            return [], [], []
        results = await asyncio.gather(*tasks)
        drafts_list: List[Dict[str, str]] = []
        reviews_list: List[Dict[str, Any]] = []
        failures_list: List[Dict[str, Any]] = []
        for d, r, f in results:
            drafts_list.extend(d)
            reviews_list.extend(r)
            failures_list.extend(f)
        return drafts_list, reviews_list, failures_list

    chapter_tasks = [
        _process_one_chapter(ci, ch)
        for ci, ch in enumerate(chapters_struct, start=1)
    ]
    if chapter_tasks:
        chapter_results = await asyncio.gather(*chapter_tasks)
        for d, r, f in chapter_results:
            drafts_all.extend(d)
            reviews_all.extend(r)
            failures_all.extend(f)

    return {**state, "drafts": drafts_all, "reviews": reviews_all, "failures": failures_all}


async def generate_and_review_by_chapter_node_tool(state: WorkState, llm_generate, llm_review) -> WorkState:
    """å·¥å…·å‹ä¸»é¢˜ç‰ˆæœ¬ï¼šæç¤ºè¯é‡‡ç”¨ Prompt 2ï¼ˆå·¥å…·ç±»ï¼‰ï¼Œå¹¶ä¿æŒå¹¶å‘ä¸ä¸Šä¸‹æ–‡ç­–ç•¥ä¸€è‡´ã€‚"""
    cfg = state.get("config", {})
    max_parallel = int(cfg.get("max_parallel_requests", 8))
    retries = int(cfg.get("retry_times", 3))
    delay = int(cfg.get("retry_delay", 10))
    sem = asyncio.Semaphore(max_parallel)

    outline = state.get("outline_struct", {}) or {}
    topic = state.get("topic", "")
    language = str((state.get("topic_meta", {}) or {}).get("lang", "zh"))

    path_by_id: Dict[str, str] = {}
    chapters_struct = outline.get("chapters") or []
    for ci, ch in enumerate(chapters_struct, start=1):
        ch_title = ch.get("title", f"ç¬¬{ci}ç« ")
        for gi, gr in enumerate(ch.get("groups") or [], start=1):
            gr_title = gr.get("title", f"{ci}.{gi} å°èŠ‚")
            for si, sec in enumerate(gr.get("sections") or [], start=1):
                sid = sec.get("id") or ""
                stitle = sec.get("title") or f"{ci}.{gi}.{si}"
                if sid:
                    path_by_id[sid] = f"{topic} / ç¬¬{ci}ç« ï¼š{ch_title} / {gr_title} / {stitle}"

    chapters_ordered = chapters_struct
    selected_titles = state.get("selected_chapters") or [c.get("title", "") for c in chapters_ordered]

    topic_slug = state.get("topic_slug", "topic")
    out_dir = BASE_DIR / "output" / topic_slug
    drafts_dir = out_dir / "drafts"
    reviews_dir = out_dir / "reviews"
    ensure_dir(drafts_dir)
    ensure_dir(reviews_dir)

    drafts_all: List[Dict[str, str]] = []
    reviews_all: List[Dict[str, Any]] = []
    failures_all: List[Dict[str, Any]] = []

    async def _process_one_group(ci: int, gi: int, ch: Dict[str, Any], gr: Dict[str, Any]) -> Tuple[List[Dict[str, str]], List[Dict[str, Any]], List[Dict[str, Any]]]:
        ch_title = ch.get("title", f"ç¬¬{ci}ç« ")
        # æŒ‰ç”¨æˆ·é€‰æ‹©çš„ç« èŠ‚è¿›è¡Œè¿‡æ»¤
        if selected_titles and ch_title not in selected_titles:
            return [], [], []
        gr_title = gr.get("title", f"{ci}.{gi} å°èŠ‚")
        stype = str(gr.get("structure_type", "toolbox")).strip().lower()
        sections = gr.get("sections") or []
        if not sections:
            return [], [], []

        logging.getLogger(__name__).info(f"  â””â”€ å°èŠ‚ï¼š{gr_title} ç»“æ„={stype}ï¼ŒçŸ¥è¯†ç‚¹={len(sections)} [tool-mode]")

        group_drafts: Dict[str, str] = {}
        if stype == "pipeline":
            group_context = ""
            for si, sec in enumerate(sections, start=1):
                sid = sec.get("id") or ""
                stitle = sec.get("title") or f"{ci}.{gi}.{si}"
                primary_goal = sec.get("primary_goal") or sec.get("goal") or ""
                mods = sec.get("suggested_modules") if isinstance(sec.get("suggested_modules"), list) else []
                conts = sec.get("suggested_contents") if isinstance(sec.get("suggested_contents"), list) else []
                prompt = _build_tool_content_prompt(
                    topic=topic,
                    language=language,
                    path=path_by_id.get(sid, ""),
                    section_title=stitle,
                    primary_goal=str(primary_goal),
                    suggested_modules=mods,
                    suggested_contents=conts,
                    structure_type="pipeline",
                    relation_to_previous=str(sec.get("relation_to_previous") or ""),
                    prior_context=group_context,
                )
                async with sem:
                    txt = await _gen_one_point(llm_generate, prompt, retries, delay, debug=bool(cfg.get("debug")), tag="generate")
                if cfg.get("sanitize_mermaid", True):
                    txt_s, _issues = sanitize_mermaid_in_markdown(txt or "")
                else:
                    txt_s = txt or ""
                group_drafts[sid] = txt_s
                if txt_s:
                    group_context = (group_context + "\n\n" + txt_s).strip()
                try:
                    (drafts_dir / f"{sid}.md").write_text(txt_s, encoding="utf-8")
                except Exception:
                    pass
        else:
            indices = list(range(len(sections)))
            def _rel(i: int) -> str:
                return str((sections[i] or {}).get("relation_to_previous", "")).strip().lower()

            roots = [i for i in indices if _rel(i) in {"first_in_sequence", "tool_in_toolbox", "alternative_to", ""}]
            pending = [i for i in indices if i not in roots]

            async def _gen_root(i: int):
                sec = sections[i]
                sid = sec.get("id") or ""
                stitle = sec.get("title") or ""
                primary_goal = sec.get("primary_goal") or sec.get("goal") or ""
                mods = sec.get("suggested_modules") if isinstance(sec.get("suggested_modules"), list) else []
                conts = sec.get("suggested_contents") if isinstance(sec.get("suggested_contents"), list) else []
                base = _build_tool_content_prompt(
                    topic=topic,
                    language=language,
                    path=path_by_id.get(sid, ""),
                    section_title=stitle,
                    primary_goal=str(primary_goal),
                    suggested_modules=mods,
                    suggested_contents=conts,
                    structure_type="toolbox",
                    prior_context="",
                )
                async with sem:
                    txt = await _gen_one_point(llm_generate, base, retries, delay, debug=bool(cfg.get("debug")), tag="generate")
                if cfg.get("sanitize_mermaid", True):
                    txt_s, _issues = sanitize_mermaid_in_markdown(txt or "")
                else:
                    txt_s = txt or ""
                group_drafts[sid] = txt_s
                try:
                    (drafts_dir / f"{sid}.md").write_text(txt_s, encoding="utf-8")
                except Exception:
                    pass

            if roots:
                await asyncio.gather(*[_gen_root(i) for i in roots])

            produced_set = set([sections[i].get("id") for i in roots])
            loop_guard = 0
            while pending and loop_guard < len(sections) * 2:
                ready: List[int] = []
                for i in list(pending):
                    dep = _rel(i)
                    if dep not in {"builds_on", "deep_dive_into"}:
                        ready.append(i)
                        continue
                    parent_idx = i - 1
                    if parent_idx >= 0:
                        parent_id = sections[parent_idx].get("id")
                        if parent_id in produced_set:
                            ready.append(i)
                if not ready:
                    ready = pending[:]

                async def _gen_dep(i: int):
                    sec = sections[i]
                    sid = sec.get("id") or ""
                    stitle = sec.get("title") or ""
                    primary_goal = sec.get("primary_goal") or sec.get("goal") or ""
                    mods = sec.get("suggested_modules") if isinstance(sec.get("suggested_modules"), list) else []
                    conts = sec.get("suggested_contents") if isinstance(sec.get("suggested_contents"), list) else []
                    parent_txt = ""
                    if i - 1 >= 0:
                        parent_id = sections[i-1].get("id") or ""
                        parent_txt = group_drafts.get(parent_id, "")
                    prompt = _build_tool_content_prompt(
                        topic=topic,
                        language=language,
                        path=path_by_id.get(sid, ""),
                        section_title=stitle,
                        primary_goal=str(primary_goal),
                        suggested_modules=mods,
                        suggested_contents=conts,
                        structure_type="toolbox",
                        relation_to_previous=_rel(i),
                        prior_context=parent_txt,
                    )
                    async with sem:
                        txt = await _gen_one_point(llm_generate, prompt, retries, delay, debug=bool(cfg.get("debug")), tag="generate")
                    if cfg.get("sanitize_mermaid", True):
                        txt_s, _issues = sanitize_mermaid_in_markdown(txt or "")
                    else:
                        txt_s = txt or ""
                    group_drafts[sid] = txt_s
                    try:
                        (drafts_dir / f"{sid}.md").write_text(txt_s, encoding="utf-8")
                    except Exception:
                        pass

                await asyncio.gather(*[_gen_dep(i) for i in ready])
                for i in ready:
                    produced_set.add(sections[i].get("id"))
                    if i in pending:
                        pending.remove(i)
                loop_guard += 1

        group_reviews: List[Dict[str, Any]] = []
        if not cfg.get("skip_content_review", False):
            peer_meta = [{"id": sec.get("id"), "title": sec.get("title")} for sec in sections if sec.get("id")]
            async def _review_one_for_group(sec: Dict[str, Any]) -> Dict[str, Any]:
                pid = sec.get("id") or ""
                peers = [pm for pm in peer_meta if (pm.get("id") or "") != pid]
                content = group_drafts.get(pid, "")
                async with sem:
                    rv = await _review_one_point_with_context(llm_review, pid, content, peers, debug=bool(cfg.get("debug")))
                try:
                    (reviews_dir / f"{pid}.json").write_text(json.dumps(rv, ensure_ascii=False, indent=2), encoding="utf-8")
                except Exception:
                    pass
                return rv
            group_reviews = await asyncio.gather(*[_review_one_for_group(sec) for sec in sections if sec.get("id")])

        drafts_list = []
        for sec in sections:
            sid = sec.get("id") or ""
            if sid:
                drafts_list.append({"id": sid, "content": group_drafts.get(sid, "")})
        failures_list = []
        for rv in group_reviews:
            if _severity_score(rv) >= 3:
                failures_list.append({"id": rv.get("file_id"), "review": rv})
        return drafts_list, group_reviews, failures_list

    async def _process_one_chapter(ci: int, ch: Dict[str, Any]) -> Tuple[List[Dict[str, str]], List[Dict[str, Any]], List[Dict[str, Any]]]:
        ch_title = ch.get("title", f"ç¬¬{ci}ç« ")
        if selected_titles and ch_title not in selected_titles:
            return [], [], []
        logging.getLogger(__name__).info(f"[ç»“æ„æ„ŸçŸ¥] å¤„ç†ç« èŠ‚ï¼šã€Š{ch_title}ã€‹ [tool-mode] â€¦")
        groups = ch.get("groups") or []
        tasks = [
            _process_one_group(ci, gi, ch, gr)
            for gi, gr in enumerate(groups, start=1)
            if (gr.get("sections") or [])
        ]
        if not tasks:
            return [], [], []
        results = await asyncio.gather(*tasks)
        drafts_list: List[Dict[str, str]] = []
        reviews_list: List[Dict[str, Any]] = []
        failures_list: List[Dict[str, Any]] = []
        for d, r, f in results:
            drafts_list.extend(d)
            reviews_list.extend(r)
            failures_list.extend(f)
        return drafts_list, reviews_list, failures_list

    chapter_tasks = [
        _process_one_chapter(ci, ch)
        for ci, ch in enumerate(chapters_struct, start=1)
    ]
    if chapter_tasks:
        chapter_results = await asyncio.gather(*chapter_tasks)
        for d, r, f in chapter_results:
            drafts_all.extend(d)
            reviews_all.extend(r)
            failures_all.extend(f)

    return {**state, "drafts": drafts_all, "reviews": reviews_all, "failures": failures_all}


async def propose_and_apply_fixes_node(state: WorkState, llm) -> WorkState:
    cfg_in = state.get("config", {}) or {}
    if cfg_in.get("skip_fixes", False):
        logging.getLogger(__name__).info("å·²è·³è¿‡ä¿®å¤ææ¡ˆä¸è‡ªåŠ¨åº”ç”¨ï¼ˆ--skip-fixesï¼‰")
        return {**state, "fix_proposals": [], "fix_applied": [], "fix_skipped": [], "fix_iterations": []}
    cfg: Dict[str, Any] = {**AUTO_APPLY_DEFAULTS, **cfg_in}
    max_rounds = int(cfg.get("max_fix_rounds", 3))

    topic = state.get("topic", "")
    outline_md = state.get("outline_final_md", "")

    draft_by_id: Dict[str, str] = {d.get("id", ""): d.get("content", "") for d in (state.get("drafts") or [])}
    reviews_in = state.get("reviews", []) or []
    reviews_by_id: Dict[str, Dict[str, Any]] = {}
    for rv in reviews_in:
        rid = str(rv.get("file_id") or rv.get("id") or "").strip()
        if not rid:
            continue
        reviews_by_id[rid] = rv

    title_by_id: Dict[str, str] = {p.id: p.title for p in (state.get("points") or [])}

    fix_proposals: List[Dict[str, Any]] = []
    fix_applied: List[str] = []
    fix_skipped: List[str] = []
    fix_iterations: List[Dict[str, Any]] = []
    auto_stats = {
        "mode": cfg.get("auto_apply_mode"),
        "applied": 0,
        "skipped": 0,
        "reasons": [],
    }

    target_ids = [pid for pid, rv in reviews_by_id.items() if _has_non_ok(rv)]
    if not target_ids:
        logging.getLogger(__name__).info("æ‰€æœ‰çŸ¥è¯†ç‚¹å®¡æŸ¥å‡ä¸º OKï¼Œæ— éœ€ä¿®å¤ã€‚")
        return {**state, "fix_proposals": [], "fix_applied": [], "fix_skipped": [], "fix_iterations": []}
    logging.getLogger(__name__).info(f"å‘ç° {len(target_ids)} ä¸ªéœ€è¦ä¿®å¤çš„çŸ¥è¯†ç‚¹ï¼ˆä»»ä¸€ç»´åº¦é OKï¼‰ã€‚")

    auto_ids: List[str] = []
    auto_reasons: Dict[str, str] = {}
    if (cfg.get("auto_apply_mode") or "off") != "off":
        for pid in target_ids:
            rv = reviews_by_id.get(pid, {})
            ok, why = _should_auto_apply_by_review(cfg, rv)
            if ok:
                auto_ids.append(pid)
                auto_reasons[pid] = why
            else:
                auto_stats["reasons"].append(f"skip {pid}: {why}")
        logging.getLogger(__name__).info(
            f"è‡ªåŠ¨åº”ç”¨ç­–ç•¥å¼€å¯: mode={cfg.get('auto_apply_mode')} | major_threshold={cfg.get('auto_apply_threshold_major', 0.8)}"
        )

    applied_set = set()

    sem_apply = asyncio.Semaphore(int(cfg.get("max_parallel_requests", 8)))

    async def _gen_and_apply(pid: str, auto_flag: bool, reason: str = ""):
        nonlocal fix_proposals, fix_applied, fix_iterations
        title = title_by_id.get(pid, pid)
        current = draft_by_id.get(pid, "")
        review = reviews_by_id.get(pid, {})
        async with sem_apply:
            proposal = await _propose_fix(llm, pid, title, topic, outline_md, current, review, debug=bool(cfg.get("debug")))
        revised = proposal.get("revised_content") or current
        if cfg.get("sanitize_mermaid", True):
            revised, _issues = sanitize_mermaid_in_markdown(revised)
        draft_by_id[pid] = revised
        fix_applied.append(pid)
        fix_iterations.append({"id": pid, "iterations": 1})
        applied_set.add(pid)
        fix_proposals.append({"id": pid, "title": title, **proposal, "applied": True, "iterations": 1, "auto_applied": auto_flag, **({"auto_reason": reason} if auto_flag else {})})

    if auto_ids:
        await asyncio.gather(*[_gen_and_apply(pid, True, auto_reasons.get(pid, "")) for pid in auto_ids])

    pending_ids = [pid for pid in target_ids if pid not in applied_set]
    if pending_ids:
        # éäº¤äº’ç¯å¢ƒï¼šé»˜è®¤è·³è¿‡ï¼Œåªè®°å½•ï¼ˆä¸ run_full è¡Œä¸ºä¸€è‡´ï¼‰
        fix_skipped.extend(pending_ids)

    out_drafts = []
    for d in (state.get("drafts") or []):
        pid = d.get("id", "")
        if pid in draft_by_id:
            out_drafts.append({"id": pid, "content": draft_by_id[pid]})
        else:
            out_drafts.append(d)

    return {
        **state,
        "drafts": out_drafts,
        "fix_proposals": fix_proposals,
        "fix_applied": fix_applied,
        "fix_skipped": fix_skipped,
        "fix_iterations": fix_iterations,
        "auto_apply_stats": auto_stats,
    }


def save_and_publish_node(state: WorkState) -> WorkState:
    topic_slug = state.get("topic_slug", "topic")
    out_dir = CONTENT_ROOT / topic_slug
    ensure_dir(out_dir)
    publish_paths: List[str] = []
    cfg = state.get("config", {}) or {}
    fname_style = str(cfg.get("filename_style", "id")).strip().lower()
    title_by_id: Dict[str, str] = {p.id: p.title for p in (state.get("points") or [])}
    for item in state.get("drafts", []) or []:
        pid = item.get("id", "")
        content = item.get("content", "")
        if cfg.get("sanitize_mermaid", True):
            content, _issues = sanitize_mermaid_in_markdown(content)
        # æ–‡ä»¶å‘½åè§„åˆ™ï¼š
        # - è‹¥é…ç½®ä¸º structuredï¼Œæ²¿ç”¨åŸç»“æ„åŒ–å‘½å
        # - å¦åˆ™ï¼ˆé»˜è®¤ï¼‰ï¼šä½¿ç”¨ id + æ¸…ç†åçš„æ ‡é¢˜ï¼ˆå»æ‰å‰ç¼€ç¼–å·ä¸å°¾éƒ¨è‹±æ–‡æ‹¬æ³¨ï¼‰
        if fname_style == "structured":
            name = _make_filename(pid, title_by_id.get(pid, ""), fname_style)
        else:
            title_clean = _clean_title_for_filename(title_by_id.get(pid, ""))
            name = f"{pid}-{title_clean}.md" if title_clean else f"{pid}.md"
        p = out_dir / name
        try:
            p.write_text(content, encoding="utf-8")
            publish_paths.append(str(p.relative_to(BASE_DIR)))
            logging.getLogger(__name__).info(f"å·²ä¿å­˜: {p}")
        except Exception as e:
            logging.getLogger(__name__).error(f"ä¿å­˜å¤±è´¥ {p}: {e}")
    outline_md = state.get("outline_final_md", "")
    if outline_md:
        outline_dir = CONTENT_ROOT / topic_slug
        ensure_dir(outline_dir)
        outline_path = outline_dir / f"{topic_slug}-learning-path.md"
        try:
            outline_path.write_text(outline_md, encoding="utf-8")
            publish_paths.append(str(outline_path.relative_to(BASE_DIR)))
            logging.getLogger(__name__).info(f"å¤§çº²å·²ä¿å­˜: {outline_path}")
        except Exception as e:
            logging.getLogger(__name__).error(f"ä¿å­˜å¤§çº²å¤±è´¥ {outline_path}: {e}")
    return {**state, "publish_paths": publish_paths}


def gather_and_report_node(state: WorkState) -> WorkState:
    reviews_in = state.get("reviews", []) or []
    reviews_by_id: Dict[str, Dict[str, Any]] = {}
    for rv in reviews_in:
        rid = str(rv.get("file_id") or rv.get("id") or "").strip()
        if not rid:
            continue
        reviews_by_id[rid] = rv

    failures_unique: List[Dict[str, Any]] = []
    non_ok_ids: List[str] = []
    for rid, rv in reviews_by_id.items():
        if _severity_score(rv) >= 3:
            failures_unique.append({"id": rid, "review": rv})
        if _has_non_ok(rv):
            non_ok_ids.append(rid)

    ok_cnt = len(reviews_by_id) - len(non_ok_ids)
    topic_slug = state.get("topic_slug", "topic")

    fix_proposals = state.get("fix_proposals", []) or []
    fix_applied = set(state.get("fix_applied", []) or [])
    fix_skipped = set(state.get("fix_skipped", []) or [])
    title_by_id: Dict[str, str] = {p.id: p.title for p in (state.get("points") or [])}

    report = [
        f"# ç”Ÿæˆä¸å®¡æŸ¥æŠ¥å‘Šï¼ˆ{topic_slug}ï¼‰",
        "",
        f"- æ€»è®¡ç”Ÿæˆ: {len(reviews_by_id)}",
        f"- é€šè¿‡ï¼ˆå…¨OKï¼‰: {ok_cnt}",
        f"- éœ€ä¿®å¤ï¼ˆä»»ä¸€ç»´åº¦éOKï¼‰: {len(non_ok_ids)}",
        "",
        "## å¤±è´¥é¡¹ï¼ˆæ—§é˜ˆå€¼ï¼Œä¾›å‚è€ƒï¼‰",
    ]
    for f in failures_unique:
        rid = f.get("id")
        rv = f.get("review", {})
        report.append(f"- {rid}: {json.dumps(rv, ensure_ascii=False)}")

    if fix_proposals:
        report.append("")
        report.append("## ä¿®å¤ä¸å¤„ç†æƒ…å†µ")
        for fp in fix_proposals:
            rid = fp.get("id", "")
            title = fp.get("title", "")
            applied = fp.get("applied", False)
            iterations = fp.get("iterations", 1)
            summary = fp.get("summary", "")
            status = "å·²åº”ç”¨" if applied else ("å·²è·³è¿‡" if rid in fix_skipped else "æœªåº”ç”¨")
            auto_flag = "(è‡ªåŠ¨)" if fp.get("auto_applied") else ""
            report.append(f"- {rid} | {title} | {status}{auto_flag} | è½®æ¬¡: {iterations} | æ‘˜è¦: {summary}")

    if state.get("fix_skipped"):
        proposed_ids = {fp.get("id", "") for fp in fix_proposals}
        skipped_only = [rid for rid in state.get("fix_skipped") if rid not in proposed_ids]
        if skipped_only:
            report.append("")
            report.append("## å¾…å¤„ç†é¡¹ï¼ˆæœªåº”ç”¨ï¼‰")
            for rid in skipped_only:
                title = title_by_id.get(rid, rid)
                report.append(f"- {rid} | {title}")

    auto_stats = state.get("auto_apply_stats", {}) or {}
    if auto_stats:
        report.append("")
        report.append("## è‡ªåŠ¨åº”ç”¨ç»Ÿè®¡")
        report.append(f"- æ¨¡å¼: {auto_stats.get('mode')}")
        report.append(f"- è‡ªåŠ¨åº”ç”¨: {auto_stats.get('applied', 0)}")
        report.append(f"- è‡ªåŠ¨è·³è¿‡: {auto_stats.get('skipped', 0)}")
        reasons = auto_stats.get("reasons") or []
        if reasons:
            report.append("")
            report.append("<details><summary>è°ƒè¯•åŸå› ï¼ˆå‰10æ¡ï¼‰</summary>")
            for r in reasons[:10]:
                report.append(f"- {r}")
            report.append("</details>")

    report_md = "\n".join(report)
    out = BASE_DIR / f"pipeline_report_{topic_slug}.md"
    try:
        out.write_text(report_md, encoding="utf-8")
        logging.getLogger(__name__).info(f"æŠ¥å‘Šå·²å†™å‡º: {out}")
    except Exception:
        pass
    return {**state, "report_md": report_md}


# ----------------------------
# reconstructed_outline â†’ outline_struct æ˜ å°„
# ----------------------------

def _convert_reconstructed_to_outline_struct(subject: str, reconstructed: Dict[str, Any], topic_slug_hint: Optional[str] = None) -> Dict[str, Any]:
    topic = subject or reconstructed.get("title") or "ä¸»é¢˜"
    # ä¼˜å…ˆä½¿ç”¨ä¸Šæ¸¸äº§å‡ºçš„ slugï¼ˆreconstructed.meta.topic_slug â†’ ä¼ å…¥ hint â†’ æœ¬åœ°å›é€€ï¼‰
    topic_slug_meta = None
    try:
        meta_obj = reconstructed.get("meta") if isinstance(reconstructed, dict) else None
        if isinstance(meta_obj, dict):
            tsm = meta_obj.get("topic_slug")
            if isinstance(tsm, str) and tsm.strip():
                topic_slug_meta = tsm.strip()
    except Exception:
        topic_slug_meta = None
    topic_slug = topic_slug_meta or (topic_slug_hint or slugify(topic))
    groups_in: List[Dict[str, Any]] = reconstructed.get("groups") or []
    chapters: List[Dict[str, Any]] = []
    for ci, ch in enumerate(groups_in, start=1):
        ch_title = ch.get("title") or f"ç¬¬{ci}ç« "
        ch_id = ch.get("id") or f"outline-ch-{ci}"
        stype = (ch.get("structure_type") or "toolbox").strip().lower()
        secs = ch.get("sections") or []
        group = {
            "title": ch_title,
            "id": f"{ch_id}-gr-1",
            "structure_type": stype,
            "sections": [],
        }
        for si, sec in enumerate(secs, start=1):
            group["sections"].append({
                "id": sec.get("id") or f"{topic_slug}-sec-{ci}-1-{si}",
                "title": sec.get("title") or f"{ci}.1.{si}",
                "relation_to_previous": sec.get("relation_to_previous") or "",
                "primary_goal": sec.get("primary_goal") or sec.get("goal") or "",
                "suggested_modules": sec.get("suggested_modules") if isinstance(sec.get("suggested_modules"), list) else [],
                "suggested_contents": sec.get("suggested_contents") if isinstance(sec.get("suggested_contents"), list) else [],
            })
        chapters.append({"title": ch_title, "id": ch_id, "groups": [group]})
    return {"meta": {"topic": topic, "topic_slug": topic_slug}, "chapters": chapters}


def _points_from_struct(data: Dict[str, Any]) -> List[Point]:
    pts: List[Point] = []
    for ch in (data.get("chapters") or []):
        ch_title = ch.get("title", "")
        for gr in (ch.get("groups") or []):
            gr_title = gr.get("title", "")
            for sec in (gr.get("sections") or []):
                pid = sec.get("id") or ""
                ptitle = sec.get("title") or ""
                if pid and ptitle:
                    pts.append(Point(id=str(pid), title=str(ptitle), chapter=str(ch_title), section=str(gr_title)))
    return pts


def _parse_selected(spec: Optional[str], total: int) -> List[int]:
    if not spec:
        return list(range(1, total + 1))
    out: List[int] = []
    for part in spec.split(','):
        p = (part or '').strip()
        if not p:
            continue
        if '-' in p:
            a, b = p.split('-', 1)
            if a.strip().isdigit() and b.strip().isdigit():
                lo, hi = int(a), int(b)
                if lo <= hi:
                    out.extend(list(range(lo, hi + 1)))
        elif p.isdigit():
            out.append(int(p))
    uniq = sorted({i for i in out if 1 <= i <= total})
    return uniq or list(range(1, total + 1))


# ----------------------------
# ä¸»æµç¨‹
# ----------------------------

def main() -> int:
    ap = argparse.ArgumentParser(description="Generate chapters (standalone nodes) from integrated reconstructed_outline")
    ap.add_argument("--input", required=True, help="åŒ…å« reconstructed_outline çš„ JSON æ–‡ä»¶è·¯å¾„")
    ap.add_argument("--config", default=str(BASE_DIR / "config.json"), help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ config.jsonï¼‰")
    ap.add_argument("--selected-chapters", default=None, help="é€‰æ‹©çš„ç« èŠ‚ç¼–å·ï¼ˆå¦‚ 1,3-4ï¼›ç•™ç©ºä¸ºå…¨éƒ¨ï¼‰")
    ap.add_argument("--max-parallel", type=int, default=None, help="å¹¶è¡Œä¸Šé™ï¼ˆè¦†ç›–é…ç½®ï¼‰")
    ap.add_argument("--skip-content-review", action="store_true", help="è·³è¿‡çŸ¥è¯†ç‚¹å®¡æŸ¥ï¼ˆæ›´å¿«ï¼‰")
    ap.add_argument("--skip-fixes", action="store_true", help="è·³è¿‡ä¿®å¤ææ¡ˆä¸è‡ªåŠ¨åº”ç”¨")
    ap.add_argument("--auto-apply-mode", type=str, choices=["off", "safe", "aggressive", "all"], default=None, help="è‡ªåŠ¨åº”ç”¨æ¨¡å¼")
    ap.add_argument("--auto-apply-threshold-major", type=float, default=None, help="aggressive æ¨¡å¼ä¸‹å¯¹ major çš„æœ€ä½ä¿¡å¿ƒé˜ˆå€¼ï¼ˆé»˜è®¤ 0.8ï¼‰")
    ap.add_argument("--no-sanitize-mermaid", action="store_true", help="ç¦ç”¨ Mermaid è¯­æ³•è§„èŒƒåŒ–ä¿®å¤ï¼ˆé»˜è®¤å¼€å¯ï¼‰")
    ap.add_argument("--output-subdir", type=str, default=None, help="è¾“å‡ºå­ç›®å½•ï¼ˆé»˜è®¤ topic slugï¼‰")
    ap.add_argument("--debug", action="store_true", help="å¼€å¯è°ƒè¯•æ—¥å¿—ï¼Œè¾“å‡ºæ¨¡å‹ä¸æ‰€æœ‰ LLM Promptï¼Œå¹¶å†™å…¥ output/<subdir>/log.txt")
    ap.add_argument("--subject-type", type=str, choices=["tool", "theory"], default=None, help="ä¸»é¢˜ç±»å‹ï¼ˆè¦†ç›–è‡ªåŠ¨åˆ†ç±»ï¼‰")
    ap.add_argument("--classify-llm-key", type=str, default=None, help="åˆ†ç±»ç”¨ LLM é”®åï¼ˆè¦†ç›– node_llm/classify_subject/defaultï¼‰")
    args = ap.parse_args()

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[logging.StreamHandler()],
    )
    logger = logging.getLogger(__name__)

    # è¯»å–è¾“å…¥
    p = Path(args.input)
    if not p.exists():
        logger.error(f"æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶: {p}")
        return 1
    try:
        data = json.loads(p.read_text(encoding="utf-8"))
    except Exception as e:
        logger.error(f"è§£æè¾“å…¥ JSON å¤±è´¥: {e}")
        return 1

    reconstructed = data.get("reconstructed_outline") or {}
    if not reconstructed:
        logger.error("è¾“å…¥ JSON ç¼ºå°‘ reconstructed_outline å­—æ®µ")
        return 1
    subject = data.get("subject") or reconstructed.get("title") or "ä¸»é¢˜"

    # åŠ è½½é…ç½®
    try:
        cfg = load_config(args.config)
    except Exception as e:
        logger.error(str(e))
        return 1
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        cfg["debug"] = True
    if args.max_parallel:
        cfg["max_parallel_requests"] = int(args.max_parallel)
    if args.skip_content_review:
        cfg["skip_content_review"] = True
    if args.skip_fixes:
        cfg["skip_fixes"] = True
    if args.auto_apply_mode is not None:
        cfg["auto_apply_mode"] = args.auto_apply_mode
    if args.auto_apply_threshold_major is not None:
        cfg["auto_apply_threshold_major"] = float(args.auto_apply_threshold_major)
    if args.no_sanitize_mermaid:
        cfg["sanitize_mermaid"] = False

    # æ„é€  outline_struct ä¸ points
    # ç»Ÿä¸€ç¡®å®š topic_slugï¼šä¼˜å…ˆ reconstructed.meta.topic_slug â†’ é¡¶å±‚ subject_slug â†’ å›é€€ slugify(subject)
    topic_slug_hint = None
    try:
        meta_top = reconstructed.get("meta") if isinstance(reconstructed, dict) else None
        if isinstance(meta_top, dict):
            tsm = meta_top.get("topic_slug")
            if isinstance(tsm, str) and tsm.strip():
                topic_slug_hint = tsm.strip()
        if not topic_slug_hint:
            tsm2 = data.get("subject_slug") if isinstance(data, dict) else None
            if isinstance(tsm2, str) and tsm2.strip():
                topic_slug_hint = tsm2.strip()
    except Exception:
        topic_slug_hint = None

    outline_struct = _convert_reconstructed_to_outline_struct(subject, reconstructed, topic_slug_hint)
    points = _points_from_struct(outline_struct)

    topic_slug = outline_struct.get("meta", {}).get("topic_slug") or slugify(subject)
    output_subdir = args.output_subdir or topic_slug

    # Debug æ–‡ä»¶è¾“å‡º
    if args.debug:
        try:
            dbg_dir = BASE_DIR / "output" / output_subdir
            ensure_dir(dbg_dir)
            dbg_path = dbg_dir / "log.txt"
            fh = logging.FileHandler(dbg_path, encoding="utf-8")
            fh.setLevel(logging.DEBUG)
            fh.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
            logging.getLogger().addHandler(fh)
            logger.debug(f"è°ƒè¯•æ—¥å¿—æ–‡ä»¶: {dbg_path}")
        except Exception as e:
            logger.warning(f"åˆ›å»ºè°ƒè¯•æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")

    # è®°å½•æ¨¡å‹è·¯ç”±
    def _resolve_llm_key_for_node(cfg: Dict[str, Any], node_key: str, subrole: Optional[str]) -> Optional[str]:
        mapping = cfg.get("node_llm", {}) or {}
        def _get(nk: str, sr: Optional[str]) -> Optional[str]:
            if sr:
                return mapping.get(f"{nk}.{sr}") or mapping.get(nk)
            return mapping.get(nk)
        name = _get(node_key, subrole)
        if not name and node_key == "generate_and_review_by_chapter":
            name = _get("generate_and_review_parallel", subrole)
        if not name:
            name = mapping.get("default")
        return name
    if args.debug:
        llms = cfg.get("llms", {}) or {}
        def _fmt_llm(k: str) -> str:
            ent = llms.get(k) if isinstance(llms, dict) else None
            if isinstance(ent, dict):
                prov = ent.get("provider") or ent.get("api_provider") or cfg.get("api_provider")
                model = ent.get("model") or cfg.get("model")
                return f"{k} (provider={prov}, model={model})"
            return f"{k} (provider={cfg.get('api_provider')}, model={cfg.get('model')})"
        logger.debug(f"ä½¿ç”¨ç”Ÿæˆæ¨¡å‹: {_fmt_llm(_resolve_llm_key_for_node(cfg, 'generate_and_review_by_chapter', 'generate') or '<registry.default>')}")
        logger.debug(f"ä½¿ç”¨å®¡æŸ¥æ¨¡å‹: {_fmt_llm(_resolve_llm_key_for_node(cfg, 'generate_and_review_by_chapter', 'review') or '<registry.default>')}")
        logger.debug(f"ä½¿ç”¨ä¿®å¤æ¨¡å‹: {_fmt_llm(_resolve_llm_key_for_node(cfg, 'propose_and_apply_fixes', 'propose') or '<registry.default>')}")

    # æ¸²æŸ“å¤§çº² Markdownï¼ˆä¾›ä¿®å¤æç¤ºä½¿ç”¨ï¼‰
    try:
        # å¤ç”¨ generator è„šæœ¬çš„æ¸²æŸ“é€»è¾‘ä¼šå¼•å…¥æ›´å¤šä¾èµ–ï¼›æ•…è¿™é‡Œç›´æ¥ç”Ÿæˆç®€è¦å¤§çº² Markdown
        lines: List[str] = []
        lines.append(f"# {outline_struct.get('meta', {}).get('topic', subject)} (id: {topic_slug})")
        chs = outline_struct.get("chapters") or []
        for ci, ch in enumerate(chs, start=1):
            lines.append("")
            lines.append(f"## ç¬¬{ci}ç« ï¼š{ch.get('title','')} (id: {ch.get('id')})")
            for gi, gr in enumerate(ch.get("groups") or [], start=1):
                lines.append(f"### {gr.get('title','')} (id: {gr.get('id')})")
                for si, sec in enumerate((gr.get("sections") or []), start=1):
                    lines.append(f"#### {sec.get('title','')} (id: {sec.get('id')})")
        outline_md = "\n".join(lines) + "\n"
    except Exception:
        outline_md = ""

    # ç« èŠ‚é€‰æ‹©
    chapters = outline_struct.get("chapters") or []
    total = len(chapters)
    if total == 0:
        logger.error("æœªæ‰¾åˆ°ä»»ä½•ç« èŠ‚")
        return 1
    indices = _parse_selected(args.selected_chapters, total)
    selected_titles = [chapters[i-1].get("title", "") for i in indices]
    logger.info(f"é€‰æ‹©ç« èŠ‚: {indices} -> {[t or 'æœªå‘½å' for t in selected_titles]}")

    # åˆ†ç±»ï¼ˆä¼˜å…ˆ JSON -> CLI -> AI åˆ†ç±»ï¼‰
    subject_type = (args.subject_type or "").strip().lower()
    if not subject_type:
        # 1) ä¼˜å…ˆä»è¾“å…¥ JSON ä¸­è¯»å–ï¼ˆreconstructed_outline.meta.subject_type æˆ– é¡¶å±‚ meta.subject_typeï¼‰
        subject_type_json = None
        try:
            if isinstance(reconstructed, dict):
                meta_rec = reconstructed.get("meta")
                if isinstance(meta_rec, dict):
                    st = meta_rec.get("subject_type")
                    if isinstance(st, str):
                        subject_type_json = st.strip().lower()
            meta_top = data.get("meta") if isinstance(data, dict) else None
            if not subject_type_json and isinstance(meta_top, dict):
                st = meta_top.get("subject_type")
                if isinstance(st, str):
                    subject_type_json = st.strip().lower()
        except Exception:
            subject_type_json = None

        if subject_type_json in {"tool", "theory"}:
            subject_type = subject_type_json
            logger.info(f"ä¸»é¢˜åˆ†ç±»ï¼ˆæ¥è‡ªè¾“å…¥ JSONï¼‰ï¼š{subject} => {subject_type}")

    if not subject_type:
        # 2) AI åˆ†ç±»
        try:
            registry = build_llm_registry(cfg)
            llm_for_cls = None
            if args.classify_llm_key and args.classify_llm_key in registry:
                llm_for_cls = registry[args.classify_llm_key]
            else:
                llm_for_cls = select_llm_for_node(cfg, registry, "classify_subject")
            subject_type = asyncio.run(_classify_subject_async(llm_for_cls, subject))  # type: ignore
            if subject_type not in {"tool", "theory"}:
                subject_type = "theory"
            logger.info(f"ä¸»é¢˜åˆ†ç±»ï¼š{subject} => {subject_type}")
        except Exception as e:
            logger.warning(f"ä¸»é¢˜åˆ†ç±»å¤±è´¥ï¼Œé»˜è®¤æŒ‰ theory å¤„ç†ï¼š{e}")
            subject_type = "theory"

    # åˆå§‹çŠ¶æ€
    state: WorkState = {
        "topic": subject,
        "topic_meta": {"lang": "zh"},
        "topic_slug": topic_slug,
        "config": cfg,
        "output_subdir": output_subdir,
        "outline_struct": outline_struct,
        "outline_final_md": outline_md,
        "points": points,
        "selected_chapters": selected_titles,
        "subject_type": subject_type,
    }

    # æ„å»º LLM å®ä¾‹ï¼ˆç”Ÿæˆã€å®¡æŸ¥ã€ä¿®å¤ï¼‰
    default_llm = init_llm(cfg)
    registry = build_llm_registry(cfg)
    def _pick(node: str, subrole: Optional[str] = None):
        return select_llm_for_node(cfg, registry, node, subrole) or default_llm

    gen_llm = _pick("generate_and_review_by_chapter", "generate")
    rev_llm = _pick("generate_and_review_by_chapter", "review")
    prop_llm = _pick("propose_and_apply_fixes", "propose")

    # è¿è¡Œ
    try:
        if subject_type == "tool":
            s1 = asyncio.run(generate_and_review_by_chapter_node_tool(state, gen_llm, rev_llm))
        else:
            s1 = asyncio.run(generate_and_review_by_chapter_node(state, gen_llm, rev_llm))
    except Exception as e:
        logger.error(f"ç”Ÿæˆ/å®¡æŸ¥é˜¶æ®µå¤±è´¥: {e}")
        return 1

    try:
        s2 = asyncio.run(propose_and_apply_fixes_node(s1, prop_llm)) if not cfg.get("skip_fixes", False) else s1
    except Exception as e:
        logger.error(f"ä¿®å¤ææ¡ˆé˜¶æ®µå¤±è´¥: {e}")
        return 1

    s3 = save_and_publish_node(s2)
    s4 = gather_and_report_node(s3)

    report_md = s4.get("report_md", "")
    if report_md:
        print("\nâœ… å®Œæˆã€‚æŠ¥å‘Šå¦‚ä¸‹ï¼š\n")
        print(report_md)
    else:
        print("\nâœ… å®Œæˆã€‚æ— æŠ¥å‘Šå¯æ˜¾ç¤ºã€‚\n")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
