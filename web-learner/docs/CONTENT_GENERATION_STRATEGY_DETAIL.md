# 内容生成策略核心设计文档 V2

本文档旨在沉淀和明确内容生成环节的核心执行逻辑与Prompt设计范式，作为后续开发与重构的统一参考。

核心是根据大纲中为每个小节（Group）预定义的`structure_type`元数据，执行不同的生成策略。

---

## 1. 生成执行顺序

### 1.1 流水线 (Pipeline) 模式：严格串行

当一个小节的 `structure_type` 被标记为 `pipeline` 时，系统会采用严格的串行生成模式。

- **执行逻辑**:
  1.  严格按照知识点（Section）在`outline.json`中的物理顺序，一个接一个地生成。
  2.  在处理第 N 个知识点时，必须等待第 N-1 个知识点的内容完全生成并审查完毕。
  3.  整个过程是线性的，不存在任何并行操作。

- **上下文处理**:
  - 系统会维护一个`cumulative_context`（累积上下文）变量。
  - 每当一个知识点生成后，其完整内容会被追加到这个变量中。
  - 在为下一个知识点构建Prompt时，这个累积的、包含前面所有内容的上下文将被注入，以实现“无缝续写”。

### 1.2 工具箱 (Toolbox) 模式：分阶段并行

当一个小节的 `structure_type` 被标记为 `toolbox` 时，系统会采用一种更智能的、带依赖关系的分阶段并行模式。

- **执行逻辑**:
  1.  **分析依赖**: 首先，系统会扫描该`toolbox`小节内所有知识点，并根据`relation_to_previous`标签构建一个局部的依赖关系图。
  2.  **第一阶段：并行生成“根”节点**:
      - 识别出所有不依赖于其他知识点的“根”节点（通常`relation_to_previous`为`first_in_sequence`或`tool_in_toolbox`）。
      - **并行**地为所有这些根节点生成内容。
  3.  **同步点**: 系统会等待第一阶段的所有任务全部完成。
  4.  **第二阶段：并行生成“派生”节点**:
      - 在第一阶段完成后，系统会识别出所有依赖于根节点的“派生”节点（如`deep_dive_into`或`builds_on`）。
      - **并行**地为这些派生节点生成内容，此时会向它们的Prompt中注入其父节点的上下文。
  5.  **多层依赖**: 如果存在更深的依赖关系（例如，第三层依赖于第二层），系统会重复“同步-并行生成”的模式，直至所有节点处理完毕。

- **上下文处理**:
  - 上下文的使用是局部的、非累积的。
  - 只有“派生”节点在生成时会接收其**直接父节点**的内容作为上下文，用于解释两者间的改进或从属关系。根节点之间不存在上下文传递。

---

## 2. Prompt 工作流示例

### 2.1 场景一：Pipeline 模式的“续写式”Prompt

**背景**: 处理`1.1 文本预处理` (`pipeline`) 小节。

**第1步：生成开篇 (1.1.1)**
- **上下文**: 空
- **Prompt**:
```markdown
# NLP 课程内容生成任务

### 角色
你是一位资深的NLP教育专家和技术作家，擅长将复杂的技术概念讲解得清晰易懂、引人入胜。

### 任务
请为我们的NLP课程《自然语言处理：从理论到实践》撰写第一章第一节（1.1）的开篇部分。

### 当前知识点详情
- **标题**: 1.1.1 起点：为何机器无法直接阅读文本？
- **核心目标**: 解释原始文本中存在的歧义性、非结构化等问题，阐明机器进行自然语言理解前必须进行预处理的原因。
- **风格要求**: `tone_style: "expository"`

### 指令
请开始撰写 **1.1.1** 的内容。
```

**第2步：生成后续 (1.1.2)**
- **上下文**: 1.1.1 已生成的完整内容。
- **Prompt**:
```markdown
# NLP 课程内容生成任务

### 角色
你是一位资深的NLP教育专家和技术作家，正在续写之前的内容。

### 已完成的章节内容 (Context)
> 【此处为1.1.1的完整生成内容】
> 想象一下，你交给一位只能识别标准印刷体字母的机器人一封手写的草书信件...（省略）...它是构建所有NLP应用不可或缺的第一步。

### 任务
请**紧接上述内容**，自然地过渡并撰写本小节的下一个知识点。

### 当前知识点详情
- **标题**: 1.1.2 第一步：基础清洗 (Data Cleaning)
- **核心目标**: 介绍文本预处理流程中的第一个具体步骤——数据清洗。

### 指令
请基于提供的上下文，继续撰写 **1.1.2** 的内容。
```

### 2.2 场景二：Toolbox 模式的“独立指令式”与“混合式”Prompt

**背景**: 处理`1.2 向量化表示` (`toolbox`) 小节。

**阶段一：并行生成根节点**

**Prompt A (生成 1.2.2 Bag-of-Words)**
```markdown
# NLP 课程内容生成任务

### 角色
你是一位资深的NLP教育专家和技术作家。

### 任务
请为我们的NLP课程撰写一篇关于“词袋模型 (Bag-of-Words)”的独立教学文章。

### 当前知识点详情
- **标题**: 1.2.2 工具一 (离散表示)：词袋模型 (Bag-of-Words)
- **核心目标**: 完整地介绍词袋模型的核心思想、构建步骤、优缺点。
- **内容要求**:
  - **必须包含以下几个部分**:现在
    1.  核心思想 (忽略词序，关注词频)。
    2.  构建步骤 (用简单例子分步展示)。
    3.  优点 (简洁、高效)。
    4.  缺点 (无法保留词序、语义鸿沟)。

### 指令
请撰写一篇关于 **词袋模型 (Bag-of-Words)** 的、内容完备的教学文章。
```

**Prompt B (生成 1.2.4 Word Embedding)**
```markdown
# NLP 课程内容生成任务

### 角色
你是一位资深的NLP教育专家和技术作家。

### 任务
请为我们的NLP课程撰写一篇关于“词嵌入 (Word Embedding)”的独立教学文章。

### 当前知识点详情
- **标题**: 1.2.4 工具二 (分布式表示)：词嵌入 (Word Embedding)
- **核心目标**: 完整地介绍词嵌入的核心思想、价值以及它与离散表示的根本区别。
- **内容要求**:
  - **必须包含以下几个部分**:
    1.  核心思想 (稠密向量，捕捉语义)。
    2.  核心价值 (向量空间中的语义关系)。
    3.  与离散表示的对比。
    4.  使用经典类比 (如 king - man + woman ≈ queen)。

### 指令
请撰写一篇关于 **词嵌入 (Word Embedding)** 的、内容完备的教学文章。
```

**阶段二：生成派生节点 (等待阶段一完成)**

**Prompt C (生成 1.2.3 TF-IDF, 依赖于 1.2.2 BoW)**
```markdown
# NLP 课程内容生成任务

### 角色
你是一位资深的NLP教育专家，擅长在现有概念的基础上进行深化讲解。

### 父级知识点上下文 (Parent Context)
> 【此处为1.2.2 Bag-of-Words的完整生成内容】
> 词袋模型（Bag-of-Words, BoW）是自然语言处理中最简单、最直观的文本表示方法之一...（省略）...

### 任务
请基于以上关于“词袋模型”的介绍，撰写一篇**深入探讨 (Deep Dive)** 的文章，主题为“TF-IDF”。

### 当前知识点详情
- **标题**: 1.2.3 改进：从词频(BoW)到TF-IDF权重
- **关系**: `deep_dive_into` (深入父级知识点)
- **核心目标**: 解释TF-IDF是如何作为词袋模型的一种重要改进，它如何通过“逆文档频率”来修正简单词频统计的缺点。
- **内容要求**:
  - **必须以前文为基础**: 你的开篇应该明确指出这是对BoW中简单词频统计的一种优化。
  - **解释TF-IDF的组成**: 分别解释词频 (TF) 和逆文档频率 (IDF) 的概念。
  - **阐明其优势**: 解释为什么常见词的权重会被降低，而关键词的权重会得到提升。

### 指令
请基于提供的BoW上下文，撰写一篇关于 **TF-IDF** 的、作为其改进方案的教学文章。
```
