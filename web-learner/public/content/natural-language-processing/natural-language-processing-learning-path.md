# 自然语言处理 (id: natural-language-processing)

## 第1章：第一章：基础篇 · 让机器理解语言的基石 (id: nlp-ch-1)
### 第一章：基础篇 · 让机器理解语言的基石 (id: nlp-ch-1-gr-1)
#### 1.1 根本问题：为何机器处理文本如此困难？ (id: nlp-sec-1-1)
#### 1.2 文本预处理：从原始语料到结构化词元流 (id: nlp-sec-1-2)

## 第2章：第二章：文本表示 · 将词语转化为向量 (id: nlp-ch-2)
### 第二章：文本表示 · 将词语转化为向量 (id: nlp-ch-2-gr-1)
#### 2.1 根本问题：如何用数学语言表示词汇的含义？ (id: nlp-sec-2-1)
#### 2.2 早期思想：基于统计的稀疏表示 (TF-IDF) (id: nlp-sec-2-2)
#### 2.3 范式革命：基于预测的密集表示 (Word2Vec & GloVe) (id: nlp-sec-2-3)
#### 2.4 承上启下：静态向量的局限与上下文的呼唤 (id: nlp-sec-2-4)

## 第3章：第三章：序列建模 · 捕捉文本中的时序依赖 (id: nlp-ch-3)
### 第三章：序列建模 · 捕捉文本中的时序依赖 (id: nlp-ch-3-gr-1)
#### 3.1 根本问题：如何让模型拥有“记忆”来处理序列？ (id: nlp-sec-3-1)
#### 3.2 拆解关键机制：长短期记忆网络 (LSTM) 与门控循环单元 (GRU) (id: nlp-sec-3-2)
#### 3.3 实践指南：构建与应用序列模型 (id: nlp-sec-3-3)

## 第4章：第四章：序列到序列 · 从编码到生成的跨越 (id: nlp-ch-4)
### 第四章：序列到序列 · 从编码到生成的跨越 (id: nlp-ch-4-gr-1)
#### 4.1 根本问题：如何处理输入和输出序列长度不同的任务？ (id: nlp-sec-4-1)
#### 4.2 范式革命：注意力机制 (Attention Mechanism) (id: nlp-sec-4-2)

## 第5章：第五章：Transformer · 注意力是全部所需 (id: nlp-ch-5)
### 第五章：Transformer · 注意力是全部所需 (id: nlp-ch-5-gr-1)
#### 5.1 根本问题：如何摆脱RNN的顺序计算限制，实现大规模并行？ (id: nlp-sec-5-1)
#### 5.2 拆解关键机制 (一)：自注意力与多头注意力 (id: nlp-sec-5-2)
#### 5.3 拆解关键机制 (二)：位置编码、残差连接与层归一化 (id: nlp-sec-5-3)

## 第6章：第六章：新范式 · 预训练、提示与微调 (id: nlp-ch-6)
### 第六章：新范式 · 预训练、提示与微调 (id: nlp-ch-6-gr-1)
#### 6.1 核心思想：从零训练到“迁移学习”的范式转变 (id: nlp-sec-6-1)
#### 6.2 工具一 (编码器)：BERT及其变体 (id: nlp-sec-6-2)
#### 6.3 工具二 (解码器)：GPT系列与生成式预训练 (id: nlp-sec-6-3)
#### 6.4 工具三 (编码器-解码器)：T5/BART与序列到序列预训练 (id: nlp-sec-6-4)

## 第7章：第七章：大型语言模型 (LLM) · 涌现能力与新范式 (id: nlp-ch-7)
### 第七章：大型语言模型 (LLM) · 涌现能力与新范式 (id: nlp-ch-7-gr-1)
#### 7.1 根本问题：当模型规模达到临界点会发生什么？ (id: nlp-sec-7-1)
#### 7.2 核心交互范式：提示工程与上下文学习 (id: nlp-sec-7-2)
#### 7.3 对齐技术：从人类反馈中学习 (RLHF) (id: nlp-sec-7-3)
#### 7.4 实践与挑战：RAG、评估与伦理 (id: nlp-sec-7-4)
