# 自然语言处理 (id: natural-language-processing)

## 第1章：第一章：NLP基础：文本预处理与表示 (id: natural-language-processing-ch-1)
### 1.1 文本预处理：从原始语料到机器可读数据 (id: natural-language-processing-gr-1-1)
#### 1.1.1 什么是文本预处理及其重要性 (id: natural-language-processing-sec-1-1-1-sec-1-1-1)
#### 1.1.2 基础文本清洗：大小写转换、去除标点与数字 (id: natural-language-processing-sec-1-1-2-sec-1-1-2)
#### 1.1.3 文本切分（Tokenization）：将连续文本分解为词元 (id: natural-language-processing-sec-1-1-3-tokenization)
#### 1.1.4 停用词（Stop Words）移除与词形归一化 (id: natural-language-processing-sec-1-1-4-stop-words)
#### 1.1.5 案例分析：构建一个完整的中英文本预处理流水线 (id: natural-language-processing-sec-1-1-5-sec-1-1-5)
#### 1.1.6 不同预处理步骤对下游任务影响的对比 (id: natural-language-processing-sec-1-1-6-sec-1-1-6)
### 1.2 文本的离散表示：词袋模型与TF-IDF (id: natural-language-processing-gr-1-2)
#### 1.2.1 为何需要文本表示：从文本到向量的核心思想 (id: natural-language-processing-sec-1-2-1-sec-1-2-1)
#### 1.2.2 独热编码（One-Hot Encoding）：最直观的词表示法 (id: natural-language-processing-sec-1-2-2-one-hot-encoding)
#### 1.2.3 词袋模型（Bag-of-Words）：忽略语序的文档向量化 (id: natural-language-processing-sec-1-2-3-bag-of-words)
#### 1.2.4 N-gram模型：在词袋模型中引入局部语序信息 (id: natural-language-processing-sec-1-2-4-n-gram)
#### 1.2.5 TF-IDF：从词频到词语重要性的加权方法 (id: natural-language-processing-sec-1-2-5-tf-idf)
### 1.3 文本的分布式表示：捕捉词语的深层语义 (id: natural-language-processing-gr-1-3)
#### 1.3.1 分布式表示思想：基于上下文理解词义 (id: natural-language-processing-sec-1-3-1-sec-1-3-1)
#### 1.3.2 Word2Vec核心原理：CBOW与Skip-gram模型 (id: natural-language-processing-sec-1-3-2-word2veccbowskip-gram)
#### 1.3.3 实践：加载并使用预训练的词向量 (id: natural-language-processing-sec-1-3-3-sec-1-3-3)
#### 1.3.4 GloVe模型：融合全局共现统计的词向量学习 (id: natural-language-processing-sec-1-3-4-glove)
#### 1.3.5 从词到篇章：句子和文档向量的生成方法 (id: natural-language-processing-sec-1-3-5-sec-1-3-5)
#### 1.3.6 词向量的评估方法与可视化 (id: natural-language-processing-sec-2-1-5-sec-2-1-5)
#### 1.3.7 案例：使用预训练词向量完成文本分类任务 (id: natural-language-processing-sec-2-1-6-sec-2-1-6)

## 第2章：第二章：序列建模：从RNN到注意力机制 (id: natural-language-processing-ch-2)
### 2.1 第一节：词的向量化表示：让机器理解词义 (id: natural-language-processing-gr-2-1)
### 2.2 循环神经网络（RNN）及其挑战 (id: natural-language-processing-gr-2-2)
#### 2.2.1 为何需要序列模型？从词袋模型到序列信息的挑战 (id: natural-language-processing-sec-2-2-1-sec-2-2-1)
#### 2.2.2 循环神经网络（RNN）的核心结构与工作原理 (id: natural-language-processing-sec-2-2-2-rnn)
#### 2.2.3 RNN的前向传播与反向传播（BPTT） (id: natural-language-processing-sec-2-2-3-rnnbptt)
#### 2.2.4 RNN的应用场景：语言模型与文本生成 (id: natural-language-processing-sec-2-2-4-rnn)
#### 2.2.5 RNN的长期依赖问题：梯度消失与梯度爆炸 (id: natural-language-processing-sec-2-2-5-rnn)
### 2.3 高级序列模型：LSTM、GRU与注意力机制 (id: natural-language-processing-gr-2-3)
#### 2.3.1 长短期记忆网络（LSTM）的门控机制 (id: natural-language-processing-sec-2-3-1-lstm)
#### 2.3.2 门控循环单元（GRU）：LSTM的简化变体 (id: natural-language-processing-sec-2-3-2-grulstm)
#### 2.3.3 双向RNN（Bi-RNN）：融合上下文信息 (id: natural-language-processing-sec-2-3-3-rnnbi-rnn)
#### 2.3.4 序列到序列模型（Seq2Seq）的编解码器架构 (id: natural-language-processing-sec-2-3-4-seq2seq)
#### 2.3.5 注意力机制（Attention Mechanism）的核心思想与流程 (id: natural-language-processing-sec-2-3-5-attention-mechanism)

## 第3章：第三章：架构革命：Transformer与预训练模型 (id: natural-language-processing-ch-3)
### 3.1 注意力机制：从信息瓶颈到“关注”核心 (id: natural-language-processing-gr-3-1)
#### 3.1.1 回顾Seq2Seq模型的局限性 (id: natural-language-processing-sec-3-1-1-seq2seq)
#### 3.1.2 注意力机制的核心思想：QKV模型 (id: natural-language-processing-sec-3-1-2-qkv)
#### 3.1.3 注意力分数计算：从点积到加性注意力 (id: natural-language-processing-sec-3-1-3-sec-3-1-3)
#### 3.1.4 自注意力机制（Self-Attention）初探 (id: natural-language-processing-sec-3-1-4-self-attention)
#### 3.1.5 注意力机制的优势与计算复杂度分析 (id: natural-language-processing-sec-3-1-5-sec-3-1-5)
### 3.2 Transformer架构全解析：“Attention Is All You Need” (id: natural-language-processing-gr-3-2)
#### 3.2.1 Transformer整体架构：告别RNN的编码器-解码器 (id: natural-language-processing-sec-3-2-1-transformerrnn)
#### 3.2.2 多头注意力机制（Multi-Head Attention） (id: natural-language-processing-sec-3-2-2-multi-head-attention)
#### 3.2.3 位置编码（Positional Encoding）：为模型注入序列顺序信息 (id: natural-language-processing-sec-3-2-3-positional-encoding)
#### 3.2.4 编码器与解码器层：残差连接与层归一化 (id: natural-language-processing-sec-3-2-4-sec-3-2-4)
#### 3.2.5 Transformer解码流程详解：自回归与Masked Self-Attention (id: natural-language-processing-sec-3-2-5-transformermasked-self-attention)
#### 3.2.6 架构对比：Transformer vs. RNN/CNN在NLP任务中的优劣 (id: natural-language-processing-sec-3-2-6-transformer-vs.-rnncnnnlp)
### 3.3 Transformer的衍生与应用：预训练语言模型时代 (id: natural-language-processing-gr-3-3)
#### 3.3.1 预训练-微调：NLP发展的新范式 (id: natural-language-processing-sec-3-3-1-pre-training-and-fine-tuning)
#### 3.3.2 BERT模型：基于Transformer编码器的双向语言表示 (id: natural-language-processing-sec-3-3-2-berttransformer)
#### 3.3.3 GPT系列模型：基于Transformer解码器的自回归生成 (id: natural-language-processing-sec-3-3-3-gpttransformer)
#### 3.3.4 模型家族对比：BERT、GPT与T5的架构与应用场景差异 (id: natural-language-processing-sec-3-3-4-bertgptt5)
#### 3.3.5 案例分析：如何为下游任务选择并微调合适的预训练模型 (id: natural-language-processing-sec-3-3-5-sec-3-3-5)

## 第4章：第四章：大语言模型：能力、范式与对齐 (id: natural-language-processing-ch-4)
### 4.1 范式演进：从预训练到大模型 (id: natural-language-processing-gr-4-1)
#### 4.1.1 预训练语言模型（PLM）的核心思想 (id: natural-language-processing-sec-4-1-1-plm)
#### 4.1.2 规模效应：Scaling Law的启示 (id: natural-language-processing-sec-4-1-4-scaling-law)
#### 4.1.3 范式转变：从微调到上下文学习（In-Context Learning） (id: natural-language-processing-sec-4-1-5-in-context-learning)
#### 4.1.4 预训练目标对比：MLM vs. Causal LM (id: natural-language-processing-sec-4-1-6-mlm-vs.-causal-lm)
### 4.2 涌现与评估：大模型的核心能力 (id: natural-language-processing-gr-4-2)
#### 4.2.1 “涌现能力”（Emergent Abilities）的概念 (id: natural-language-processing-sec-4-2-1-emergent-abilities)
#### 4.2.2 大模型的基础能力：生成、理解与摘要 (id: natural-language-processing-sec-4-2-2-sec-4-2-2)
#### 4.2.3 上下文学习（In-Context Learning）的运作机制 (id: natural-language-processing-sec-4-2-3-in-context-learning)
#### 4.2.4 思维链（Chain-of-Thought）与复杂推理 (id: natural-language-processing-sec-4-2-4-chain-of-thought)
#### 4.2.5 大模型能力评估：标准化基准（MMLU, BIG-bench） (id: natural-language-processing-sec-4-2-5-mmlu-big-bench)
#### 4.2.6 评估的挑战：幻觉、偏见与安全性 (id: natural-language-processing-sec-4-2-6-sec-4-2-6)
### 4.3 对齐与交互：构建负责任的AI (id: natural-language-processing-gr-4-3)
#### 4.3.1 AI对齐（Alignment）的基本概念 (id: natural-language-processing-sec-4-3-1-aialignment)
#### 4.3.2 指令遵循与对话式AI（Conversational AI） (id: natural-language-processing-sec-4-3-2-aiconversational-ai)
#### 4.3.3 核心技术：基于人类反馈的强化学习（RLHF） (id: natural-language-processing-sec-4-3-3-rlhf)
#### 4.3.4 高级交互范式：智能体（Agent）与工具使用（Tool Use） (id: natural-language-processing-sec-4-3-5-agenttool-use)
#### 4.3.5 伦理考量：大模型时代的社会责任 (id: natural-language-processing-sec-4-3-6-sec-4-3-6)

## 第5章：第五章：LLM应用开发：生态与实战 (id: natural-language-processing-ch-5)
### 5.1 LLM应用开发基础与生态 (id: natural-language-processing-gr-5-1)
#### 5.1.1 初探LLM能力：API与SDK的直接调用 (id: natural-language-processing-sec-5-1-1-llmapisdk)
#### 5.1.2 开发框架入门：LangChain的核心组件与链（Chains） (id: natural-language-processing-sec-5-1-2-langchainchains)
#### 5.1.3 为应用注入知识：向量数据库的核心概念与集成 (id: natural-language-processing-sec-5-1-3-sec-5-1-3)
#### 5.1.4 技术栈选型思辨：开源模型 vs. 闭源API (id: natural-language-processing-sec-5-1-4-vs.-api)
### 5.2 核心开发范式：RAG与智能体 (id: natural-language-processing-gr-5-2)
#### 5.2.1 提示工程（Prompt Engineering）的核心原则与技巧 (id: natural-language-processing-sec-5-2-1-prompt-engineering)
#### 5.2.2 实战检索增强生成（RAG）：构建知识库问答系统 (id: natural-language-processing-sec-5-2-2-rag)
#### 5.2.3 智能体（Agents）入门：赋予LLM使用工具的能力 (id: natural-language-processing-sec-5-2-3-agentsllm)
#### 5.2.4 高级RAG优化策略：提升检索与生成质量 (id: natural-language-processing-sec-5-2-4-rag)
#### 5.2.5 开发范式对比：Fine-tuning、RAG与Agents的选择之道 (id: natural-language-processing-sec-5-2-5-fine-tuningragagents)
### 5.3 应用生产化：健壮性、安全与部署 (id: natural-language-processing-gr-5-3)
#### 5.3.1 提升用户体验：流式输出（Streaming）与响应缓存 (id: natural-language-processing-sec-5-3-2-streaming)
#### 5.3.2 保障应用安全：Prompt注入的风险与防御 (id: natural-language-processing-sec-5-3-3-prompt)
#### 5.3.3 迈向生产：LLMOps与应用可观测性 (id: natural-language-processing-sec-5-3-4-llmops)
#### 5.3.4 综合案例分析：从零到一构建并部署一个垂直领域知识库问答系统 (id: natural-language-processing-sec-5-3-5-sec-5-3-5)

## 第6章：第六章：模型边界：评估、伦理与前沿 (id: natural-language-processing-ch-6)
### 6.1 模型的量尺：NLP 效果评估体系 (id: natural-language-processing-gr-6-1)
#### 6.1.1 什么是模型评估？ (id: natural-language-processing-sec-6-1-1-sec-6-1-1)
#### 6.1.2 分类与生成任务的核心指标 (id: natural-language-processing-sec-6-1-2-sec-6-1-2)
#### 6.1.3 困惑度（Perplexity）：语言模型性能的度量 (id: natural-language-processing-sec-6-1-3-perplexity)
#### 6.1.4 超越自动评估：人工评估的方法与重要性 (id: natural-language-processing-sec-6-1-4-sec-6-1-4)
#### 6.1.5 基于模型的评估：以LLM-as-a-Judge为例 (id: natural-language-processing-sec-new-llm-as-a-judge-7)
#### 6.1.6 综合基准（Benchmark）的实践：以GLUE/SuperGLUE为例 (id: natural-language-processing-sec-6-1-5-benchmarkgluesuperglue)
#### 6.1.7 对抗性评估：测试模型的鲁棒性与安全性 (id: natural-language-processing-sec-6-1-6-sec-6-1-6)
### 6.2 技术的双刃剑：NLP 伦理挑战与对策 (id: natural-language-processing-gr-6-2)
#### 6.2.1 数据偏见：模型不公平的根源 (id: natural-language-processing-sec-6-2-1-sec-6-2-1)
#### 6.2.2 模型幻觉（Hallucination）与事实性 (id: natural-language-processing-sec-6-2-2-hallucination)
#### 6.2.3 隐私泄露风险与数据安全 (id: natural-language-processing-sec-6-2-3-sec-6-2-3)
#### 6.2.4 滥用与恶意生成：虚假信息与深度伪造 (id: natural-language-processing-sec-6-2-4-sec-6-2-4)
#### 6.2.5 算法公平性审计与偏见缓解技术 (id: natural-language-processing-sec-6-2-5-sec-6-2-5)
#### 6.2.6 负责任AI（Responsible AI）框架与治理 (id: natural-language-processing-sec-6-2-6-airesponsible-ai)
### 6.3 眺望未来：NLP 的前沿趋势与开放问题 (id: natural-language-processing-gr-6-3)
#### 6.3.1 多模态学习：融合文本、图像与声音 (id: natural-language-processing-sec-6-3-1-sec-6-3-1)
#### 6.3.2 大模型的高效化：蒸馏、量化与剪枝 (id: natural-language-processing-sec-6-3-2-sec-6-3-2)
#### 6.3.3 AI Agent：从语言理解到自主行动 (id: natural-language-processing-sec-6-3-3-ai-agent)
#### 6.3.4 具身智能（Embodied AI）：语言与物理世界的交互 (id: natural-language-processing-sec-6-3-4-embodied-ai)
#### 6.3.5 可解释性（XAI）与因果推理 (id: natural-language-processing-sec-6-3-5-xai)
#### 6.3.6 走向通用人工智能（AGI）的路径与挑战 (id: natural-language-processing-sec-6-3-6-agi)
