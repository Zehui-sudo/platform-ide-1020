```markdown
# å®ç°ä»»åŠ¡åˆ†å‘ä¸ç»“æœæ±‡æ€»

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ
ä»»åŠ¡åˆ†å‘ä¸ç»“æœæ±‡æ€»æ˜¯å¤šæ™ºèƒ½ä½“åä½œçš„æ ¸å¿ƒæœºåˆ¶ï¼Œå®ƒè§£å†³äº†å¦‚ä½•å°†å¤æ‚ä»»åŠ¡æ‹†è§£åˆ†é…ç»™ä¸åŒä¸“ä¸šAgentï¼Œå¹¶æœ‰æ•ˆæ•´åˆå„Agentå·¥ä½œæˆæœçš„é—®é¢˜ï¼Œæ˜¯æ„å»ºå¯æ‰©å±•Agentç³»ç»Ÿçš„å…³é”®ã€‚

## ğŸ’¡ ä½¿ç”¨æ–¹å¼
åœ¨LangGraphä¸­ï¼Œé€šè¿‡`StateGraph`çš„çŠ¶æ€å…±äº«æœºåˆ¶å’Œæ¡ä»¶è¾¹å®ç°ä»»åŠ¡åˆ†å‘ï¼Œä½¿ç”¨èšåˆèŠ‚ç‚¹è¿›è¡Œç»“æœæ±‡æ€»ã€‚æ ¸å¿ƒAPIåŒ…æ‹¬ï¼š
- `add_node()` æ·»åŠ å·¥ä½œèŠ‚ç‚¹
- `add_conditional_edges()` å®ç°åŠ¨æ€è·¯ç”±
- å…±äº«Stateå®ç°æ•°æ®ä¼ é€’

## ğŸ“š Level 1: åŸºç¡€è®¤çŸ¥ï¼ˆ30ç§’ç†è§£ï¼‰
æœ€ç®€å•çš„ä»»åŠ¡åˆ†å‘ä¸æ±‡æ€»å®ç°ï¼ŒåŒ…å«ä¸¤ä¸ªå·¥ä½œAgentå’Œä¸€ä¸ªæ±‡æ€»èŠ‚ç‚¹ï¼š

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, List, Annotated
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
import operator

# å®šä¹‰çŠ¶æ€
class ResearchState(TypedDict):
    tasks: List[str]
    results: Annotated[List[str], operator.add]
    final_report: str

# åˆå§‹åŒ–LLM
llm = ChatOpenAI(model="gpt-3.5-turbo")

# å®šä¹‰å·¥ä½œèŠ‚ç‚¹
def research_agent(state: ResearchState):
    task = state["tasks"].pop(0)
    response = llm.invoke([
        HumanMessage(content=f"è¯·ç ”ç©¶ä»¥ä¸‹ä»»åŠ¡ï¼š{task}ï¼Œæä¾›è¯¦ç»†åˆ†æ")
    ])
    return {"results": [response.content]}

def writer_agent(state: ResearchState):
    task = state["tasks"].pop(0)
    response = llm.invoke([
        HumanMessage(content=f"è¯·æ’°å†™å…³äº {task} çš„è¯¦ç»†æŠ¥å‘Š")
    ])
    return {"results": [response.content]}

# å®šä¹‰æ±‡æ€»èŠ‚ç‚¹
def summarizer_agent(state: ResearchState):
    all_results = "\n".join(state["results"])
    response = llm.invoke([
        HumanMessage(content=f"åŸºäºä»¥ä¸‹ç ”ç©¶ç»“æœç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼š\n{all_results}")
    ])
    return {"final_report": response.content}

# æ„å»ºå›¾
builder = StateGraph(ResearchState)

# æ·»åŠ èŠ‚ç‚¹
builder.add_node("researcher", research_agent)
builder.add_node("writer", writer_agent)
builder.add_node("summarizer", summarizer_agent)

# è®¾ç½®å…¥å£ç‚¹
builder.set_entry_point("researcher")

# æ·»åŠ è¾¹
builder.add_edge("researcher", "writer")
builder.add_edge("writer", "summarizer")
builder.add_edge("summarizer", END)

# ç¼–è¯‘å›¾
graph = builder.compile()

# è¿è¡Œ
result = graph.invoke({
    "tasks": ["äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿", "æœºå™¨å­¦ä¹ æœ€æ–°è¿›å±•"],
    "results": [],
    "final_report": ""
})

print("æœ€ç»ˆæŠ¥å‘Š:", result["final_report"])
```

## ğŸ“ˆ Level 2: æ ¸å¿ƒç‰¹æ€§ï¼ˆæ·±å…¥ç†è§£ï¼‰

### ç‰¹æ€§1: åŠ¨æ€ä»»åŠ¡åˆ†å‘
æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨è·¯ç”±åˆ°ä¸åŒçš„ä¸“ä¸šAgentï¼š

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, List, Annotated, Literal
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
import operator

class ResearchState(TypedDict):
    tasks: List[str]
    results: Annotated[List[str], operator.add]
    final_report: str
    current_task: str

llm = ChatOpenAI(model="gpt-3.5-turbo")

def task_router(state: ResearchState) -> Literal["tech_researcher", "business_analyst", "summarizer"]:
    task = state["tasks"][0] if state["tasks"] else ""
    
    if "æŠ€æœ¯" in task or "AI" in task or "ç®—æ³•" in task:
        return "tech_researcher"
    elif "å¸‚åœº" in task or "å•†ä¸š" in task or "ç»æµ" in task:
        return "business_analyst"
    else:
        return "summarizer"

def tech_researcher(state: ResearchState):
    task = state["tasks"].pop(0)
    response = llm.invoke([
        HumanMessage(content=f"ä½œä¸ºæŠ€æœ¯ç ”ç©¶å‘˜ï¼Œè¯·åˆ†æï¼š{task}")
    ])
    return {"results": [f"æŠ€æœ¯åˆ†æ: {response.content}"]}

def business_analyst(state: ResearchState):
    task = state["tasks"].pop(0)
    response = llm.invoke([
        HumanMessage(content=f"ä½œä¸ºå•†ä¸šåˆ†æå¸ˆï¼Œè¯·åˆ†æï¼š{task}")
    ])
    return {"results": [f"å•†ä¸šåˆ†æ: {response.content}"]}

def summarizer_agent(state: ResearchState):
    if not state["tasks"]:
        all_results = "\n".join(state["results"])
        response = llm.invoke([
            HumanMessage(content=f"æ•´åˆæ‰€æœ‰åˆ†æç»“æœï¼š\n{all_results}")
        ])
        return {"final_report": response.content}
    return {}

builder = StateGraph(ResearchState)

builder.add_node("router", lambda state: state)  # è·¯ç”±èŠ‚ç‚¹
builder.add_node("tech_researcher", tech_researcher)
builder.add_node("business_analyst", business_analyst)
builder.add_node("summarizer", summarizer_agent)

builder.set_entry_point("router")

# æ¡ä»¶è¾¹å®ç°åŠ¨æ€è·¯ç”±
builder.add_conditional_edges(
    "router",
    task_router,
    {
        "tech_researcher": "tech_researcher",
        "business_analyst": "business_analyst",
        "summarizer": "summarizer"
    }
)

builder.add_edge("tech_researcher", "router")
builder.add_edge("business_analyst", "router")
builder.add_conditional_edges("summarizer", lambda state: END if state["final_report"] else "router")

graph = builder.compile()

# æ‰§è¡Œ
result = graph.invoke({
    "tasks": ["AIæŠ€æœ¯å‘å±•è¶‹åŠ¿", "æœºå™¨å­¦ä¹ å¸‚åœºåˆ†æ", "äººå·¥æ™ºèƒ½ç»æµå½±å“"],
    "results": [],
    "final_report": "",
    "current_task": ""
})

print("æ•´åˆæŠ¥å‘Š:", result["final_report"])
```

## ğŸ” Level 3: å¯¹æ¯”å­¦ä¹ ï¼ˆé¿å…é™·é˜±ï¼‰

### é”™è¯¯ç”¨æ³• vs æ­£ç¡®ç”¨æ³•

```python
# âŒ é”™è¯¯ç”¨æ³•ï¼šçŠ¶æ€ç®¡ç†æ··ä¹±
def bad_researcher(state: ResearchState):
    # ç›´æ¥ä¿®æ”¹åŸå§‹ä»»åŠ¡åˆ—è¡¨ï¼Œå¯èƒ½å¯¼è‡´çŠ¶æ€ä¸ä¸€è‡´
    task = state["tasks"].pop(0)
    # æ²¡æœ‰è¿”å›å®Œæ•´çš„çŠ¶æ€æ›´æ–°
    return {"result": f"åˆ†æ: {task}"}

# âœ… æ­£ç¡®ç”¨æ³•ï¼šçº¯å‡€çš„çŠ¶æ€å¤„ç†
def good_researcher(state: ResearchState):
    # åˆ›å»ºä»»åŠ¡å‰¯æœ¬è¿›è¡Œå¤„ç†
    remaining_tasks = state["tasks"][1:]
    current_task = state["tasks"][0]
    
    response = llm.invoke([
        HumanMessage(content=f"åˆ†æä»»åŠ¡: {current_task}")
    ])
    
    # è¿”å›å®Œæ•´çš„çŠ¶æ€æ›´æ–°
    return {
        "tasks": remaining_tasks,
        "results": [response.content]
    }
```

## ğŸš€ Level 4: å®æˆ˜åº”ç”¨ï¼ˆçœŸå®åœºæ™¯ï¼‰
æ„å»ºå®Œæ•´çš„ç ”ç©¶å›¢é˜Ÿï¼ŒåŒ…å«ä»»åŠ¡æ‹†åˆ†ã€ä¸“ä¸šåˆ†é…ã€è´¨é‡æ£€æŸ¥å’Œæœ€ç»ˆæ±‡æ€»ï¼š

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, List, Annotated, Literal
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
import operator

class ResearchState(TypedDict):
    main_task: str
    subtasks: List[str]
    research_results: Annotated[List[str], operator.add]
    quality_checked: bool
    final_report: str

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

def task_decomposer(state: ResearchState):
    """å°†ä¸»ä»»åŠ¡æ‹†åˆ†ä¸ºå­ä»»åŠ¡"""
    response = llm.invoke([
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡åˆ†è§£ä¸“å®¶ï¼Œè¯·å°†å¤æ‚ä»»åŠ¡æ‹†åˆ†ä¸º3-5ä¸ªå­ä»»åŠ¡"),
        HumanMessage(content=f"è¯·å°†ä»¥ä¸‹ä»»åŠ¡æ‹†åˆ†ä¸ºå­ä»»åŠ¡ï¼š{state['main_task']}")
    ])
    return {"subtasks": response.content.split("\n")}

def research_agent(state: ResearchState):
    """ç ”ç©¶Agentå¤„ç†å­ä»»åŠ¡"""
    if not state["subtasks"]:
        return {"subtasks": []}
    
    task = state["subtasks"].pop(0)
    response = llm.invoke([
        SystemMessage(content="ä½ æ˜¯ä¸“ä¸šç ”ç©¶å‘˜ï¼Œæä¾›è¯¦ç»†çš„æŠ€æœ¯åˆ†æ"),
        HumanMessage(content=f"è¯·æ·±å…¥ç ”ç©¶ï¼š{task}")
    ])
    return {
        "subtasks": state["subtasks"],
        "research_results": [f"## {task}\n{response.content}"]
    }

def quality_checker(state: ResearchState):
    """è´¨é‡æ£€æŸ¥èŠ‚ç‚¹"""
    if not state["research_results"]:
        return {"quality_checked": False}
    
    latest_result = state["research_results"][-1]
    response = llm.invoke([
        SystemMessage(content="ä½ æ˜¯è´¨é‡æ£€æŸ¥ä¸“å®¶ï¼Œè¯„ä¼°ç ”ç©¶å†…å®¹çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§"),
        HumanMessage(content=f"è¯·æ£€æŸ¥ä»¥ä¸‹ç ”ç©¶è´¨é‡ï¼š\n{latest_result}")
    ])
    return {"quality_checked": "é€šè¿‡" in response.content}

def report_generator(state: ResearchState):
    """æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ"""
    all_results = "\n\n".join(state["research_results"])
    response = llm.invoke([
        SystemMessage(content="ä½ æ˜¯é«˜çº§æŠ¥å‘Šæ’°å†™ä¸“å®¶ï¼Œç”Ÿæˆç»“æ„åŒ–çš„ä¸“ä¸šæŠ¥å‘Š"),
        HumanMessage(content=f"åŸºäºä»¥ä¸‹ç ”ç©¶ç»“æœç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼š\n{all_results}")
    ])
    return {"final_report": response.content}

def router(state: ResearchState) -> Literal["research", "quality_check", "generate_report", "end"]:
    """æ™ºèƒ½è·¯ç”±"""
    if not state.get("subtasks"):
        return "research"
    elif state["subtasks"] and not state.get("quality_checked", False):
        return "quality_check"
    elif not state["subtasks"] and state.get("quality_checked", False):
        return "generate_report"
    else:
        return "end"

# æ„å»ºå›¾
builder = StateGraph(ResearchState)

builder.add_node("decomposer", task_decomposer)
builder.add_node("researcher", research_agent)
builder.add_node("quality_checker", quality_checker)
builder.add_node("report_generator", report_generator)

builder.set_entry_point("decomposer")

builder.add_conditional_edges(
    "decomposer",
    lambda state: "researcher",
    {"researcher": "researcher"}
)

builder.add_conditional_edges(
    "researcher",
    router,
    {
        "research": "researcher",
        "quality_check": "quality_checker",
        "generate_report": "report_generator",
        "end": END
    }
)

builder.add_conditional_edges(
    "quality_checker",
    lambda state: "researcher" if not state["quality_checked"] else "researcher",
    {"researcher": "researcher"}
)

builder.add_edge("report_generator", END)

graph = builder.compile()

# æ‰§è¡Œå®Œæ•´æµç¨‹
result = graph.invoke({
    "main_task": "äººå·¥æ™ºèƒ½å¯¹æœªæ¥å°±ä¸šå¸‚åœºçš„å½±å“",
    "subtasks": [],
    "research_results": [],
    "quality_checked": False,
    "final_report": ""
})

print("=" * 50)
print("æœ€ç»ˆç ”ç©¶æŠ¥å‘Š:")
print("=" * 50)
print(result["final_report"])
```

## ğŸ’¡ è®°å¿†è¦ç‚¹
- çŠ¶æ€è®¾è®¡æ˜¯å…³é”®ï¼šåˆç†è®¾è®¡Stateç»“æ„ç¡®ä¿æ•°æ®æµæ¸…æ™°
- èŠ‚ç‚¹èŒè´£å•ä¸€ï¼šæ¯ä¸ªèŠ‚ç‚¹åªè´Ÿè´£ä¸€ä¸ªæ˜ç¡®çš„ä»»åŠ¡
- é”™è¯¯å¤„ç†é‡è¦ï¼šç¡®ä¿ä»»åŠ¡å¤±è´¥æ—¶æœ‰é€‚å½“çš„æ¢å¤æœºåˆ¶
- ç»“æœèšåˆç­–ç•¥ï¼šè®¾è®¡æœ‰æ•ˆçš„ç»“æœæ±‡æ€»å’Œå†²çªè§£å†³æœºåˆ¶
```