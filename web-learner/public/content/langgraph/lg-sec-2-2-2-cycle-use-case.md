```markdown
## æ¡ˆä¾‹: å¤šè½®é—®ç­”æˆ–è‡ªæˆ‘ä¿®æ­£

### ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ
å¤šè½®é—®ç­”å’Œè‡ªä¿®æ­£æœºåˆ¶è®© Agent èƒ½å¤Ÿå¤„ç†å¤æ‚æŸ¥è¯¢ï¼Œé€šè¿‡å¾ªç¯è¿­ä»£é€æ­¥å®Œå–„ç­”æ¡ˆæˆ–è¯·æ±‚æ›´å¤šä¿¡æ¯ï¼Œè¿™æ˜¯æ„å»ºæ™ºèƒ½å¯¹è¯ç³»ç»Ÿçš„å…³é”®èƒ½åŠ›ã€‚

### ğŸ’¡ ä½¿ç”¨æ–¹å¼
é€šè¿‡ `add_conditional_edges()` å’Œå¾ªç¯è¾¹å®ç°å¤šè½®äº¤äº’ï¼Œä½¿ç”¨çŠ¶æ€ç®¡ç†è¿½è¸ªå¯¹è¯å†å²ã€‚

### ğŸ“š Level 1: åŸºç¡€è®¤çŸ¥ï¼ˆ30ç§’ç†è§£ï¼‰
ä¸€ä¸ªç®€å•çš„å¤šè½®é—®ç­”ç³»ç»Ÿï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·è¾“å…¥å†³å®šæ˜¯å¦éœ€è¦æ›´å¤šä¿¡æ¯ã€‚

```python
from typing import Literal, TypedDict
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI

# å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    question: str
    history: list[str]
    response: str
    need_clarification: bool

# åˆå§‹åŒ–æ¨¡å‹
llm = ChatOpenAI(model="gpt-3.5-turbo")

# å®šä¹‰é—®ç­”èŠ‚ç‚¹
def answer_node(state: AgentState):
    history_str = "\n".join(state["history"])
    prompt = f"""åŸºäºå¯¹è¯å†å²å›ç­”é—®é¢˜ï¼š
{history_str}
å½“å‰é—®é¢˜: {state['question']}

è¯·ç»™å‡ºä¸“ä¸šå›ç­”ã€‚å¦‚æœé—®é¢˜ä¸å¤Ÿæ˜ç¡®éœ€è¦æ¾„æ¸…ï¼Œè¯·è¯´æ˜éœ€è¦ä»€ä¹ˆé¢å¤–ä¿¡æ¯ã€‚"""
    
    response = llm.invoke(prompt)
    need_clarification = "éœ€è¦æ›´å¤šä¿¡æ¯" in response.content or "ä¸å¤Ÿæ˜ç¡®" in response.content
    
    return {
        "response": response.content,
        "need_clarification": need_clarification,
        "history": state["history"] + [f"ç”¨æˆ·: {state['question']}", f"åŠ©æ‰‹: {response.content}"]
    }

# å®šä¹‰è·¯ç”±é€»è¾‘
def route_question(state: AgentState):
    if state["need_clarification"]:
        return "clarify"
    return "end"

# æ„å»ºå›¾
builder = StateGraph(AgentState)
builder.add_node("answer", answer_node)
builder.set_entry_point("answer")
builder.add_conditional_edges(
    "answer",
    route_question,
    {"clarify": "answer", "end": END}
)

graph = builder.compile()

# è¿è¡Œç¤ºä¾‹
result = graph.invoke({
    "question": "è¯·è§£é‡Šé‡å­è®¡ç®—",
    "history": [],
    "response": "",
    "need_clarification": False
})
print(result["response"])
```

### ğŸ“ˆ Level 2: æ ¸å¿ƒç‰¹æ€§ï¼ˆæ·±å…¥ç†è§£ï¼‰

#### ç‰¹æ€§1: è‡ªæˆ‘ä¿®æ­£æœºåˆ¶
Agent èƒ½å¤Ÿæ£€æµ‹å›ç­”è´¨é‡å¹¶è‡ªåŠ¨ä¿®æ­£ä¸å®Œå–„çš„å›ç­”ã€‚

```python
from typing import Literal, TypedDict
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
import re

class RefinementState(TypedDict):
    question: str
    draft_answer: str
    final_answer: str
    refinement_count: int

llm = ChatOpenAI(model="gpt-3.5-turbo")

def generate_draft(state: RefinementState):
    prompt = f"è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{state['question']}"
    response = llm.invoke(prompt)
    return {"draft_answer": response.content}

def refine_answer(state: RefinementState):
    critique_prompt = f"""è¯·æ‰¹åˆ¤æ€§è¯„ä¼°ä»¥ä¸‹å›ç­”çš„è´¨é‡ï¼š
é—®é¢˜: {state['question']}
å›ç­”: {state['draft_answer']}

æŒ‡å‡ºå›ç­”ä¸­çš„ä¸è¶³æˆ–éœ€è¦æ”¹è¿›çš„åœ°æ–¹ã€‚"""
    
    critique = llm.invoke(critique_prompt)
    
    refinement_prompt = f"""åŸºäºä»¥ä¸‹æ‰¹è¯„æ”¹è¿›å›ç­”ï¼š
åŸå§‹é—®é¢˜: {state['question']}
åŸå§‹å›ç­”: {state['draft_answer']}
æ‰¹è¯„æ„è§: {critique.content}

è¯·æä¾›æ”¹è¿›åçš„å›ç­”ã€‚"""
    
    refined = llm.invoke(refinement_prompt)
    
    return {
        "draft_answer": refined.content,
        "refinement_count": state["refinement_count"] + 1
    }

def should_continue(state: RefinementState):
    if state["refinement_count"] >= 2:  # æœ€å¤šä¿®æ­£2æ¬¡
        return "end"
    
    quality_check = f"""è¯„ä¼°ä»¥ä¸‹å›ç­”çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§ï¼š
é—®é¢˜: {state['question']}
å›ç­”: {state['draft_answer']}

å¦‚æœå›ç­”å·²ç»å®Œæ•´å‡†ç¡®ï¼Œå›å¤'EXCELLENT'ï¼Œå¦åˆ™å›å¤'NEEDS_IMPROVEMENT'ã€‚"""
    
    assessment = llm.invoke(quality_check)
    if "EXCELLENT" in assessment.content:
        return "end"
    return "refine"

builder = StateGraph(RefinementState)
builder.add_node("draft", generate_draft)
builder.add_node("refine", refine_answer)
builder.set_entry_point("draft")
builder.add_conditional_edges("draft", should_continue, 
                             {"refine": "refine", "end": END})
builder.add_conditional_edges("refine", should_continue,
                             {"refine": "refine", "end": END})

refinement_graph = builder.compile()

# æµ‹è¯•è‡ªæˆ‘ä¿®æ­£
result = refinement_graph.invoke({
    "question": "è¯·è¯¦ç»†è§£é‡ŠTransformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨",
    "draft_answer": "",
    "final_answer": "",
    "refinement_count": 0
})
print(f"æœ€ç»ˆå›ç­”ï¼ˆç»è¿‡{result['refinement_count']}æ¬¡ä¿®æ­£ï¼‰ï¼š")
print(result['draft_answer'])
```

### ğŸ” Level 3: å¯¹æ¯”å­¦ä¹ ï¼ˆé¿å…é™·é˜±ï¼‰

```python
# é”™è¯¯ç”¨æ³•ï¼šç¼ºå°‘ç»ˆæ­¢æ¡ä»¶çš„æ— é™å¾ªç¯
def faulty_router(state):
    return "always_loop"  # æ€»æ˜¯è¿”å›å¾ªç¯ï¼Œå¯¼è‡´æ— é™å¾ªç¯

# æ­£ç¡®ç”¨æ³•ï¼šç¡®ä¿æœ‰ç»ˆæ­¢æ¡ä»¶
def proper_router(state):
    if state["iteration_count"] > 5:  # è®¾ç½®æœ€å¤§è¿­ä»£æ¬¡æ•°
        return "end"
    if "ç­”æ¡ˆå®Œæ•´" in state["assessment"]:
        return "end"
    return "continue_loop"
```

### ğŸš€ Level 4: å®æˆ˜åº”ç”¨ï¼ˆçœŸå®åœºæ™¯ï¼‰
æ„å»ºä¸€ä¸ªæ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ï¼Œèƒ½å¤Ÿè¿›è¡Œå¤šè½®é—®ç­”å¹¶è‡ªæˆ‘ä¿®æ­£ã€‚

```python
from typing import TypedDict, Literal
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
import asyncio

class ResearchState(TypedDict):
    research_topic: str
    conversation_history: list
    current_query: str
    current_response: str
    research_depth: int
    needs_more_info: bool

llm = ChatOpenAI(model="gpt-3.5-turbo")

def research_agent(state: ResearchState):
    history = "\n".join(state["conversation_history"][-5:])  # æœ€è¿‘5è½®å¯¹è¯
    
    prompt = f"""ä½œä¸ºç ”ç©¶åŠ©æ‰‹ï¼Œè¯·å¸®åŠ©ç”¨æˆ·æ·±å…¥ç ”ç©¶ä¸»é¢˜ï¼š{state['research_topic']}

å¯¹è¯å†å²ï¼š
{history}

å½“å‰æŸ¥è¯¢ï¼š{state['current_query']}

è¯·æä¾›è¯¦ç»†çš„ç ”ç©¶æ€§å›ç­”ã€‚å¦‚æœéœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯¢é—®ã€‚"""
    
    response = llm.invoke(prompt)
    needs_info = any(phrase in response.content.lower() 
                    for phrase in ["è¯·é—®", "éœ€è¦æ›´å¤š", "èƒ½å¦æä¾›"])
    
    return {
        "current_response": response.content,
        "needs_more_info": needs_info,
        "conversation_history": state["conversation_history"] + [
            f"ç”¨æˆ·: {state['current_query']}",
            f"åŠ©æ‰‹: {response.content}"
        ],
        "research_depth": state["research_depth"] + 1
    }

def research_router(state: ResearchState):
    if state["needs_more_info"] and state["research_depth"] < 5:
        return "continue_research"
    if state["research_depth"] >= 5:
        return "end"
    return "end"

def user_simulator(state: ResearchState):
    # æ¨¡æ‹Ÿç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯
    follow_up = f"å…³äº{state['research_topic']}ï¼Œæˆ‘æƒ³äº†è§£æ›´å¤šç»†èŠ‚"
    return {"current_query": follow_up}

# æ„å»ºç ”ç©¶åŠ©æ‰‹å›¾
builder = StateGraph(ResearchState)
builder.add_node("research", research_agent)
builder.add_node("get_user_input", user_simulator)
builder.set_entry_point("research")

builder.add_conditional_edges(
    "research",
    research_router,
    {"continue_research": "get_user_input", "end": END}
)
builder.add_edge("get_user_input", "research")

research_graph = builder.compile()

# è¿è¡Œç ”ç©¶åŠ©æ‰‹
async def run_research_assistant():
    result = await research_graph.ainvoke({
        "research_topic": "äººå·¥æ™ºèƒ½ä¼¦ç†",
        "conversation_history": [],
        "current_query": "è¯·ä»‹ç»AIä¼¦ç†çš„ä¸»è¦åŸåˆ™",
        "current_response": "",
        "research_depth": 0,
        "needs_more_info": False
    })
    
    print("ç ”ç©¶å®Œæˆï¼å¯¹è¯è½®æ•°:", result["research_depth"])
    print("æœ€ç»ˆå›ç­”:", result["current_response"])

# asyncio.run(run_research_assistant())
```

### ğŸ’¡ è®°å¿†è¦ç‚¹
- å¾ªç¯æœºåˆ¶é€šè¿‡ `add_conditional_edges()` å®ç°ï¼Œå¿…é¡»åŒ…å«æ˜ç¡®çš„ç»ˆæ­¢æ¡ä»¶
- çŠ¶æ€ç®¡ç†æ˜¯å…³é”®ï¼Œéœ€è¦å¦¥å–„ç»´æŠ¤å¯¹è¯å†å²å’Œè¿­ä»£è®¡æ•°
- è‡ªæˆ‘ä¿®æ­£éœ€è¦è®¾è®¡è´¨é‡è¯„ä¼°æœºåˆ¶æ¥å†³ç­–æ˜¯å¦ç»§ç»­è¿­ä»£
- å¤šè½®é—®ç­”è¦åˆç†å¤„ç†ä¿¡æ¯éœ€æ±‚å’Œä¸Šä¸‹æ–‡ç»´æŠ¤
```