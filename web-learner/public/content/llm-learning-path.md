# 语言大模型LLM (id: llm)

## 第1章：第一章：语言的数学抽象 · 万物皆为序列 (id: llm-ch-1)
### 1.1 第一章：语言的数学抽象 · 万物皆为序列 (id: llm-ch-1-gr-1)
#### 1.1.1 1.1 根本问题：如何让机器“理解”语言？ (id: llm-sec-1-1)
#### 1.1.2 1.2 经典思路：N-Gram模型 (id: llm-sec-1-2)
#### 1.1.3 1.3 核心基石：词向量（Word Embedding） (id: llm-sec-1-3)

## 第2章：第二章：神经网络的记忆革命 · 从RNN到Encoder-Decoder (id: llm-ch-2)
### 2.1 第二章：神经网络的记忆革命 · 从RNN到Encoder-Decoder (id: llm-ch-2-gr-1)
#### 2.1.1 2.1 新问题：如何处理任意长度的上下文？ (id: llm-sec-2-1)
#### 2.1.2 2.2 解决方案：循环神经网络（RNN） (id: llm-sec-2-2)
#### 2.1.3 2.3 改进方案：长短期记忆网络（LSTM） (id: llm-sec-2-3)
#### 2.1.4 2.4 架构范式：编码器-解码器（Encoder-Decoder） (id: llm-sec-2-4)

## 第3章：第三章：Transformer架构 · 注意力是核心驱动力 (id: llm-ch-3)
### 3.1 第三章：Transformer架构 · 注意力是核心驱动力 (id: llm-ch-3-gr-1)
#### 3.1.1 3.1 终极问题：能否摆脱序列依赖，并行处理一切？ (id: llm-sec-3-1)
#### 3.1.2 3.2 核心思想：自注意力机制（Self-Attention） (id: llm-sec-3-2)
#### 3.1.3 3.3 架构剖析：Transformer的核心组件 (id: llm-sec-3-3)
#### 3.1.4 3.4 全景图：GPT与BERT的不同路径 (id: llm-sec-3-4)

## 第4章：第四章：大模型时代 · 规模、涌现与对齐 (id: llm-ch-4)
### 4.1 第四章：大模型时代 · 规模、涌现与对齐 (id: llm-ch-4-gr-1)
#### 4.1.1 4.1 第一性原理：规模法则（Scaling Laws） (id: llm-sec-4-1)
#### 4.1.2 4.2 核心范式：预训练-微调（Pre-training & Fine-tuning） (id: llm-sec-4-2)
#### 4.1.3 4.3 惊人能力：上下文学习（In-Context Learning） (id: llm-sec-4-3)
#### 4.1.4 4.4 对齐技术：让AI与人类价值观一致 (id: llm-sec-4-4)
