{
  "id": "natural-language-processing",
  "title": "自然语言处理",
  "subject": "natural-language-processing",
  "chapters": [
    {
      "id": "nlp-ch-1",
      "title": "第1章：基础篇 · 让机器理解语言的基石",
      "groups": [],
      "sections": [
        {
          "id": "nlp-sec-1-1",
          "title": "根本问题：为何机器处理文本如此困难？",
          "chapterId": "nlp-ch-1"
        },
        {
          "id": "nlp-sec-1-2",
          "title": "文本预处理：从原始语料到结构化词元流",
          "chapterId": "nlp-ch-1"
        }
      ]
    },
    {
      "id": "nlp-ch-2",
      "title": "第2章：文本表示 · 将词语转化为向量",
      "groups": [],
      "sections": [
        {
          "id": "nlp-sec-2-1",
          "title": "根本问题：如何用数学语言表示词汇的含义？",
          "chapterId": "nlp-ch-2"
        },
        {
          "id": "nlp-sec-2-2",
          "title": "早期思想：基于统计的稀疏表示",
          "chapterId": "nlp-ch-2"
        },
        {
          "id": "nlp-sec-2-3",
          "title": "范式革命：基于预测的密集表示",
          "chapterId": "nlp-ch-2"
        },
        {
          "id": "nlp-sec-2-4",
          "title": "承上启下：静态向量的局限与上下文的呼唤",
          "chapterId": "nlp-ch-2"
        }
      ]
    },
    {
      "id": "nlp-ch-3",
      "title": "第3章：序列建模 · 捕捉文本中的时序依赖",
      "groups": [],
      "sections": [
        {
          "id": "nlp-sec-3-1",
          "title": "根本问题：如何让模型拥有“记忆”来处理序列？",
          "chapterId": "nlp-ch-3"
        },
        {
          "id": "nlp-sec-3-2",
          "title": "拆解关键机制：长短期记忆网络与门控循环单元",
          "chapterId": "nlp-ch-3"
        },
        {
          "id": "nlp-sec-3-3",
          "title": "实践指南：构建与应用序列模型",
          "chapterId": "nlp-ch-3"
        }
      ]
    },
    {
      "id": "nlp-ch-4",
      "title": "第4章：序列到序列 · 从编码到生成的跨越",
      "groups": [],
      "sections": [
        {
          "id": "nlp-sec-4-1",
          "title": "根本问题：如何处理输入和输出序列长度不同的任务？",
          "chapterId": "nlp-ch-4"
        },
        {
          "id": "nlp-sec-4-2",
          "title": "范式革命：注意力机制",
          "chapterId": "nlp-ch-4"
        }
      ]
    },
    {
      "id": "nlp-ch-5",
      "title": "第5章：Transformer · 注意力是全部所需",
      "groups": [],
      "sections": [
        {
          "id": "nlp-sec-5-1",
          "title": "根本问题：如何摆脱RNN的顺序计算限制，实现大规模并行？",
          "chapterId": "nlp-ch-5"
        },
        {
          "id": "nlp-sec-5-2",
          "title": "拆解关键机制：自注意力与多头注意力",
          "chapterId": "nlp-ch-5"
        },
        {
          "id": "nlp-sec-5-3",
          "title": "拆解关键机制：位置编码、残差连接与层归一化",
          "chapterId": "nlp-ch-5"
        }
      ]
    },
    {
      "id": "nlp-ch-6",
      "title": "第6章：新范式 · 预训练、提示与微调",
      "groups": [],
      "sections": [
        {
          "id": "nlp-sec-6-1",
          "title": "核心思想：从零训练到“迁移学习”的范式转变",
          "chapterId": "nlp-ch-6"
        },
        {
          "id": "nlp-sec-6-2",
          "title": "工具一：BERT及其变体",
          "chapterId": "nlp-ch-6"
        },
        {
          "id": "nlp-sec-6-3",
          "title": "工具二：GPT系列与生成式预训练",
          "chapterId": "nlp-ch-6"
        },
        {
          "id": "nlp-sec-6-4",
          "title": "工具三：T5/BART与序列到序列预训练",
          "chapterId": "nlp-ch-6"
        }
      ]
    },
    {
      "id": "nlp-ch-7",
      "title": "第7章：大型语言模型 · 涌现能力与新范式",
      "groups": [],
      "sections": [
        {
          "id": "nlp-sec-7-1",
          "title": "根本问题：当模型规模达到临界点会发生什么？",
          "chapterId": "nlp-ch-7"
        },
        {
          "id": "nlp-sec-7-2",
          "title": "核心交互范式：提示工程与上下文学习",
          "chapterId": "nlp-ch-7"
        },
        {
          "id": "nlp-sec-7-3",
          "title": "对齐技术：从人类反馈中学习",
          "chapterId": "nlp-ch-7"
        },
        {
          "id": "nlp-sec-7-4",
          "title": "实践与挑战：RAG、评估与伦理",
          "chapterId": "nlp-ch-7"
        }
      ]
    }
  ]
}