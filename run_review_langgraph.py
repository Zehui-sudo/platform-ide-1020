"""
LangGraph-based agent team to review Markdown content.

Reimplements the logic from run_review_crew.py using LangGraph, avoiding CrewAI.

Pipeline:
1) Plan: Extract chapter numbers from `web-learner/public/javascript-learning-path.md`.
2) Review: For each chapter, read all chapter markdowns from `web-learner/public/content/`
   and have an LLM review per file into a structured JSON list.
3) Synthesize: Summarize structured findings into a final Markdown report.

Requirements (install if missing):
  pip install -U langgraph langchain langchain-google-genai

Environment/config:
- Reads `config.json` and uses `gemini_api_key` if `GOOGLE_API_KEY` is not set.
- Model defaults to `gemini-2.5-pro` and can be overridden by `config.json:model`.
"""

from __future__ import annotations

import json
import os
import re
import sys
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, TypedDict, Annotated
import argparse
import asyncio
from operator import add

# Soft-imports with helpful error if missing
try:
    from langgraph.graph import StateGraph, START, END
except Exception as e:  # pragma: no cover
    sys.stderr.write(
        "[é”™è¯¯] æœªæ‰¾åˆ° langgraphã€‚è¯·å…ˆå®‰è£…ï¼š\n"
        "  python3 -m pip install -U langgraph\n"
    )
    raise

# Optional: langchain-google-genai. If missing or broken, we fallback to google-generativeai
try:
    from langchain_google_genai import ChatGoogleGenerativeAI  # type: ignore
    _LC_GENAI_AVAILABLE = True
    _LC_GENAI_IMPORT_ERR = None
except Exception as e:  # pragma: no cover
    ChatGoogleGenerativeAI = None  # type: ignore
    _LC_GENAI_AVAILABLE = False
    _LC_GENAI_IMPORT_ERR = e


# --- 0. é…ç½®ä¸æ¨¡å‹åˆå§‹åŒ– ---

BASE_DIR = Path(__file__).parent.resolve()
CONFIG_PATH = BASE_DIR / "config.json"


def _load_config() -> Dict[str, Any]:
    try:
        with open(CONFIG_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return {}
    except Exception:
        return {}


cfg = _load_config()

# ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå…¶æ¬¡ä½¿ç”¨ config.json çš„ gemini_api_key
if not os.environ.get("GOOGLE_API_KEY") and cfg.get("gemini_api_key"):
    os.environ["GOOGLE_API_KEY"] = cfg.get("gemini_api_key")

MODEL_NAME = cfg.get("model", "gemini-2.5-pro")
SHOW_PROMPTS = str(os.environ.get("SHOW_PROMPTS", "0")).lower() in {"1", "true", "yes", "on"}

def _init_llm():
    # Prefer langchain wrapper if available and not explicitly disabled
    if _LC_GENAI_AVAILABLE and str(os.environ.get("USE_NATIVE_GEMINI", "0")).lower() not in {"1","true","yes","on"}:
        try:
            return ChatGoogleGenerativeAI(model=MODEL_NAME)  # type: ignore
        except Exception as e:  # pragma: no cover
            sys.stderr.write(
                f"[è­¦å‘Š] åˆå§‹åŒ– ChatGoogleGenerativeAI å¤±è´¥ï¼Œåˆ‡æ¢åˆ°åŸç”Ÿ google-generativeaiã€‚åŸå› : {e}\n"
            )
    # Fallback: native google-generativeai client
    try:
        import google.generativeai as genai  # type: ignore
    except Exception:
        sys.stderr.write(
            "[é”™è¯¯] æœªæ‰¾åˆ° google-generativeaiã€‚è¯·å®‰è£…ï¼š\n"
            "  python3 -m pip install -U google-generativeai\n"
        )
        # åŒæ—¶æç¤ºæ½œåœ¨çš„ protobuf å…¼å®¹æ€§æ–¹æ¡ˆ
        if _LC_GENAI_IMPORT_ERR:
            sys.stderr.write(
                "æ­¤å¤–ï¼Œå¯å°è¯•è§£å†³ protobuf å…¼å®¹æ€§ï¼š\n"
                "  python3 -m pip install 'protobuf<5' 'googleapis-common-protos<2'\n"
            )
        raise SystemExit(1)

    api_key = os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        sys.stderr.write("[é”™è¯¯] ç¼ºå°‘ GOOGLE_API_KEY ç¯å¢ƒå˜é‡æˆ– config.json.gemini_api_keyã€‚\n")
        raise SystemExit(1)
    genai.configure(api_key=api_key)

    model = genai.GenerativeModel(MODEL_NAME)

    class _NativeLLM:
        def __init__(self, m):
            self._m = m

        def invoke(self, prompt: str):
            resp = self._m.generate_content(prompt)
            text = getattr(resp, "text", "") or "".join(
                getattr(resp, "candidates", []) and [
                    getattr(getattr(resp.candidates[0], "content", None), "parts", [])[0].text  # type: ignore
                ] or [""]
            )

            class R:
                def __init__(self, content: str):
                    self.content = content

            return R(text)

        async def ainvoke(self, prompt: str):
            # åœ¨åŸç”Ÿ SDK ä¸Šç”¨çº¿ç¨‹æ± é¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            return await asyncio.to_thread(self.invoke, prompt)

    return _NativeLLM(model)


# åˆå§‹åŒ–Geminiæ¨¡å‹ï¼ˆä¼˜å…ˆ langchain-google-genaiï¼Œå¤±è´¥åˆ™å›é€€åˆ°åŸç”Ÿ SDKï¼‰
llm = _init_llm()

# å…¨å±€å¹¶å‘æ§åˆ¶ï¼ˆé»˜è®¤ 8ï¼Œå¯ç”¨ MAX_PARALLEL_REVIEWS è¦†ç›–ï¼‰
MAX_PARALLEL = int(os.environ.get("MAX_PARALLEL_REVIEWS", "8") or "8")
_SEM = asyncio.Semaphore(MAX_PARALLEL)


# --- 1. æ•°æ®ä¸å·¥å…· ---

LEARNING_PATH_FILE = BASE_DIR / "web-learner/public/javascript-learning-path.md"
CONTENT_DIR = BASE_DIR / "web-learner/public/content/"


def _read_text(p: Path) -> str:
    try:
        return p.read_text(encoding="utf-8")
    except Exception as e:
        return f"[è¯»å–å¤±è´¥] {p.name}: {e}"


def _extract_chapter_numbers_from_learning_path(md_text: str) -> List[str]:
    # åŒ¹é…å½¢å¦‚ â€œç¬¬1ç« â€ã€â€œç¬¬ 2 ç« â€ ç­‰ï¼Œå°½é‡é²æ£’
    nums = re.findall(r"ç¬¬\s*(\d+)\s*ç« ", md_text)
    # å»é‡å¹¶æŒ‰æ•°å­—æ’åº
    unique = sorted({int(n) for n in nums})
    return [str(n) for n in unique]


def _read_chapter_content(chapter_number: str) -> str:
    base_path = CONTENT_DIR
    all_files_content = ""
    prefix = f"js-sec-{chapter_number}-"
    try:
        for filename in sorted(os.listdir(base_path)):
            if filename.startswith(prefix) and filename.endswith(".md"):
                file_path = base_path / filename
                try:
                    content = file_path.read_text(encoding="utf-8")
                    all_files_content += (
                        f"--- START OF FILE: {filename} ---\n\n{content}\n\n--- END OF FILE: {filename}---\n\n"
                    )
                except Exception as e:
                    all_files_content += f"--- ERROR READING FILE: {filename} - {e}---\n"
        if not all_files_content:
            return f"æ²¡æœ‰æ‰¾åˆ°ç« èŠ‚ {chapter_number} çš„æ–‡ä»¶ï¼Œæ–‡ä»¶åå‰ç¼€ä¸º '{prefix}'ã€‚"
        return all_files_content
    except Exception as e:
        return f"è®¿é—®ç›®å½• {base_path} æ—¶å‡ºé”™: {e}"


def _list_chapter_files(chapter_number: str) -> List[Path]:
    prefix = f"js-sec-{chapter_number}-"
    files: List[Path] = []
    try:
        for filename in sorted(os.listdir(CONTENT_DIR)):
            if filename.startswith(prefix) and filename.endswith(".md"):
                files.append(CONTENT_DIR / filename)
    except Exception:
        return []
    return files


def _try_parse_json_list(text: str) -> List[Dict[str, Any]]:
    # å¼ºåŠ›è§£æå™¨ï¼šå°½å¯èƒ½ä»æ–‡æœ¬ä¸­æå–ä¸€ä¸ª JSON æ•°ç»„
    text = text.strip()
    # å¸¸è§æƒ…å†µï¼šåŒ…åœ¨ä»£ç å—ä¸­
    if "```" in text:
        # å–å‡ºç¬¬ä¸€ä¸ªä»£ç å—å†…å®¹
        parts = text.split("```")
        for part in parts:
            part = part.strip()
            if part.startswith("[") and part.endswith("]"):
                try:
                    return json.loads(part)
                except Exception:
                    pass
    # å›é€€ï¼šåœ¨å…¨æ–‡ä¸­å¯»æ‰¾ç¬¬ä¸€ä¸ª [ ä¸æœ€åä¸€ä¸ª ]
    try:
        start = text.index("[")
        end = text.rindex("]")
        return json.loads(text[start : end + 1])
    except Exception:
        return []


# --- 2. LangGraph çŠ¶æ€ä¸èŠ‚ç‚¹ ---


class ReviewState(TypedDict, total=False):
    chapters: List[str]
    reviews: Annotated[List[Dict[str, Any]], add]
    report: str
    chapters_filter: List[str]
    current_chapter: str


def _apply_chapter_filter(all_chapters: List[str], filter_list: List[str]) -> List[str]:
    if not filter_list:
        return all_chapters
    filt = [c.strip() for c in filter_list if c and c.strip().isdigit()]
    allowed = set(filt)
    return [c for c in all_chapters if c in allowed]


def _infer_chapters_from_content_dir() -> List[str]:
    """å½“å­¦ä¹ è·¯å¾„å¤§çº²ç¼ºå¤±æˆ–æ— æ³•è¯†åˆ«ç« èŠ‚æ—¶ï¼Œä»å†…å®¹ç›®å½•æ–‡ä»¶åæ¨æ–­ç« èŠ‚å·ã€‚"""
    chapters_set = set()
    try:
        for filename in sorted(os.listdir(CONTENT_DIR)):
            # ä»…åŒ¹é…å½¢å¦‚ js-sec-7-... çš„æ–‡ä»¶
            m = re.match(r"js-sec-(\d+)-", filename)
            if m and filename.endswith(".md"):
                try:
                    chapters_set.add(int(m.group(1)))
                except Exception:
                    pass
    except Exception:
        return []
    return [str(n) for n in sorted(chapters_set)]
    
def plan_node(state: ReviewState) -> ReviewState:
    print("\nğŸ” [Plan] è¯»å–è¯¾ç¨‹å¤§çº²:", LEARNING_PATH_FILE)
    md_text = _read_text(LEARNING_PATH_FILE)
    chapters_all = _extract_chapter_numbers_from_learning_path(md_text)
    chapters_filter = state.get("chapters_filter", []) or []
    chapters = _apply_chapter_filter(chapters_all, chapters_filter)
    if chapters_filter:
        print("ğŸ¯ [Plan] ç›®æ ‡ç« èŠ‚è¿‡æ»¤:", ", ".join(chapters_filter))
        missing = [c for c in chapters_filter if c not in chapters_all]
        if missing:
            print("âš ï¸ [Plan] è¿‡æ»¤ä¸­åŒ…å«æœªåœ¨å¤§çº²æ‰¾åˆ°çš„ç« èŠ‚:", ", ".join(missing))
    if chapters:
        print("âœ… [Plan] è¯†åˆ«ç« èŠ‚ç¼–å·:", ", ".join(chapters))
    else:
        print("âš ï¸ [Plan] æœªèƒ½ä»å¤§çº²ä¸­è¯†åˆ«åˆ°ç« èŠ‚ç¼–å·ã€‚")
    return {**state, "chapters": chapters}


def dispatch_chapters(state: ReviewState):
    chapters = state.get("chapters", []) or []
    if not chapters:
        print("âš ï¸ [Dispatch] æ— å¯ç”¨ç« èŠ‚ï¼Œè·³è¿‡åˆ†å‘ã€‚")
        return state
    print(f"\nğŸšš [Dispatch] æ´¾å‘ {len(chapters)} ä¸ªç« èŠ‚è¿›è¡Œå¹¶è¡Œå®¡æŸ¥...")
    return [Send("review_one", {"current_chapter": ch}) for ch in chapters]


async def review_one_chapter(state: ReviewState) -> ReviewState:
    ch = state.get("current_chapter", "")
    if not ch:
        return {}
    print(f"\nğŸ“š [Review-One] å¼€å§‹å®¡æŸ¥ç« èŠ‚: {ch}")
    files = _list_chapter_files(ch)
    print(f"ğŸ“‚ [Review-One] ç¬¬{ch}ç« åŒ¹é…åˆ° {len(files)} ä¸ªæ–‡ä»¶")
    for f in files[:20]:
        print("  -", f.name)

    system_preamble = (
        "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„JavaScriptä¸“å®¶å’Œä¸€ä¸ä¸è‹Ÿçš„æŠ€æœ¯ç¼–è¾‘ã€‚"
        "è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚å¯¹æ–‡ä»¶è¿›è¡Œå®¡æŸ¥ï¼Œå¹¶ä»…è¾“å‡ºç¬¦åˆè§„èŒƒçš„JSONæ•°ç»„ã€‚"
    )
    instruction = (
        "é’ˆå¯¹æä¾›çš„ç« èŠ‚å·ä¸è¯¥ç« èŠ‚å†…çš„æ‰€æœ‰æ–‡ä»¶å†…å®¹ï¼Œé€ä¸ªæ–‡ä»¶è¿›è¡Œå®¡æŸ¥ã€‚\n\n"
        "ã€å®¡æŸ¥æŒ‡ä»¤ã€‘\n"
        "1. ä¸»é¢˜ä¸€è‡´æ€§: æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸æ–‡ä»¶æ ‡é¢˜åŠå…¶åœ¨è¯¾ç¨‹ä½“ç³»ä¸­çš„ä½ç½®ç›¸ç¬¦ã€‚\n"
        "2. å†…å®¹ç‹¬ç‰¹æ€§: åˆ¤æ–­æ˜¯å¦ä¸å…¶å®ƒéƒ¨åˆ†åº”æœ‰çš„å†…å®¹å­˜åœ¨æ˜¾è‘—é‡å ã€‚\n"
        "3. è´¨é‡ä¿è¯: ç¤ºä¾‹æ˜¯å¦æ¸…æ™°ä¸”æ­£ç¡®ï¼Œè§£é‡Šæ˜¯å¦ç¬¦åˆé€»è¾‘ã€‚\n\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘\n"
        "- ä»…è¾“å‡ºä¸€ä¸ª JSON æ•°ç»„å­—ç¬¦ä¸²ï¼Œä¸è¦æ·»åŠ é¢å¤–æ–‡æœ¬ã€‚\n"
        "- æ•°ç»„ä¸­çš„æ¯ä¸ªå¯¹è±¡å¿…é¡»åŒ…å«é”®: \"file_name\", \"chapter\", \"topic_adherence_score\", \"topic_adherence_comment\", \"uniqueness_score\", \"uniqueness_comment\", \"quality_score\", \"quality_comment\"ã€‚\n"
        "- æ‰€æœ‰åˆ†æ•°å­—æ®µçš„å–å€¼åªèƒ½æ˜¯: \"OK\"ã€\"Warning\"ã€\"Critical Error\"ã€‚\n"
    )

    content = _read_chapter_content(ch)
    prompt = (
        f"{system_preamble}\n\n"
        f"å½“å‰ç« èŠ‚: {ch}\n\n"
        f"{instruction}\n"
        f"ä»¥ä¸‹ä¸ºè¯¥ç« èŠ‚çš„æ‰€æœ‰æ–‡ä»¶å†…å®¹ï¼ˆå¸¦æœ‰æ–‡ä»¶åè¾¹ç•Œæ ‡è®°ï¼‰ï¼š\n\n"
        f"{content}\n"
        f"è¯·ç”Ÿæˆ JSON æ•°ç»„ï¼Œç¡®ä¿æ¯ä¸ªå¯¹è±¡åŒ…å«æ–‡ä»¶å(æ¥è‡ªè¾¹ç•Œæ ‡è®°)ã€ç« èŠ‚å·ç­‰å­—æ®µã€‚"
    )

    try:
        print(f"ğŸ§  [Review-One] å‘æ¨¡å‹å‘é€è¯·æ±‚ â†’ ç¬¬{ch}ç«  ...")
        if SHOW_PROMPTS:
            print("â€”" * 40)
            print("[Prompt - ç¬¬" + ch + "ç« ]\n" + prompt)
            print("â€”" * 40)
        async with _SEM:
            if hasattr(llm, "ainvoke"):
                resp = await llm.ainvoke(prompt)  # type: ignore
            else:
                resp = await asyncio.to_thread(llm.invoke, prompt)  # type: ignore
        print("ğŸ“¥ [Review-One] å·²æ”¶åˆ°æ¨¡å‹å“åº”ï¼Œè§£æJSON...")
        text = getattr(resp, "content", str(resp))
        parsed = _try_parse_json_list(text)
        for item in parsed:
            item.setdefault("chapter", ch)
        if not parsed:
            sys.stderr.write(f"[è­¦å‘Š] ç¬¬{ch}ç« å®¡æŸ¥ç»“æœè§£æå¤±è´¥ï¼Œè·³è¿‡æˆ–ä¸ºç©ºã€‚\n")
        else:
            # å±€éƒ¨ç»Ÿè®¡ä»…æ—¥å¿—ç”¨é€”
            levels = {"OK": 0, "Warning": 0, "Critical Error": 0}
            for it in parsed:
                for k in ["topic_adherence_score","uniqueness_score","quality_score"]:
                    v = str(it.get(k, "")).strip()
                    if v in levels:
                        levels[v] += 1
            print(
                f"âœ… [Review-One] ç¬¬{ch}ç« è§£ææˆåŠŸ: {len(parsed)} æ¡è®°å½• | "
                f"OK={levels['OK']} Warning={levels['Warning']} Critical={levels['Critical Error']}"
            )
        return {"reviews": parsed}
    except Exception as e:
        sys.stderr.write(f"[é”™è¯¯] ç¬¬{ch}ç« å®¡æŸ¥å¤±è´¥: {e}\n")
        return {"reviews": []}


async def review_parallel_node(state: ReviewState) -> ReviewState:
    """åœ¨å•ä¸ª LangGraph èŠ‚ç‚¹å†…å¹¶è¡Œå®¡æŸ¥æ‰€æœ‰ç« èŠ‚ï¼ˆä½¿ç”¨ asyncio.gather + Semaphoreï¼‰ã€‚"""
    chapters = state.get("chapters", []) or []
    if not chapters:
        print("âš ï¸ [Review-Parallel] æœªå‘ç°ç« èŠ‚ï¼Œè·³è¿‡å¹¶è¡Œå®¡æŸ¥ã€‚")
        return {**state, "reviews": []}

    print(f"\nâš™ï¸ [Review-Parallel] å‡†å¤‡å¹¶è¡Œå¤„ç† {len(chapters)} ä¸ªç« èŠ‚ï¼ˆå¹¶å‘ä¸Šé™={MAX_PARALLEL}ï¼‰...")
    tasks = [review_one_chapter({"current_chapter": ch}) for ch in chapters]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    aggregated: List[Dict[str, Any]] = []
    for idx, res in enumerate(results):
        if isinstance(res, Exception):
            sys.stderr.write(f"[é”™è¯¯] å¹¶è¡Œä»»åŠ¡ {idx} å¤±è´¥: {res}\n")
            continue
        if isinstance(res, dict):
            aggregated.extend(res.get("reviews", []))

    print(f"ğŸ“Š [Review-Parallel] å¹¶è¡Œå®Œæˆï¼Œç´¯è®¡è®°å½•: {len(aggregated)}")
    return {**state, "reviews": aggregated}


def review_node(state: ReviewState) -> ReviewState:
    # ä¿ç•™æ—§çš„ä¸²è¡Œå®ç°ä»¥ä¾¿å›é€€æˆ–å¯¹æ¯”ï¼Œä¸åœ¨ä¸»å›¾ä¸­ä½¿ç”¨
    chapters = state.get("chapters", [])
    all_reviews: List[Dict[str, Any]] = []

    system_preamble = (
        "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„JavaScriptä¸“å®¶å’Œä¸€ä¸ä¸è‹Ÿçš„æŠ€æœ¯ç¼–è¾‘ã€‚"
        "è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚å¯¹æ–‡ä»¶è¿›è¡Œå®¡æŸ¥ï¼Œå¹¶ä»…è¾“å‡ºç¬¦åˆè§„èŒƒçš„JSONæ•°ç»„ã€‚"
    )

    instruction = (
        "é’ˆå¯¹æä¾›çš„ç« èŠ‚å·ä¸è¯¥ç« èŠ‚å†…çš„æ‰€æœ‰æ–‡ä»¶å†…å®¹ï¼Œé€ä¸ªæ–‡ä»¶è¿›è¡Œå®¡æŸ¥ã€‚\n\n"
        "ã€å®¡æŸ¥æŒ‡ä»¤ã€‘\n"
        "1. ä¸»é¢˜ä¸€è‡´æ€§: æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸æ–‡ä»¶æ ‡é¢˜åŠå…¶åœ¨è¯¾ç¨‹ä½“ç³»ä¸­çš„ä½ç½®ç›¸ç¬¦ã€‚\n"
        "2. å†…å®¹ç‹¬ç‰¹æ€§: åˆ¤æ–­æ˜¯å¦ä¸å…¶å®ƒéƒ¨åˆ†åº”æœ‰çš„å†…å®¹å­˜åœ¨æ˜¾è‘—é‡å ã€‚\n"
        "3. è´¨é‡ä¿è¯: ä»£ç ç¤ºä¾‹æ˜¯å¦æ¸…æ™°ä¸”æ­£ç¡®ï¼Œè§£é‡Šæ˜¯å¦ç¬¦åˆé€»è¾‘ã€‚\n\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘\n"
        "- ä»…è¾“å‡ºä¸€ä¸ª JSON æ•°ç»„å­—ç¬¦ä¸²ï¼Œä¸è¦æ·»åŠ é¢å¤–æ–‡æœ¬ã€‚\n"
        "- æ•°ç»„ä¸­çš„æ¯ä¸ªå¯¹è±¡å¿…é¡»åŒ…å«é”®: \"file_name\", \"chapter\", \"topic_adherence_score\", \"topic_adherence_comment\", \"uniqueness_score\", \"uniqueness_comment\", \"quality_score\", \"quality_comment\"ã€‚\n"
        "- æ‰€æœ‰åˆ†æ•°å­—æ®µçš„å–å€¼åªèƒ½æ˜¯: \"OK\"ã€\"Warning\"ã€\"Critical Error\"ã€‚\n"
    )

    def _count_severity(items: List[Dict[str, Any]]):
        levels = {"OK": 0, "Warning": 0, "Critical Error": 0}
        keys = [
            "topic_adherence_score",
            "uniqueness_score",
            "quality_score",
        ]
        for it in items:
            for k in keys:
                v = str(it.get(k, "")).strip()
                if v in levels:
                    levels[v] += 1
        return levels

    for ch in chapters:
        print("\nğŸ“š [Review] å¼€å§‹å®¡æŸ¥ç« èŠ‚:", ch)
        files = _list_chapter_files(ch)
        print(f"ğŸ“‚ [Review] ç¬¬{ch}ç« åŒ¹é…åˆ° {len(files)} ä¸ªæ–‡ä»¶")
        for f in files[:20]:  # é¿å…æ§åˆ¶å°è¿‡é•¿ï¼Œåªå±•ç¤ºå‰20ä¸ª
            print("  -", f.name)
        content = _read_chapter_content(ch)
        prompt = (
            f"{system_preamble}\n\n"
            f"å½“å‰ç« èŠ‚: {ch}\n\n"
            f"{instruction}\n"
            f"ä»¥ä¸‹ä¸ºè¯¥ç« èŠ‚çš„æ‰€æœ‰æ–‡ä»¶å†…å®¹ï¼ˆå¸¦æœ‰æ–‡ä»¶åè¾¹ç•Œæ ‡è®°ï¼‰ï¼š\n\n"
            f"{content}\n"
            f"è¯·ç”Ÿæˆ JSON æ•°ç»„ï¼Œç¡®ä¿æ¯ä¸ªå¯¹è±¡åŒ…å«æ–‡ä»¶å(æ¥è‡ªè¾¹ç•Œæ ‡è®°)ã€ç« èŠ‚å·ç­‰å­—æ®µã€‚"
        )

        try:
            print(f"ğŸ§  [Review] å‘æ¨¡å‹å‘é€è¯·æ±‚ â†’ ç¬¬{ch}ç«  ...")
            if SHOW_PROMPTS:
                print("â€”" * 40)
                print("[Prompt - ç¬¬" + ch + "ç« ]\n" + prompt)
                print("â€”" * 40)
            resp = llm.invoke(prompt)
            print("ğŸ“¥ [Review] å·²æ”¶åˆ°æ¨¡å‹å“åº”ï¼Œè§£æJSON...")
            text = getattr(resp, "content", str(resp))
            parsed = _try_parse_json_list(text)
            # å¼ºåˆ¶å¡«å…… chapter å­—æ®µï¼ˆè‹¥æ¨¡å‹é—æ¼ï¼‰
            for item in parsed:
                item.setdefault("chapter", ch)
            if not parsed:
                sys.stderr.write(f"[è­¦å‘Š] ç¬¬{ch}ç« å®¡æŸ¥ç»“æœè§£æå¤±è´¥ï¼Œè·³è¿‡æˆ–ä¸ºç©ºã€‚\n")
            else:
                sev = _count_severity(parsed)
                print(
                    f"âœ… [Review] ç¬¬{ch}ç« è§£ææˆåŠŸ: {len(parsed)} æ¡è®°å½• | "
                    f"OK={sev['OK']} Warning={sev['Warning']} Critical={sev['Critical Error']}"
                )
            all_reviews.extend(parsed)
        except Exception as e:
            sys.stderr.write(f"[é”™è¯¯] ç¬¬{ch}ç« å®¡æŸ¥å¤±è´¥: {e}\n")

    # æ±‡æ€»æ•´ä½“ç»Ÿè®¡
    overall_levels = {"OK": 0, "Warning": 0, "Critical Error": 0}
    if all_reviews:
        sev = _count_severity(all_reviews)
        overall_levels.update(sev)
    print(
        f"\nğŸ“Š [Review] å…¨éƒ¨ç« èŠ‚æ±‡æ€»: {len(all_reviews)} æ¡è®°å½• | "
        f"OK={overall_levels['OK']} Warning={overall_levels['Warning']} Critical={overall_levels['Critical Error']}"
    )

    return {**state, "reviews": all_reviews}


def synthesize_node(state: ReviewState) -> ReviewState:
    reviews = state.get("reviews", [])
    print("\nğŸ§© [Synthesize] æ±‡æ€»å®¡æŸ¥æ•°æ®:", len(reviews), "æ¡")
    try:
        review_json = json.dumps(reviews, ensure_ascii=False)
    except Exception:
        review_json = "[]"

    prompt = (
        "ä½ æ˜¯ä¸€ä½æ€»ç¼–è¾‘ã€‚è¯·åŸºäºä¸‹é¢ä¸¥æ ¼ç»“æ„åŒ–çš„JSONå®¡æŸ¥ç»“æœï¼Œ"
        "ç”Ÿæˆä¸€ä»½äººç±»å¯è¯»ã€å¯æ“ä½œçš„Markdownæœ€ç»ˆå®¡è®¡æŠ¥å‘Šã€‚\n\n"
        "æŠ¥å‘Šå¿…é¡»åŒ…å«ï¼š\n"
        "1. æ‰§è¡Œæ‘˜è¦ï¼ˆæ€»è®¡æ–‡ä»¶æ•°ã€ä¸¥é‡é—®é¢˜æ•°ç­‰ç»Ÿè®¡ï¼‰ã€‚\n"
        "2. ä¸¥é‡é—®é¢˜ï¼ˆåˆ—å‡ºæ‰€æœ‰è¯„çº§ä¸ºâ€œä¸¥é‡é”™è¯¯â€çš„å‘ç°ï¼‰ã€‚\n"
        "3. è­¦å‘Šä¸å»ºè®®ï¼ˆåˆ—å‡ºæ‰€æœ‰â€œè­¦å‘Šâ€çš„å‘ç°å’Œæ”¹è¿›å»ºè®®ï¼‰ã€‚\n\n"
        f"ä»¥ä¸‹æ˜¯JSONæ•°æ®ï¼š\n{review_json}\n\n"
        "åªè¾“å‡ºMarkdownï¼Œä¸è¦åŒ…å«å…¶ä»–è¯´æ˜ã€‚"
    )

    try:
        print("ğŸ§  [Synthesize] ç”Ÿæˆæœ€ç»ˆMarkdownæŠ¥å‘Š...")
        resp = llm.invoke(prompt)
        report_md = getattr(resp, "content", str(resp))
    except Exception as e:
        report_md = f"# å®¡è®¡æŠ¥å‘Š\n\n[é”™è¯¯] ç”ŸæˆæŠ¥å‘Šå¤±è´¥ï¼š{e}"

    return {**state, "report": report_md}


def build_graph():
    graph = StateGraph(ReviewState)
    graph.add_node("plan", plan_node)
    graph.add_node("review_parallel", review_parallel_node)
    graph.add_node("review", review_node)  # å¤‡ç”¨ï¼šä¸æ¥å…¥ä¸»é“¾
    graph.add_node("synthesize", synthesize_node)

    graph.add_edge(START, "plan")
    graph.add_edge("plan", "review_parallel")
    graph.add_edge("review_parallel", "synthesize")
    graph.add_edge("synthesize", END)
    return graph.compile()


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="LangGraph Review Runner")
    parser.add_argument(
        "-c", "--chapters",
        help="é€—å·åˆ†éš”çš„ç« èŠ‚ç¼–å·ï¼Œå¦‚: 7 æˆ– 1,2,3ã€‚ä¹Ÿå¯ç”¨ç¯å¢ƒå˜é‡ CHAPTERS=...",
        default=os.environ.get("CHAPTERS", ""),
    )
    return parser.parse_args()


def main() -> None:
    args = _parse_args()
    chapters_filter: List[str] = []
    if args.chapters:
        chapters_filter = [s.strip() for s in str(args.chapters).split(",") if s.strip()]
    start_ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print("ğŸš€ ä½¿ç”¨ LangGraph å¯åŠ¨å†…å®¹å®¡æŸ¥å›¢é˜Ÿ...")
    print(f"ğŸ› ï¸ é…ç½®: model={MODEL_NAME} | content_dir={CONTENT_DIR} | MAX_PARALLEL_REVIEWS={MAX_PARALLEL}")
    if chapters_filter:
        print(f"ğŸ¯ ä»…å®¡æŸ¥æŒ‡å®šç« èŠ‚: {', '.join(chapters_filter)}")
    print("ğŸ—ºï¸ å·¥ä½œæµ: START â†’ plan â†’ review_parallel(å¹¶è¡Œ) â†’ synthesize â†’ END")
    app = build_graph()

    # --- å¯è§†åŒ–éƒ¨åˆ† ---
    try:
        print("\nğŸ“Š ASCII å›¾ï¼š")
        print(app.draw_ascii())
        print("\nğŸ“Š Mermaid è¯­æ³•ï¼š")
        print(app.draw_mermaid())
    except Exception:
        # æŸäº›ç‰ˆæœ¬ä¸æ”¯æŒå¯è§†åŒ–å¯¼å‡ºï¼Œå¿½ç•¥
        pass
    
    print("ğŸ”§ å›¾å·²ç¼–è¯‘ï¼Œå¼€å§‹æ‰§è¡Œ...")
    initial_state: ReviewState = {
        "chapters": [],
        "reviews": [],
        "report": "",
        "chapters_filter": chapters_filter,
    }
    try:
        final_state: ReviewState = asyncio.run(app.ainvoke(initial_state))  # type: ignore
    except Exception:
        final_state = app.invoke(initial_state)

    end_ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"â±ï¸ æ‰§è¡Œæ—¶é—´: {start_ts} â†’ {end_ts}")
    print("\n\nâœ… å›¢é˜Ÿå·¥ä½œå®Œæˆï¼è¿™æ˜¯æœ€ç»ˆæŠ¥å‘Š:\n")
    print("=" * 50)
    print(final_state.get("report", "(æ— æŠ¥å‘Š)"))


if __name__ == "__main__":
    main()
